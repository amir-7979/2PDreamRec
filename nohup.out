Traceback (most recent call last):
  File "/home/nit/Desktop/2PDreamRec/DreamRec_Gen.py", line 500, in <module>
    main()
  File "/home/nit/Desktop/2PDreamRec/DreamRec_Gen.py", line 310, in main
    tuning_scheduler_eps_recorder = TuningRecorder("scheduler_eps", scheduler_eps_candidates, save_dir="category")
NameError: name 'scheduler_eps_candidates' is not defined. Did you mean: 'scheduler_candidates'?
Tuning lr:   0%|          | 0/5 [00:00<?, ?it/s]
========== Fold 3 ==========
INFO:root:Using statics: seq_size = 10, genre_vocab_size = 18
Fold 3 Epoch 001; Train loss: 241.2081; Time: 00:00:03
Fold 3 Epoch 002; Train loss: 54.9684; Time: 00:00:01
Fold 3 Epoch 003; Train loss: 10.4042; Time: 00:00:01
Fold 3 Epoch 004; Train loss: 7.0151; Time: 00:00:01
Fold 3 Epoch 005; Train loss: 3.1335; Time: 00:00:01
Fold 3 Epoch 006; Train loss: 1.9810; Time: 00:00:02
Fold 3 Epoch 007; Train loss: 1.0523; Time: 00:00:02
Fold 3 Epoch 008; Train loss: 0.7284; Time: 00:00:02
Fold 3 Epoch 009; Train loss: 0.5692; Time: 00:00:02
Fold 3 Epoch 010; Train loss: 0.4276; Time: 00:00:02
Fold 3: Evaluation at Epoch 10
HR@5       NDCG@5     HR@10      NDCG@10   
0.125828   0.082645   0.433775   0.179360  
Loss: 0.4041
Fold 3 Epoch 011; Train loss: 0.3519; Time: 00:00:02
Fold 3 Epoch 012; Train loss: 0.2933; Time: 00:00:02
Fold 3 Epoch 013; Train loss: 0.2681; Time: 00:00:02
Fold 3 Epoch 014; Train loss: 0.2472; Time: 00:00:01
Fold 3 Epoch 015; Train loss: 0.2324; Time: 00:00:01
Fold 3 Epoch 016; Train loss: 0.2209; Time: 00:00:02
Fold 3 Epoch 017; Train loss: 0.2108; Time: 00:00:02
Fold 3 Epoch 018; Train loss: 0.2049; Time: 00:00:02
Fold 3 Epoch 019; Train loss: 0.2005; Time: 00:00:01
Fold 3 Epoch 020; Train loss: 0.1960; Time: 00:00:02
Fold 3: Evaluation at Epoch 20
HR@5       NDCG@5     HR@10      NDCG@10   
0.306291   0.195405   0.591060   0.281694  
Loss: 0.2379
Fold 3 Epoch 021; Train loss: 0.1860; Time: 00:00:01
Fold 3 Epoch 022; Train loss: 0.1770; Time: 00:00:01
Fold 3 Epoch 023; Train loss: 0.1745; Time: 00:00:01
Fold 3 Epoch 024; Train loss: 0.1719; Time: 00:00:01
Fold 3 Epoch 025; Train loss: 0.1677; Time: 00:00:01
Fold 3 Epoch 026; Train loss: 0.1606; Time: 00:00:01
Fold 3 Epoch 027; Train loss: 0.1573; Time: 00:00:02
Fold 3 Epoch 028; Train loss: 0.1519; Time: 00:00:01
Fold 3 Epoch 029; Train loss: 0.1481; Time: 00:00:01
Fold 3 Epoch 030; Train loss: 0.1434; Time: 00:00:01
Fold 3: Evaluation at Epoch 30
HR@5       NDCG@5     HR@10      NDCG@10   
0.246689   0.125098   0.557947   0.221078  
Loss: 0.1723
Fold 3 Epoch 031; Train loss: 0.1422; Time: 00:00:01
Fold 3 Epoch 032; Train loss: 0.1306; Time: 00:00:01
Fold 3 Epoch 033; Train loss: 0.1297; Time: 00:00:01
Fold 3 Epoch 034; Train loss: 0.1257; Time: 00:00:01
Fold 3 Epoch 035; Train loss: 0.1223; Time: 00:00:01
Fold 3 Epoch 036; Train loss: 0.1154; Time: 00:00:01
Fold 3 Epoch 037; Train loss: 0.1132; Time: 00:00:02
Fold 3 Epoch 038; Train loss: 0.1107; Time: 00:00:01
Fold 3 Epoch 039; Train loss: 0.1041; Time: 00:00:01
Fold 3 Epoch 040; Train loss: 0.1008; Time: 00:00:01
Fold 3: Evaluation at Epoch 40
HR@5       NDCG@5     HR@10      NDCG@10   
0.312914   0.171729   0.617550   0.264241  
Loss: 0.1203
Fold 3 Epoch 041; Train loss: 0.1004; Time: 00:00:02
Fold 3 Epoch 042; Train loss: 0.0956; Time: 00:00:02
Fold 3 Epoch 043; Train loss: 0.0925; Time: 00:00:02
Fold 3 Epoch 044; Train loss: 0.0885; Time: 00:00:02
Fold 3 Epoch 045; Train loss: 0.0863; Time: 00:00:02
Fold 3 Epoch 046; Train loss: 0.0796; Time: 00:00:02
Fold 3 Epoch 047; Train loss: 0.0778; Time: 00:00:02
Fold 3 Epoch 048; Train loss: 0.0755; Time: 00:00:02
Fold 3 Epoch 049; Train loss: 0.0740; Time: 00:00:02
Fold 3 Epoch 050; Train loss: 0.0708; Time: 00:00:01
Fold 3: Evaluation at Epoch 50
HR@5       NDCG@5     HR@10      NDCG@10   
0.240066   0.111896   0.548013   0.207605  
Loss: 0.0794
Fold 3 Epoch 051; Train loss: 0.0669; Time: 00:00:01
Fold 3 Epoch 052; Train loss: 0.0651; Time: 00:00:01
Fold 3 Epoch 053; Train loss: 0.0597; Time: 00:00:01
Fold 3 Epoch 054; Train loss: 0.0620; Time: 00:00:01
Fold 3 Epoch 055; Train loss: 0.0581; Time: 00:00:02
Fold 3 Epoch 056; Train loss: 0.0543; Time: 00:00:01
Fold 3 Epoch 057; Train loss: 0.0549; Time: 00:00:01
Fold 3 Epoch 058; Train loss: 0.0517; Time: 00:00:01
Fold 3 Epoch 059; Train loss: 0.0499; Time: 00:00:01
Fold 3 Epoch 060; Train loss: 0.0477; Time: 00:00:01
Fold 3: Evaluation at Epoch 60
HR@5       NDCG@5     HR@10      NDCG@10   
0.236755   0.108998   0.582781   0.219663  
Loss: 0.0426
Fold 3 Epoch 061; Train loss: 0.0464; Time: 00:00:01
Fold 3 Epoch 062; Train loss: 0.0444; Time: 00:00:01
Fold 3 Epoch 063; Train loss: 0.0418; Time: 00:00:02
Fold 3 Epoch 064; Train loss: 0.0414; Time: 00:00:01
Fold 3 Epoch 065; Train loss: 0.0401; Time: 00:00:02
Fold 3 Epoch 066; Train loss: 0.0385; Time: 00:00:01
Fold 3 Epoch 067; Train loss: 0.0369; Time: 00:00:01
Fold 3 Epoch 068; Train loss: 0.0361; Time: 00:00:01
Fold 3 Epoch 069; Train loss: 0.0353; Time: 00:00:01
Fold 3 Epoch 070; Train loss: 0.0327; Time: 00:00:01
Fold 3: Evaluation at Epoch 70
HR@5       NDCG@5     HR@10      NDCG@10   
0.344371   0.152535   0.612583   0.236765  
Loss: 0.0247
Fold 3 Epoch 071; Train loss: 0.0320; Time: 00:00:02
Fold 3 Epoch 072; Train loss: 0.0307; Time: 00:00:02
Fold 3 Epoch 073; Train loss: 0.0280; Time: 00:00:01
Fold 3 Epoch 074; Train loss: 0.0274; Time: 00:00:01
Fold 3 Epoch 075; Train loss: 0.0252; Time: 00:00:01
Fold 3 Epoch 076; Train loss: 0.0242; Time: 00:00:01
Fold 3 Epoch 077; Train loss: 0.0228; Time: 00:00:01
Fold 3 Epoch 078; Train loss: 0.0217; Time: 00:00:01
Fold 3 Epoch 079; Train loss: 0.0212; Time: 00:00:01
Fold 3 Epoch 080; Train loss: 0.0192; Time: 00:00:01
Fold 3: Evaluation at Epoch 80
HR@5       NDCG@5     HR@10      NDCG@10   
0.346026   0.151953   0.587748   0.225467  
Loss: 0.0138
Fold 3 Epoch 081; Train loss: 0.0184; Time: 00:00:01
Fold 3 Epoch 082; Train loss: 0.0174; Time: 00:00:01
Fold 3 Epoch 083; Train loss: 0.0163; Time: 00:00:01
Fold 3 Epoch 084; Train loss: 0.0150; Time: 00:00:01
Fold 3 Epoch 085; Train loss: 0.0143; Time: 00:00:02
Fold 3 Epoch 086; Train loss: 0.0126; Time: 00:00:01
Fold 3 Epoch 087; Train loss: 0.0120; Time: 00:00:02
Fold 3 Epoch 088; Train loss: 0.0112; Time: 00:00:02
Fold 3 Epoch 089; Train loss: 0.0103; Time: 00:00:02
Fold 3 Epoch 090; Train loss: 0.0096; Time: 00:00:02
Fold 3: Evaluation at Epoch 90
HR@5       NDCG@5     HR@10      NDCG@10   
0.346026   0.156231   0.586093   0.235892  
Loss: 0.0073
Fold 3 Epoch 091; Train loss: 0.0089; Time: 00:00:01
Fold 3 Epoch 092; Train loss: 0.0082; Time: 00:00:01
Fold 3 Epoch 093; Train loss: 0.0078; Time: 00:00:01
Fold 3 Epoch 094; Train loss: 0.0070; Time: 00:00:01
Fold 3 Epoch 095; Train loss: 0.0064; Time: 00:00:01
Fold 3 Epoch 096; Train loss: 0.0062; Time: 00:00:01
Fold 3 Epoch 097; Train loss: 0.0058; Time: 00:00:01
Fold 3 Epoch 098; Train loss: 0.0052; Time: 00:00:01
Fold 3 Epoch 099; Train loss: 0.0050; Time: 00:00:01
Fold 3 Epoch 100; Train loss: 0.0048; Time: 00:00:01
Fold 3: Evaluation at Epoch 100
HR@5       NDCG@5     HR@10      NDCG@10   
0.071192   0.039148   0.645695   0.224916  
Loss: 0.0052
[lr candidate 0.1] Fold 3: HR@10 = [np.float64(0.4337748344370861), np.float64(0.5910596026490066), np.float64(0.5579470198675497), np.float64(0.6175496688741722), np.float64(0.5480132450331126), np.float64(0.5827814569536424), np.float64(0.6125827814569537), np.float64(0.5877483443708609), np.float64(0.5860927152317881), np.float64(0.6456953642384106)]
Tuning lr:  20%|██        | 1/5 [03:40<14:41, 220.40s/it]
========== Fold 3 ==========
INFO:root:Using statics: seq_size = 10, genre_vocab_size = 18
Fold 3 Epoch 001; Train loss: 11.7587; Time: 00:00:02
Fold 3 Epoch 002; Train loss: 2.7335; Time: 00:00:02
Fold 3 Epoch 003; Train loss: 2.3047; Time: 00:00:01
Fold 3 Epoch 004; Train loss: 1.2520; Time: 00:00:02
Fold 3 Epoch 005; Train loss: 0.6489; Time: 00:00:01
Fold 3 Epoch 006; Train loss: 0.4626; Time: 00:00:01
Fold 3 Epoch 007; Train loss: 0.3003; Time: 00:00:01
Fold 3 Epoch 008; Train loss: 0.2489; Time: 00:00:02
Fold 3 Epoch 009; Train loss: 0.2008; Time: 00:00:02
Fold 3 Epoch 010; Train loss: 0.1758; Time: 00:00:01
Fold 3: Evaluation at Epoch 10
HR@5       NDCG@5     HR@10      NDCG@10   
0.370861   0.230371   0.600993   0.302937  
Loss: 0.1979
Fold 3 Epoch 011; Train loss: 0.1512; Time: 00:00:02
Fold 3 Epoch 012; Train loss: 0.1338; Time: 00:00:02
Fold 3 Epoch 013; Train loss: 0.1236; Time: 00:00:02
Fold 3 Epoch 014; Train loss: 0.1130; Time: 00:00:02
Fold 3 Epoch 015; Train loss: 0.1028; Time: 00:00:02
Fold 3 Epoch 016; Train loss: 0.0952; Time: 00:00:02
Fold 3 Epoch 017; Train loss: 0.0836; Time: 00:00:02
Fold 3 Epoch 018; Train loss: 0.0745; Time: 00:00:02
Fold 3 Epoch 019; Train loss: 0.0673; Time: 00:00:02
Fold 3 Epoch 020; Train loss: 0.0585; Time: 00:00:02
Fold 3: Evaluation at Epoch 20
HR@5       NDCG@5     HR@10      NDCG@10   
0.331126   0.184963   0.604305   0.270910  
Loss: 0.0609
Fold 3 Epoch 021; Train loss: 0.0532; Time: 00:00:02
Fold 3 Epoch 022; Train loss: 0.0481; Time: 00:00:02
Fold 3 Epoch 023; Train loss: 0.0425; Time: 00:00:02
Fold 3 Epoch 024; Train loss: 0.0373; Time: 00:00:02
Fold 3 Epoch 025; Train loss: 0.0337; Time: 00:00:02
Fold 3 Epoch 026; Train loss: 0.0298; Time: 00:00:02
Fold 3 Epoch 027; Train loss: 0.0265; Time: 00:00:02
Fold 3 Epoch 028; Train loss: 0.0239; Time: 00:00:02
Fold 3 Epoch 029; Train loss: 0.0200; Time: 00:00:02
Fold 3 Epoch 030; Train loss: 0.0166; Time: 00:00:02
Fold 3: Evaluation at Epoch 30
HR@5       NDCG@5     HR@10      NDCG@10   
0.536424   0.250288   0.647351   0.285129  
Loss: 0.0123
Fold 3 Epoch 031; Train loss: 0.0139; Time: 00:00:02
Fold 3 Epoch 032; Train loss: 0.0111; Time: 00:00:02
Fold 3 Epoch 033; Train loss: 0.0089; Time: 00:00:02
Fold 3 Epoch 034; Train loss: 0.0068; Time: 00:00:02
Fold 3 Epoch 035; Train loss: 0.0054; Time: 00:00:02
Fold 3 Epoch 036; Train loss: 0.0043; Time: 00:00:02
Fold 3 Epoch 037; Train loss: 0.0035; Time: 00:00:02
Fold 3 Epoch 038; Train loss: 0.0031; Time: 00:00:02
Fold 3 Epoch 039; Train loss: 0.0027; Time: 00:00:02
Fold 3 Epoch 040; Train loss: 0.0025; Time: 00:00:02
Fold 3: Evaluation at Epoch 40
HR@5       NDCG@5     HR@10      NDCG@10   
0.054636   0.030774   0.602649   0.206434  
Loss: 0.0040
Fold 3 Epoch 041; Train loss: 0.0023; Time: 00:00:02
Fold 3 Epoch 042; Train loss: 0.0023; Time: 00:00:02
Fold 3 Epoch 043; Train loss: 0.0022; Time: 00:00:02
Fold 3 Epoch 044; Train loss: 0.0021; Time: 00:00:02
Fold 3 Epoch 045; Train loss: 0.0021; Time: 00:00:02
Fold 3 Epoch 046; Train loss: 0.0019; Time: 00:00:02
Fold 3 Epoch 047; Train loss: 0.0019; Time: 00:00:02
Fold 3 Epoch 048; Train loss: 0.0018; Time: 00:00:02
Fold 3 Epoch 049; Train loss: 0.0018; Time: 00:00:02
Fold 3 Epoch 050; Train loss: 0.0017; Time: 00:00:02
Fold 3: Evaluation at Epoch 50
HR@5       NDCG@5     HR@10      NDCG@10   
0.309603   0.149936   0.844371   0.311888  
Loss: 0.0035
Fold 3 Epoch 051; Train loss: 0.0017; Time: 00:00:02
Fold 3 Epoch 052; Train loss: 0.0016; Time: 00:00:02
Fold 3 Epoch 053; Train loss: 0.0016; Time: 00:00:02
Fold 3 Epoch 054; Train loss: 0.0015; Time: 00:00:02
Fold 3 Epoch 055; Train loss: 0.0015; Time: 00:00:01
Fold 3 Epoch 056; Train loss: 0.0014; Time: 00:00:01
Fold 3 Epoch 057; Train loss: 0.0014; Time: 00:00:01
Fold 3 Epoch 058; Train loss: 0.0014; Time: 00:00:01
Fold 3 Epoch 059; Train loss: 0.0014; Time: 00:00:02
Fold 3 Epoch 060; Train loss: 0.0013; Time: 00:00:01
Fold 3: Evaluation at Epoch 60
HR@5       NDCG@5     HR@10      NDCG@10   
0.026490   0.012469   0.298013   0.092878  
Loss: 0.0027
Fold 3 Epoch 061; Train loss: 0.0013; Time: 00:00:01
Fold 3 Epoch 062; Train loss: 0.0013; Time: 00:00:01
Fold 3 Epoch 063; Train loss: 0.0012; Time: 00:00:01
Fold 3 Epoch 064; Train loss: 0.0012; Time: 00:00:01
Fold 3 Epoch 065; Train loss: 0.0012; Time: 00:00:01
Fold 3 Epoch 066; Train loss: 0.0011; Time: 00:00:01
Fold 3 Epoch 067; Train loss: 0.0011; Time: 00:00:02
Fold 3 Epoch 068; Train loss: 0.0011; Time: 00:00:02
Fold 3 Epoch 069; Train loss: 0.0010; Time: 00:00:01
Fold 3 Epoch 070; Train loss: 0.0011; Time: 00:00:01
Fold 3: Evaluation at Epoch 70
HR@5       NDCG@5     HR@10      NDCG@10   
0.084437   0.038371   0.600993   0.197874  
Loss: 0.0026
Fold 3 Epoch 071; Train loss: 0.0010; Time: 00:00:01
Fold 3 Epoch 072; Train loss: 0.0010; Time: 00:00:02
Fold 3 Epoch 073; Train loss: 0.0010; Time: 00:00:02
Fold 3 Epoch 074; Train loss: 0.0009; Time: 00:00:02
Fold 3 Epoch 075; Train loss: 0.0009; Time: 00:00:02
Fold 3 Epoch 076; Train loss: 0.0009; Time: 00:00:02
Fold 3 Epoch 077; Train loss: 0.0009; Time: 00:00:02
Fold 3 Epoch 078; Train loss: 0.0009; Time: 00:00:02
Fold 3 Epoch 079; Train loss: 0.0009; Time: 00:00:02
Fold 3 Epoch 080; Train loss: 0.0009; Time: 00:00:02
Fold 3: Evaluation at Epoch 80
HR@5       NDCG@5     HR@10      NDCG@10   
0.344371   0.155404   0.582781   0.231020  
Loss: 0.0025
Fold 3 Epoch 081; Train loss: 0.0008; Time: 00:00:02
Fold 3 Epoch 082; Train loss: 0.0008; Time: 00:00:02
Fold 3 Epoch 083; Train loss: 0.0008; Time: 00:00:02
Fold 3 Epoch 084; Train loss: 0.0009; Time: 00:00:02
Fold 3 Epoch 085; Train loss: 0.0008; Time: 00:00:02
Fold 3 Epoch 086; Train loss: 0.0008; Time: 00:00:02
Fold 3 Epoch 087; Train loss: 0.0008; Time: 00:00:02
Fold 3 Epoch 088; Train loss: 0.0007; Time: 00:00:02
Fold 3 Epoch 089; Train loss: 0.0007; Time: 00:00:02
Fold 3 Epoch 090; Train loss: 0.0007; Time: 00:00:02
Fold 3: Evaluation at Epoch 90
HR@5       NDCG@5     HR@10      NDCG@10   
0.067881   0.034884   0.357616   0.123953  
Loss: 0.0019
Fold 3 Epoch 091; Train loss: 0.0007; Time: 00:00:02
Fold 3 Epoch 092; Train loss: 0.0007; Time: 00:00:02
Fold 3 Epoch 093; Train loss: 0.0007; Time: 00:00:02
Fold 3 Epoch 094; Train loss: 0.0007; Time: 00:00:02
Fold 3 Epoch 095; Train loss: 0.0007; Time: 00:00:02
Fold 3 Epoch 096; Train loss: 0.0007; Time: 00:00:02
Fold 3 Epoch 097; Train loss: 0.0007; Time: 00:00:02
Fold 3 Epoch 098; Train loss: 0.0007; Time: 00:00:02
Fold 3 Epoch 099; Train loss: 0.0006; Time: 00:00:02
Fold 3 Epoch 100; Train loss: 0.0006; Time: 00:00:02
Fold 3: Evaluation at Epoch 100
HR@5       NDCG@5     HR@10      NDCG@10   
0.094371   0.058511   0.379139   0.146124  
Loss: 0.0017
[lr candidate 0.05] Fold 3: HR@10 = [np.float64(0.6009933774834437), np.float64(0.6043046357615894), np.float64(0.6473509933774835), np.float64(0.6026490066225165), np.float64(0.8443708609271523), np.float64(0.2980132450331126), np.float64(0.6009933774834437), np.float64(0.5827814569536424), np.float64(0.3576158940397351), np.float64(0.3791390728476821)]
Tuning lr:  40%|████      | 2/5 [07:46<11:46, 235.35s/it]
========== Fold 3 ==========
INFO:root:Using statics: seq_size = 10, genre_vocab_size = 18
Fold 3 Epoch 001; Train loss: 0.6341; Time: 00:00:02
Fold 3 Epoch 002; Train loss: 0.3407; Time: 00:00:02
Fold 3 Epoch 003; Train loss: 0.2816; Time: 00:00:02
Fold 3 Epoch 004; Train loss: 0.2479; Time: 00:00:02
Fold 3 Epoch 005; Train loss: 0.2256; Time: 00:00:02
Fold 3 Epoch 006; Train loss: 0.2068; Time: 00:00:02
Fold 3 Epoch 007; Train loss: 0.1986; Time: 00:00:01
Fold 3 Epoch 008; Train loss: 0.1830; Time: 00:00:02
Fold 3 Epoch 009; Train loss: 0.1745; Time: 00:00:02
Fold 3 Epoch 010; Train loss: 0.1609; Time: 00:00:02
Fold 3: Evaluation at Epoch 10
HR@5       NDCG@5     HR@10      NDCG@10   
0.653974   0.504352   0.802980   0.552173  
Loss: 0.2126
Fold 3 Epoch 011; Train loss: 0.1568; Time: 00:00:02
Fold 3 Epoch 012; Train loss: 0.1480; Time: 00:00:02
Fold 3 Epoch 013; Train loss: 0.1404; Time: 00:00:02
Fold 3 Epoch 014; Train loss: 0.1334; Time: 00:00:02
Fold 3 Epoch 015; Train loss: 0.1265; Time: 00:00:02
Fold 3 Epoch 016; Train loss: 0.1181; Time: 00:00:02
Fold 3 Epoch 017; Train loss: 0.1129; Time: 00:00:02
Fold 3 Epoch 018; Train loss: 0.1052; Time: 00:00:02
Fold 3 Epoch 019; Train loss: 0.0991; Time: 00:00:02
Fold 3 Epoch 020; Train loss: 0.0925; Time: 00:00:02
Fold 3: Evaluation at Epoch 20
HR@5       NDCG@5     HR@10      NDCG@10   
0.713576   0.537925   0.857616   0.584566  
Loss: 0.1177
Fold 3 Epoch 021; Train loss: 0.0873; Time: 00:00:02
Fold 3 Epoch 022; Train loss: 0.0804; Time: 00:00:02
Fold 3 Epoch 023; Train loss: 0.0743; Time: 00:00:02
Fold 3 Epoch 024; Train loss: 0.0699; Time: 00:00:02
Fold 3 Epoch 025; Train loss: 0.0639; Time: 00:00:03
Fold 3 Epoch 026; Train loss: 0.0592; Time: 00:00:02
Fold 3 Epoch 027; Train loss: 0.0562; Time: 00:00:02
Fold 3 Epoch 028; Train loss: 0.0510; Time: 00:00:02
Fold 3 Epoch 029; Train loss: 0.0470; Time: 00:00:01
Fold 3 Epoch 030; Train loss: 0.0424; Time: 00:00:01
Fold 3: Evaluation at Epoch 30
HR@5       NDCG@5     HR@10      NDCG@10   
0.745033   0.548991   0.905629   0.601152  
Loss: 0.0598
Fold 3 Epoch 031; Train loss: 0.0385; Time: 00:00:01
Fold 3 Epoch 032; Train loss: 0.0350; Time: 00:00:01
Fold 3 Epoch 033; Train loss: 0.0319; Time: 00:00:02
Fold 3 Epoch 034; Train loss: 0.0278; Time: 00:00:01
Fold 3 Epoch 035; Train loss: 0.0247; Time: 00:00:02
Fold 3 Epoch 036; Train loss: 0.0219; Time: 00:00:01
Fold 3 Epoch 037; Train loss: 0.0194; Time: 00:00:01
Fold 3 Epoch 038; Train loss: 0.0169; Time: 00:00:01
Fold 3 Epoch 039; Train loss: 0.0146; Time: 00:00:01
Fold 3 Epoch 040; Train loss: 0.0129; Time: 00:00:02
Fold 3: Evaluation at Epoch 40
HR@5       NDCG@5     HR@10      NDCG@10   
0.581126   0.399727   0.887417   0.501296  
Loss: 0.0256
Fold 3 Epoch 041; Train loss: 0.0115; Time: 00:00:02
Fold 3 Epoch 042; Train loss: 0.0097; Time: 00:00:02
Fold 3 Epoch 043; Train loss: 0.0090; Time: 00:00:02
Fold 3 Epoch 044; Train loss: 0.0081; Time: 00:00:02
Fold 3 Epoch 045; Train loss: 0.0069; Time: 00:00:02
Fold 3 Epoch 046; Train loss: 0.0063; Time: 00:00:02
Fold 3 Epoch 047; Train loss: 0.0053; Time: 00:00:01
Fold 3 Epoch 048; Train loss: 0.0049; Time: 00:00:02
Fold 3 Epoch 049; Train loss: 0.0043; Time: 00:00:01
Fold 3 Epoch 050; Train loss: 0.0040; Time: 00:00:01
Fold 3: Evaluation at Epoch 50
HR@5       NDCG@5     HR@10      NDCG@10   
0.536424   0.348981   0.862583   0.452916  
Loss: 0.0122
Fold 3 Epoch 051; Train loss: 0.0037; Time: 00:00:01
Fold 3 Epoch 052; Train loss: 0.0032; Time: 00:00:02
Fold 3 Epoch 053; Train loss: 0.0030; Time: 00:00:01
Fold 3 Epoch 054; Train loss: 0.0026; Time: 00:00:01
Fold 3 Epoch 055; Train loss: 0.0025; Time: 00:00:01
Fold 3 Epoch 056; Train loss: 0.0025; Time: 00:00:01
Fold 3 Epoch 057; Train loss: 0.0021; Time: 00:00:01
Fold 3 Epoch 058; Train loss: 0.0019; Time: 00:00:01
Fold 3 Epoch 059; Train loss: 0.0017; Time: 00:00:02
Fold 3 Epoch 060; Train loss: 0.0017; Time: 00:00:01
Fold 3: Evaluation at Epoch 60
HR@5       NDCG@5     HR@10      NDCG@10   
0.246689   0.124249   0.862583   0.329163  
Loss: 0.0070
Fold 3 Epoch 061; Train loss: 0.0015; Time: 00:00:01
Fold 3 Epoch 062; Train loss: 0.0014; Time: 00:00:01
Fold 3 Epoch 063; Train loss: 0.0013; Time: 00:00:02
Fold 3 Epoch 064; Train loss: 0.0012; Time: 00:00:01
Fold 3 Epoch 065; Train loss: 0.0011; Time: 00:00:01
Fold 3 Epoch 066; Train loss: 0.0010; Time: 00:00:01
Fold 3 Epoch 067; Train loss: 0.0011; Time: 00:00:01
Fold 3 Epoch 068; Train loss: 0.0009; Time: 00:00:02
Fold 3 Epoch 069; Train loss: 0.0009; Time: 00:00:02
Fold 3 Epoch 070; Train loss: 0.0008; Time: 00:00:02
Fold 3: Evaluation at Epoch 70
HR@5       NDCG@5     HR@10      NDCG@10   
0.495033   0.205914   0.860927   0.321223  
Loss: 0.0045
Fold 3 Epoch 071; Train loss: 0.0008; Time: 00:00:02
Fold 3 Epoch 072; Train loss: 0.0007; Time: 00:00:02
Fold 3 Epoch 073; Train loss: 0.0007; Time: 00:00:02
Fold 3 Epoch 074; Train loss: 0.0007; Time: 00:00:02
Fold 3 Epoch 075; Train loss: 0.0006; Time: 00:00:02
Fold 3 Epoch 076; Train loss: 0.0006; Time: 00:00:02
Fold 3 Epoch 077; Train loss: 0.0005; Time: 00:00:02
Fold 3 Epoch 078; Train loss: 0.0005; Time: 00:00:02
Fold 3 Epoch 079; Train loss: 0.0005; Time: 00:00:02
Fold 3 Epoch 080; Train loss: 0.0005; Time: 00:00:02
Fold 3: Evaluation at Epoch 80
HR@5       NDCG@5     HR@10      NDCG@10   
0.288079   0.117145   0.600993   0.223925  
Loss: 0.0029
Fold 3 Epoch 081; Train loss: 0.0005; Time: 00:00:01
Fold 3 Epoch 082; Train loss: 0.0004; Time: 00:00:01
Fold 3 Epoch 083; Train loss: 0.0004; Time: 00:00:01
Fold 3 Epoch 084; Train loss: 0.0004; Time: 00:00:01
Fold 3 Epoch 085; Train loss: 0.0004; Time: 00:00:01
Fold 3 Epoch 086; Train loss: 0.0004; Time: 00:00:02
Fold 3 Epoch 087; Train loss: 0.0003; Time: 00:00:01
Fold 3 Epoch 088; Train loss: 0.0003; Time: 00:00:01
Fold 3 Epoch 089; Train loss: 0.0003; Time: 00:00:01
Fold 3 Epoch 090; Train loss: 0.0003; Time: 00:00:01
Fold 3: Evaluation at Epoch 90
HR@5       NDCG@5     HR@10      NDCG@10   
0.051325   0.025503   0.571192   0.191393  
Loss: 0.0019
Fold 3 Epoch 091; Train loss: 0.0003; Time: 00:00:01
Fold 3 Epoch 092; Train loss: 0.0003; Time: 00:00:01
Fold 3 Epoch 093; Train loss: 0.0003; Time: 00:00:01
Fold 3 Epoch 094; Train loss: 0.0002; Time: 00:00:02
Fold 3 Epoch 095; Train loss: 0.0002; Time: 00:00:01
Fold 3 Epoch 096; Train loss: 0.0002; Time: 00:00:01
Fold 3 Epoch 097; Train loss: 0.0002; Time: 00:00:01
Fold 3 Epoch 098; Train loss: 0.0002; Time: 00:00:01
Fold 3 Epoch 099; Train loss: 0.0002; Time: 00:00:01
Fold 3 Epoch 100; Train loss: 0.0002; Time: 00:00:01
Fold 3: Evaluation at Epoch 100
HR@5       NDCG@5     HR@10      NDCG@10   
0.057947   0.029226   0.397351   0.140031  
Loss: 0.0012
[lr candidate 0.01] Fold 3: HR@10 = [np.float64(0.8029801324503312), np.float64(0.8576158940397351), np.float64(0.9056291390728477), np.float64(0.8874172185430463), np.float64(0.8625827814569537), np.float64(0.8625827814569537), np.float64(0.8609271523178808), np.float64(0.6009933774834437), np.float64(0.5711920529801324), np.float64(0.3973509933774834)]
Tuning lr:  60%|██████    | 3/5 [11:37<07:47, 233.61s/it]
========== Fold 3 ==========
INFO:root:Using statics: seq_size = 10, genre_vocab_size = 18
Fold 3 Epoch 001; Train loss: 0.7849; Time: 00:00:01
Fold 3 Epoch 002; Train loss: 0.4323; Time: 00:00:01
Fold 3 Epoch 003; Train loss: 0.3442; Time: 00:00:01
Fold 3 Epoch 004; Train loss: 0.3061; Time: 00:00:01
Fold 3 Epoch 005; Train loss: 0.2828; Time: 00:00:01
Fold 3 Epoch 006; Train loss: 0.2655; Time: 00:00:01
Fold 3 Epoch 007; Train loss: 0.2461; Time: 00:00:02
Fold 3 Epoch 008; Train loss: 0.2416; Time: 00:00:01
Fold 3 Epoch 009; Train loss: 0.2336; Time: 00:00:02
Fold 3 Epoch 010; Train loss: 0.2196; Time: 00:00:01
Fold 3: Evaluation at Epoch 10
HR@5       NDCG@5     HR@10      NDCG@10   
0.607616   0.501824   0.773179   0.555361  
Loss: 0.2897
Fold 3 Epoch 011; Train loss: 0.2213; Time: 00:00:01
Fold 3 Epoch 012; Train loss: 0.2164; Time: 00:00:02
Fold 3 Epoch 013; Train loss: 0.2049; Time: 00:00:02
Fold 3 Epoch 014; Train loss: 0.2041; Time: 00:00:01
Fold 3 Epoch 015; Train loss: 0.2012; Time: 00:00:01
Fold 3 Epoch 016; Train loss: 0.1978; Time: 00:00:01
Fold 3 Epoch 017; Train loss: 0.1938; Time: 00:00:01
Fold 3 Epoch 018; Train loss: 0.1841; Time: 00:00:01
Fold 3 Epoch 019; Train loss: 0.1829; Time: 00:00:01
Fold 3 Epoch 020; Train loss: 0.1736; Time: 00:00:02
Fold 3: Evaluation at Epoch 20
HR@5       NDCG@5     HR@10      NDCG@10   
0.604305   0.462575   0.753311   0.511311  
Loss: 0.2190
Fold 3 Epoch 021; Train loss: 0.1712; Time: 00:00:02
Fold 3 Epoch 022; Train loss: 0.1655; Time: 00:00:02
Fold 3 Epoch 023; Train loss: 0.1613; Time: 00:00:02
Fold 3 Epoch 024; Train loss: 0.1539; Time: 00:00:02
Fold 3 Epoch 025; Train loss: 0.1565; Time: 00:00:02
Fold 3 Epoch 026; Train loss: 0.1504; Time: 00:00:02
Fold 3 Epoch 027; Train loss: 0.1475; Time: 00:00:02
Fold 3 Epoch 028; Train loss: 0.1442; Time: 00:00:02
Fold 3 Epoch 029; Train loss: 0.1420; Time: 00:00:02
Fold 3 Epoch 030; Train loss: 0.1324; Time: 00:00:02
Fold 3: Evaluation at Epoch 30
HR@5       NDCG@5     HR@10      NDCG@10   
0.610927   0.461531   0.781457   0.516555  
Loss: 0.1733
Fold 3 Epoch 031; Train loss: 0.1313; Time: 00:00:02
Fold 3 Epoch 032; Train loss: 0.1244; Time: 00:00:02
Fold 3 Epoch 033; Train loss: 0.1252; Time: 00:00:02
Fold 3 Epoch 034; Train loss: 0.1205; Time: 00:00:02
Fold 3 Epoch 035; Train loss: 0.1170; Time: 00:00:02
Fold 3 Epoch 036; Train loss: 0.1146; Time: 00:00:02
Fold 3 Epoch 037; Train loss: 0.1116; Time: 00:00:02
Fold 3 Epoch 038; Train loss: 0.1062; Time: 00:00:02
Fold 3 Epoch 039; Train loss: 0.1014; Time: 00:00:02
Fold 3 Epoch 040; Train loss: 0.0977; Time: 00:00:02
Fold 3: Evaluation at Epoch 40
HR@5       NDCG@5     HR@10      NDCG@10   
0.620861   0.415252   0.781457   0.466983  
Loss: 0.1342
Fold 3 Epoch 041; Train loss: 0.0962; Time: 00:00:02
Fold 3 Epoch 042; Train loss: 0.0926; Time: 00:00:02
Fold 3 Epoch 043; Train loss: 0.0901; Time: 00:00:02
Fold 3 Epoch 044; Train loss: 0.0883; Time: 00:00:02
Fold 3 Epoch 045; Train loss: 0.0848; Time: 00:00:02
Fold 3 Epoch 046; Train loss: 0.0805; Time: 00:00:02
Fold 3 Epoch 047; Train loss: 0.0787; Time: 00:00:02
Fold 3 Epoch 048; Train loss: 0.0772; Time: 00:00:02
Fold 3 Epoch 049; Train loss: 0.0736; Time: 00:00:02
Fold 3 Epoch 050; Train loss: 0.0706; Time: 00:00:02
Fold 3: Evaluation at Epoch 50
HR@5       NDCG@5     HR@10      NDCG@10   
0.513245   0.305881   0.766556   0.388523  
Loss: 0.0955
Fold 3 Epoch 051; Train loss: 0.0671; Time: 00:00:01
Fold 3 Epoch 052; Train loss: 0.0659; Time: 00:00:01
Fold 3 Epoch 053; Train loss: 0.0643; Time: 00:00:01
Fold 3 Epoch 054; Train loss: 0.0595; Time: 00:00:01
Fold 3 Epoch 055; Train loss: 0.0578; Time: 00:00:01
Fold 3 Epoch 056; Train loss: 0.0557; Time: 00:00:01
Fold 3 Epoch 057; Train loss: 0.0531; Time: 00:00:01
Fold 3 Epoch 058; Train loss: 0.0511; Time: 00:00:01
Fold 3 Epoch 059; Train loss: 0.0495; Time: 00:00:01
Fold 3 Epoch 060; Train loss: 0.0463; Time: 00:00:01
Fold 3: Evaluation at Epoch 60
HR@5       NDCG@5     HR@10      NDCG@10   
0.519868   0.291534   0.794702   0.380526  
Loss: 0.0663
Fold 3 Epoch 061; Train loss: 0.0451; Time: 00:00:01
Fold 3 Epoch 062; Train loss: 0.0426; Time: 00:00:01
Fold 3 Epoch 063; Train loss: 0.0406; Time: 00:00:01
Fold 3 Epoch 064; Train loss: 0.0391; Time: 00:00:02
Fold 3 Epoch 065; Train loss: 0.0374; Time: 00:00:01
Fold 3 Epoch 066; Train loss: 0.0349; Time: 00:00:01
Fold 3 Epoch 067; Train loss: 0.0343; Time: 00:00:01
Fold 3 Epoch 068; Train loss: 0.0318; Time: 00:00:01
Fold 3 Epoch 069; Train loss: 0.0305; Time: 00:00:02
Fold 3 Epoch 070; Train loss: 0.0287; Time: 00:00:01
Fold 3: Evaluation at Epoch 70
HR@5       NDCG@5     HR@10      NDCG@10   
0.678808   0.379331   0.887417   0.447544  
Loss: 0.0443
Fold 3 Epoch 071; Train loss: 0.0277; Time: 00:00:02
Fold 3 Epoch 072; Train loss: 0.0258; Time: 00:00:01
Fold 3 Epoch 073; Train loss: 0.0248; Time: 00:00:02
Fold 3 Epoch 074; Train loss: 0.0230; Time: 00:00:02
Fold 3 Epoch 075; Train loss: 0.0221; Time: 00:00:02
Fold 3 Epoch 076; Train loss: 0.0216; Time: 00:00:02
Fold 3 Epoch 077; Train loss: 0.0211; Time: 00:00:02
Fold 3 Epoch 078; Train loss: 0.0191; Time: 00:00:02
Fold 3 Epoch 079; Train loss: 0.0183; Time: 00:00:02
Fold 3 Epoch 080; Train loss: 0.0169; Time: 00:00:01
Fold 3: Evaluation at Epoch 80
HR@5       NDCG@5     HR@10      NDCG@10   
0.690397   0.425736   0.882450   0.489723  
Loss: 0.0297
Fold 3 Epoch 081; Train loss: 0.0163; Time: 00:00:01
Fold 3 Epoch 082; Train loss: 0.0152; Time: 00:00:01
Fold 3 Epoch 083; Train loss: 0.0147; Time: 00:00:01
Fold 3 Epoch 084; Train loss: 0.0139; Time: 00:00:01
Fold 3 Epoch 085; Train loss: 0.0131; Time: 00:00:01
Fold 3 Epoch 086; Train loss: 0.0126; Time: 00:00:01
Fold 3 Epoch 087; Train loss: 0.0117; Time: 00:00:02
Fold 3 Epoch 088; Train loss: 0.0108; Time: 00:00:02
Fold 3 Epoch 089; Train loss: 0.0105; Time: 00:00:02
Fold 3 Epoch 090; Train loss: 0.0101; Time: 00:00:02
Fold 3: Evaluation at Epoch 90
HR@5       NDCG@5     HR@10      NDCG@10   
0.756623   0.438374   0.879139   0.479555  
Loss: 0.0193
Fold 3 Epoch 091; Train loss: 0.0091; Time: 00:00:02
Fold 3 Epoch 092; Train loss: 0.0087; Time: 00:00:02
Fold 3 Epoch 093; Train loss: 0.0084; Time: 00:00:02
Fold 3 Epoch 094; Train loss: 0.0076; Time: 00:00:01
Fold 3 Epoch 095; Train loss: 0.0073; Time: 00:00:02
Fold 3 Epoch 096; Train loss: 0.0068; Time: 00:00:01
Fold 3 Epoch 097; Train loss: 0.0063; Time: 00:00:01
Fold 3 Epoch 098; Train loss: 0.0061; Time: 00:00:02
Fold 3 Epoch 099; Train loss: 0.0056; Time: 00:00:01
Fold 3 Epoch 100; Train loss: 0.0052; Time: 00:00:02
Fold 3: Evaluation at Epoch 100
HR@5       NDCG@5     HR@10      NDCG@10   
0.801325   0.420468   0.872517   0.444353  
Loss: 0.0137
[lr candidate 0.005] Fold 3: HR@10 = [np.float64(0.7731788079470199), np.float64(0.7533112582781457), np.float64(0.7814569536423841), np.float64(0.7814569536423841), np.float64(0.7665562913907285), np.float64(0.7947019867549668), np.float64(0.8874172185430463), np.float64(0.8824503311258278), np.float64(0.8791390728476821), np.float64(0.8725165562913907)]
Tuning lr:  80%|████████  | 4/5 [15:26<03:51, 231.53s/it]
========== Fold 3 ==========
INFO:root:Using statics: seq_size = 10, genre_vocab_size = 18
Fold 3 Epoch 001; Train loss: 0.9742; Time: 00:00:01
Fold 3 Epoch 002; Train loss: 0.7493; Time: 00:00:01
Fold 3 Epoch 003; Train loss: 0.6473; Time: 00:00:01
Fold 3 Epoch 004; Train loss: 0.5720; Time: 00:00:01
Fold 3 Epoch 005; Train loss: 0.5005; Time: 00:00:01
Fold 3 Epoch 006; Train loss: 0.4556; Time: 00:00:02
Fold 3 Epoch 007; Train loss: 0.4282; Time: 00:00:01
Fold 3 Epoch 008; Train loss: 0.3944; Time: 00:00:01
Fold 3 Epoch 009; Train loss: 0.3766; Time: 00:00:02
Fold 3 Epoch 010; Train loss: 0.3628; Time: 00:00:02
Fold 3: Evaluation at Epoch 10
HR@5       NDCG@5     HR@10      NDCG@10   
0.637417   0.475216   0.784768   0.523028  
Loss: 0.5102
Fold 3 Epoch 011; Train loss: 0.3397; Time: 00:00:02
Fold 3 Epoch 012; Train loss: 0.3326; Time: 00:00:02
Fold 3 Epoch 013; Train loss: 0.3253; Time: 00:00:02
Fold 3 Epoch 014; Train loss: 0.3163; Time: 00:00:02
Fold 3 Epoch 015; Train loss: 0.3013; Time: 00:00:02
Fold 3 Epoch 016; Train loss: 0.3004; Time: 00:00:02
Fold 3 Epoch 017; Train loss: 0.2893; Time: 00:00:02
Fold 3 Epoch 018; Train loss: 0.2845; Time: 00:00:02
Fold 3 Epoch 019; Train loss: 0.2821; Time: 00:00:02
Fold 3 Epoch 020; Train loss: 0.2793; Time: 00:00:02
Fold 3: Evaluation at Epoch 20
HR@5       NDCG@5     HR@10      NDCG@10   
0.652318   0.513867   0.809603   0.564280  
Loss: 0.3811
Fold 3 Epoch 021; Train loss: 0.2747; Time: 00:00:02
Fold 3 Epoch 022; Train loss: 0.2686; Time: 00:00:02
Fold 3 Epoch 023; Train loss: 0.2688; Time: 00:00:02
Fold 3 Epoch 024; Train loss: 0.2552; Time: 00:00:02
Fold 3 Epoch 025; Train loss: 0.2581; Time: 00:00:02
Fold 3 Epoch 026; Train loss: 0.2551; Time: 00:00:02
Fold 3 Epoch 027; Train loss: 0.2555; Time: 00:00:02
Fold 3 Epoch 028; Train loss: 0.2476; Time: 00:00:02
Fold 3 Epoch 029; Train loss: 0.2490; Time: 00:00:02
Fold 3 Epoch 030; Train loss: 0.2446; Time: 00:00:02
Fold 3: Evaluation at Epoch 30
HR@5       NDCG@5     HR@10      NDCG@10   
0.650662   0.517961   0.804636   0.567062  
Loss: 0.3355
Fold 3 Epoch 031; Train loss: 0.2500; Time: 00:00:02
Fold 3 Epoch 032; Train loss: 0.2453; Time: 00:00:02
Fold 3 Epoch 033; Train loss: 0.2367; Time: 00:00:02
Fold 3 Epoch 034; Train loss: 0.2342; Time: 00:00:02
Fold 3 Epoch 035; Train loss: 0.2367; Time: 00:00:02
Fold 3 Epoch 036; Train loss: 0.2344; Time: 00:00:02
Fold 3 Epoch 037; Train loss: 0.2266; Time: 00:00:02
Fold 3 Epoch 038; Train loss: 0.2345; Time: 00:00:02
Fold 3 Epoch 039; Train loss: 0.2328; Time: 00:00:02
Fold 3 Epoch 040; Train loss: 0.2299; Time: 00:00:02
Fold 3: Evaluation at Epoch 40
HR@5       NDCG@5     HR@10      NDCG@10   
0.640728   0.524004   0.794702   0.573495  
Loss: 0.3020
Fold 3 Epoch 041; Train loss: 0.2275; Time: 00:00:02
Fold 3 Epoch 042; Train loss: 0.2257; Time: 00:00:02
Fold 3 Epoch 043; Train loss: 0.2278; Time: 00:00:02
Fold 3 Epoch 044; Train loss: 0.2215; Time: 00:00:02
Fold 3 Epoch 045; Train loss: 0.2174; Time: 00:00:02
Fold 3 Epoch 046; Train loss: 0.2219; Time: 00:00:02
Fold 3 Epoch 047; Train loss: 0.2151; Time: 00:00:02
Fold 3 Epoch 048; Train loss: 0.2178; Time: 00:00:02
Fold 3 Epoch 049; Train loss: 0.2162; Time: 00:00:02
Fold 3 Epoch 050; Train loss: 0.2142; Time: 00:00:02
Fold 3: Evaluation at Epoch 50
HR@5       NDCG@5     HR@10      NDCG@10   
0.662252   0.526470   0.804636   0.572702  
Loss: 0.2944
Fold 3 Epoch 051; Train loss: 0.2125; Time: 00:00:02
Fold 3 Epoch 052; Train loss: 0.2125; Time: 00:00:02
Fold 3 Epoch 053; Train loss: 0.2116; Time: 00:00:02
Fold 3 Epoch 054; Train loss: 0.2041; Time: 00:00:02
Fold 3 Epoch 055; Train loss: 0.2139; Time: 00:00:02
Fold 3 Epoch 056; Train loss: 0.2039; Time: 00:00:02
Fold 3 Epoch 057; Train loss: 0.2099; Time: 00:00:01
Fold 3 Epoch 058; Train loss: 0.1988; Time: 00:00:01
Fold 3 Epoch 059; Train loss: 0.2074; Time: 00:00:02
Fold 3 Epoch 060; Train loss: 0.2059; Time: 00:00:01
Fold 3: Evaluation at Epoch 60
HR@5       NDCG@5     HR@10      NDCG@10   
0.649007   0.523609   0.811258   0.576097  
Loss: 0.2807
Fold 3 Epoch 061; Train loss: 0.2046; Time: 00:00:01
Fold 3 Epoch 062; Train loss: 0.1967; Time: 00:00:01
Fold 3 Epoch 063; Train loss: 0.2044; Time: 00:00:01
Fold 3 Epoch 064; Train loss: 0.1972; Time: 00:00:01
Fold 3 Epoch 065; Train loss: 0.1990; Time: 00:00:01
Fold 3 Epoch 066; Train loss: 0.1966; Time: 00:00:01
Fold 3 Epoch 067; Train loss: 0.1941; Time: 00:00:01
Fold 3 Epoch 068; Train loss: 0.1949; Time: 00:00:01
Fold 3 Epoch 069; Train loss: 0.1916; Time: 00:00:01
Fold 3 Epoch 070; Train loss: 0.1903; Time: 00:00:02
Fold 3: Evaluation at Epoch 70
HR@5       NDCG@5     HR@10      NDCG@10   
0.682119   0.544941   0.817881   0.588667  
Loss: 0.2563
Fold 3 Epoch 071; Train loss: 0.1909; Time: 00:00:01
Fold 3 Epoch 072; Train loss: 0.1889; Time: 00:00:02
Fold 3 Epoch 073; Train loss: 0.1885; Time: 00:00:01
Fold 3 Epoch 074; Train loss: 0.1870; Time: 00:00:01
Fold 3 Epoch 075; Train loss: 0.1897; Time: 00:00:01
Fold 3 Epoch 076; Train loss: 0.1839; Time: 00:00:01
Fold 3 Epoch 077; Train loss: 0.1852; Time: 00:00:01
Fold 3 Epoch 078; Train loss: 0.1812; Time: 00:00:01
Fold 3 Epoch 079; Train loss: 0.1828; Time: 00:00:01
Fold 3 Epoch 080; Train loss: 0.1858; Time: 00:00:01
Fold 3: Evaluation at Epoch 80
HR@5       NDCG@5     HR@10      NDCG@10   
0.658940   0.541147   0.801325   0.586997  
Loss: 0.2560
Fold 3 Epoch 081; Train loss: 0.1825; Time: 00:00:01
Fold 3 Epoch 082; Train loss: 0.1835; Time: 00:00:01
Fold 3 Epoch 083; Train loss: 0.1771; Time: 00:00:01
Fold 3 Epoch 084; Train loss: 0.1831; Time: 00:00:01
Fold 3 Epoch 085; Train loss: 0.1817; Time: 00:00:02
Fold 3 Epoch 086; Train loss: 0.1781; Time: 00:00:02
Fold 3 Epoch 087; Train loss: 0.1764; Time: 00:00:01
Fold 3 Epoch 088; Train loss: 0.1747; Time: 00:00:01
Fold 3 Epoch 089; Train loss: 0.1741; Time: 00:00:02
Fold 3 Epoch 090; Train loss: 0.1706; Time: 00:00:02
Fold 3: Evaluation at Epoch 90
HR@5       NDCG@5     HR@10      NDCG@10   
0.697020   0.545715   0.834437   0.590874  
Loss: 0.2413
Fold 3 Epoch 091; Train loss: 0.1701; Time: 00:00:01
Fold 3 Epoch 092; Train loss: 0.1675; Time: 00:00:01
Fold 3 Epoch 093; Train loss: 0.1714; Time: 00:00:01
Fold 3 Epoch 094; Train loss: 0.1698; Time: 00:00:01
Fold 3 Epoch 095; Train loss: 0.1674; Time: 00:00:02
Fold 3 Epoch 096; Train loss: 0.1674; Time: 00:00:02
Fold 3 Epoch 097; Train loss: 0.1670; Time: 00:00:02
Fold 3 Epoch 098; Train loss: 0.1711; Time: 00:00:01
Fold 3 Epoch 099; Train loss: 0.1628; Time: 00:00:01
Fold 3 Epoch 100; Train loss: 0.1637; Time: 00:00:01
Fold 3: Evaluation at Epoch 100
HR@5       NDCG@5     HR@10      NDCG@10   
0.653974   0.530259   0.796358   0.576466  
Loss: 0.2325
[lr candidate 0.001] Fold 3: HR@10 = [np.float64(0.7847682119205298), np.float64(0.8096026490066225), np.float64(0.804635761589404), np.float64(0.7947019867549668), np.float64(0.804635761589404), np.float64(0.8112582781456954), np.float64(0.8178807947019867), np.float64(0.8013245033112583), np.float64(0.8344370860927153), np.float64(0.7963576158940397)]
Tuning lr: 100%|██████████| 5/5 [19:18<00:00, 231.79s/it]Tuning lr: 100%|██████████| 5/5 [19:18<00:00, 231.67s/it]

Best learning rate found: 0.005
Tuning optimizer:   0%|          | 0/4 [00:00<?, ?it/s]
========== Fold 3 ==========
INFO:root:Using statics: seq_size = 10, genre_vocab_size = 18
Fold 3 Epoch 001; Train loss: 0.8368; Time: 00:00:01
Fold 3 Epoch 002; Train loss: 0.4904; Time: 00:00:01
Fold 3 Epoch 003; Train loss: 0.3779; Time: 00:00:01
Fold 3 Epoch 004; Train loss: 0.3261; Time: 00:00:01
Fold 3 Epoch 005; Train loss: 0.3058; Time: 00:00:01
Fold 3 Epoch 006; Train loss: 0.2755; Time: 00:00:01
Fold 3 Epoch 007; Train loss: 0.2633; Time: 00:00:01
Fold 3 Epoch 008; Train loss: 0.2552; Time: 00:00:01
Fold 3 Epoch 009; Train loss: 0.2403; Time: 00:00:01
Fold 3 Epoch 010; Train loss: 0.2347; Time: 00:00:01
Fold 3: Evaluation at Epoch 10
HR@5       NDCG@5     HR@10      NDCG@10   
0.493377   0.380937   0.622517   0.421877  
Loss: 0.2982
Fold 3 Epoch 011; Train loss: 0.2314; Time: 00:00:01
Fold 3 Epoch 012; Train loss: 0.2221; Time: 00:00:01
Fold 3 Epoch 013; Train loss: 0.2192; Time: 00:00:02
Fold 3 Epoch 014; Train loss: 0.2070; Time: 00:00:02
Fold 3 Epoch 015; Train loss: 0.1997; Time: 00:00:02
Fold 3 Epoch 016; Train loss: 0.2015; Time: 00:00:02
Fold 3 Epoch 017; Train loss: 0.1940; Time: 00:00:02
Fold 3 Epoch 018; Train loss: 0.1923; Time: 00:00:02
Fold 3 Epoch 019; Train loss: 0.1817; Time: 00:00:02
Fold 3 Epoch 020; Train loss: 0.1768; Time: 00:00:02
Fold 3: Evaluation at Epoch 20
HR@5       NDCG@5     HR@10      NDCG@10   
0.519868   0.363513   0.711921   0.424392  
Loss: 0.2324
Fold 3 Epoch 021; Train loss: 0.1730; Time: 00:00:02
Fold 3 Epoch 022; Train loss: 0.1641; Time: 00:00:02
Fold 3 Epoch 023; Train loss: 0.1657; Time: 00:00:02
Fold 3 Epoch 024; Train loss: 0.1583; Time: 00:00:02
Fold 3 Epoch 025; Train loss: 0.1541; Time: 00:00:02
Fold 3 Epoch 026; Train loss: 0.1518; Time: 00:00:01
Fold 3 Epoch 027; Train loss: 0.1486; Time: 00:00:01
Fold 3 Epoch 028; Train loss: 0.1445; Time: 00:00:01
Fold 3 Epoch 029; Train loss: 0.1384; Time: 00:00:01
Fold 3 Epoch 030; Train loss: 0.1374; Time: 00:00:01
Fold 3: Evaluation at Epoch 30
HR@5       NDCG@5     HR@10      NDCG@10   
0.582781   0.401947   0.754967   0.457423  
Loss: 0.1979
Fold 3 Epoch 031; Train loss: 0.1353; Time: 00:00:02
Fold 3 Epoch 032; Train loss: 0.1298; Time: 00:00:02
Fold 3 Epoch 033; Train loss: 0.1239; Time: 00:00:01
Fold 3 Epoch 034; Train loss: 0.1239; Time: 00:00:01
Fold 3 Epoch 035; Train loss: 0.1196; Time: 00:00:01
Fold 3 Epoch 036; Train loss: 0.1137; Time: 00:00:01
Fold 3 Epoch 037; Train loss: 0.1113; Time: 00:00:02
Fold 3 Epoch 038; Train loss: 0.1072; Time: 00:00:02
Fold 3 Epoch 039; Train loss: 0.1041; Time: 00:00:01
Fold 3 Epoch 040; Train loss: 0.0996; Time: 00:00:01
Fold 3: Evaluation at Epoch 40
HR@5       NDCG@5     HR@10      NDCG@10   
0.447020   0.336870   0.680464   0.411710  
Loss: 0.1317
Fold 3 Epoch 041; Train loss: 0.0988; Time: 00:00:01
Fold 3 Epoch 042; Train loss: 0.0951; Time: 00:00:01
Fold 3 Epoch 043; Train loss: 0.0908; Time: 00:00:02
Fold 3 Epoch 044; Train loss: 0.0879; Time: 00:00:01
Fold 3 Epoch 045; Train loss: 0.0860; Time: 00:00:01
Fold 3 Epoch 046; Train loss: 0.0831; Time: 00:00:01
Fold 3 Epoch 047; Train loss: 0.0808; Time: 00:00:02
Fold 3 Epoch 048; Train loss: 0.0787; Time: 00:00:02
Fold 3 Epoch 049; Train loss: 0.0753; Time: 00:00:02
Fold 3 Epoch 050; Train loss: 0.0727; Time: 00:00:02
Fold 3: Evaluation at Epoch 50
HR@5       NDCG@5     HR@10      NDCG@10   
0.548013   0.408199   0.720199   0.462430  
Loss: 0.0951
Fold 3 Epoch 051; Train loss: 0.0682; Time: 00:00:02
Fold 3 Epoch 052; Train loss: 0.0667; Time: 00:00:01
Fold 3 Epoch 053; Train loss: 0.0644; Time: 00:00:01
Fold 3 Epoch 054; Train loss: 0.0611; Time: 00:00:01
Fold 3 Epoch 055; Train loss: 0.0601; Time: 00:00:01
Fold 3 Epoch 056; Train loss: 0.0570; Time: 00:00:01
Fold 3 Epoch 057; Train loss: 0.0546; Time: 00:00:02
Fold 3 Epoch 058; Train loss: 0.0536; Time: 00:00:02
Fold 3 Epoch 059; Train loss: 0.0511; Time: 00:00:02
Fold 3 Epoch 060; Train loss: 0.0474; Time: 00:00:01
Fold 3: Evaluation at Epoch 60
HR@5       NDCG@5     HR@10      NDCG@10   
0.564570   0.395176   0.773179   0.462327  
Loss: 0.0660
Fold 3 Epoch 061; Train loss: 0.0456; Time: 00:00:02
Fold 3 Epoch 062; Train loss: 0.0437; Time: 00:00:01
Fold 3 Epoch 063; Train loss: 0.0427; Time: 00:00:01
Fold 3 Epoch 064; Train loss: 0.0402; Time: 00:00:01
Fold 3 Epoch 065; Train loss: 0.0379; Time: 00:00:01
Fold 3 Epoch 066; Train loss: 0.0364; Time: 00:00:01
Fold 3 Epoch 067; Train loss: 0.0342; Time: 00:00:01
Fold 3 Epoch 068; Train loss: 0.0327; Time: 00:00:01
Fold 3 Epoch 069; Train loss: 0.0310; Time: 00:00:01
Fold 3 Epoch 070; Train loss: 0.0294; Time: 00:00:01
Fold 3: Evaluation at Epoch 70
HR@5       NDCG@5     HR@10      NDCG@10   
0.607616   0.413859   0.884106   0.506970  
Loss: 0.0432
Fold 3 Epoch 071; Train loss: 0.0279; Time: 00:00:02
Fold 3 Epoch 072; Train loss: 0.0264; Time: 00:00:01
Fold 3 Epoch 073; Train loss: 0.0252; Time: 00:00:01
Fold 3 Epoch 074; Train loss: 0.0236; Time: 00:00:02
Fold 3 Epoch 075; Train loss: 0.0225; Time: 00:00:01
Fold 3 Epoch 076; Train loss: 0.0204; Time: 00:00:01
Fold 3 Epoch 077; Train loss: 0.0196; Time: 00:00:01
Fold 3 Epoch 078; Train loss: 0.0191; Time: 00:00:01
Fold 3 Epoch 079; Train loss: 0.0175; Time: 00:00:01
Fold 3 Epoch 080; Train loss: 0.0165; Time: 00:00:01
Fold 3: Evaluation at Epoch 80
HR@5       NDCG@5     HR@10      NDCG@10   
0.509934   0.378766   0.899007   0.508994  
Loss: 0.0269
Fold 3 Epoch 081; Train loss: 0.0158; Time: 00:00:02
Fold 3 Epoch 082; Train loss: 0.0150; Time: 00:00:02
Fold 3 Epoch 083; Train loss: 0.0138; Time: 00:00:01
Fold 3 Epoch 084; Train loss: 0.0129; Time: 00:00:01
Fold 3 Epoch 085; Train loss: 0.0119; Time: 00:00:01
Fold 3 Epoch 086; Train loss: 0.0112; Time: 00:00:01
Fold 3 Epoch 087; Train loss: 0.0107; Time: 00:00:01
Fold 3 Epoch 088; Train loss: 0.0100; Time: 00:00:01
Fold 3 Epoch 089; Train loss: 0.0092; Time: 00:00:02
Fold 3 Epoch 090; Train loss: 0.0087; Time: 00:00:01
Fold 3: Evaluation at Epoch 90
HR@5       NDCG@5     HR@10      NDCG@10   
0.581126   0.403144   0.905629   0.509952  
Loss: 0.0154
Fold 3 Epoch 091; Train loss: 0.0083; Time: 00:00:01
Fold 3 Epoch 092; Train loss: 0.0077; Time: 00:00:01
Fold 3 Epoch 093; Train loss: 0.0071; Time: 00:00:01
Fold 3 Epoch 094; Train loss: 0.0066; Time: 00:00:01
Fold 3 Epoch 095; Train loss: 0.0066; Time: 00:00:01
Fold 3 Epoch 096; Train loss: 0.0061; Time: 00:00:01
Fold 3 Epoch 097; Train loss: 0.0054; Time: 00:00:01
Fold 3 Epoch 098; Train loss: 0.0053; Time: 00:00:02
Fold 3 Epoch 099; Train loss: 0.0049; Time: 00:00:02
Fold 3 Epoch 100; Train loss: 0.0047; Time: 00:00:02
Fold 3: Evaluation at Epoch 100
HR@5       NDCG@5     HR@10      NDCG@10   
0.564570   0.403428   0.874172   0.505643  
Loss: 0.0098
[optimizer candidate nadam] Fold 3: HR@10 = [np.float64(0.6225165562913907), np.float64(0.7119205298013245), np.float64(0.7549668874172185), np.float64(0.6804635761589404), np.float64(0.7201986754966887), np.float64(0.7731788079470199), np.float64(0.8841059602649006), np.float64(0.8990066225165563), np.float64(0.9056291390728477), np.float64(0.8741721854304636)]
Tuning optimizer:  25%|██▌       | 1/4 [03:39<10:57, 219.04s/it]
========== Fold 3 ==========
INFO:root:Using statics: seq_size = 10, genre_vocab_size = 18
Fold 3 Epoch 001; Train loss: 1.1671; Time: 00:00:02
Fold 3 Epoch 002; Train loss: 1.0149; Time: 00:00:02
Fold 3 Epoch 003; Train loss: 0.9040; Time: 00:00:02
Fold 3 Epoch 004; Train loss: 0.8300; Time: 00:00:02
Fold 3 Epoch 005; Train loss: 0.7583; Time: 00:00:02
Fold 3 Epoch 006; Train loss: 0.6953; Time: 00:00:02
Fold 3 Epoch 007; Train loss: 0.6439; Time: 00:00:02
Fold 3 Epoch 008; Train loss: 0.5939; Time: 00:00:02
Fold 3 Epoch 009; Train loss: 0.5532; Time: 00:00:02
Fold 3 Epoch 010; Train loss: 0.5163; Time: 00:00:02
Fold 3: Evaluation at Epoch 10
HR@5       NDCG@5     HR@10      NDCG@10   
0.725166   0.553008   0.831126   0.586596  
Loss: 0.6552
Fold 3 Epoch 011; Train loss: 0.4812; Time: 00:00:01
Fold 3 Epoch 012; Train loss: 0.4444; Time: 00:00:01
Fold 3 Epoch 013; Train loss: 0.4235; Time: 00:00:01
Fold 3 Epoch 014; Train loss: 0.3968; Time: 00:00:01
Fold 3 Epoch 015; Train loss: 0.3696; Time: 00:00:01
Fold 3 Epoch 016; Train loss: 0.3518; Time: 00:00:02
Fold 3 Epoch 017; Train loss: 0.3347; Time: 00:00:01
Fold 3 Epoch 018; Train loss: 0.3149; Time: 00:00:01
Fold 3 Epoch 019; Train loss: 0.3002; Time: 00:00:02
Fold 3 Epoch 020; Train loss: 0.2869; Time: 00:00:02
Fold 3: Evaluation at Epoch 20
HR@5       NDCG@5     HR@10      NDCG@10   
0.652318   0.514511   0.764901   0.550250  
Loss: 0.3863
Fold 3 Epoch 021; Train loss: 0.2733; Time: 00:00:02
Fold 3 Epoch 022; Train loss: 0.2630; Time: 00:00:02
Fold 3 Epoch 023; Train loss: 0.2484; Time: 00:00:02
Fold 3 Epoch 024; Train loss: 0.2446; Time: 00:00:02
Fold 3 Epoch 025; Train loss: 0.2357; Time: 00:00:02
Fold 3 Epoch 026; Train loss: 0.2226; Time: 00:00:02
Fold 3 Epoch 027; Train loss: 0.2177; Time: 00:00:02
Fold 3 Epoch 028; Train loss: 0.2116; Time: 00:00:02
Fold 3 Epoch 029; Train loss: 0.2007; Time: 00:00:02
Fold 3 Epoch 030; Train loss: 0.2018; Time: 00:00:02
Fold 3: Evaluation at Epoch 30
HR@5       NDCG@5     HR@10      NDCG@10   
0.559603   0.435310   0.713576   0.483800  
Loss: 0.2730
Fold 3 Epoch 031; Train loss: 0.1956; Time: 00:00:02
Fold 3 Epoch 032; Train loss: 0.1947; Time: 00:00:02
Fold 3 Epoch 033; Train loss: 0.1851; Time: 00:00:02
Fold 3 Epoch 034; Train loss: 0.1767; Time: 00:00:02
Fold 3 Epoch 035; Train loss: 0.1796; Time: 00:00:02
Fold 3 Epoch 036; Train loss: 0.1727; Time: 00:00:02
Fold 3 Epoch 037; Train loss: 0.1657; Time: 00:00:02
Fold 3 Epoch 038; Train loss: 0.1633; Time: 00:00:02
Fold 3 Epoch 039; Train loss: 0.1636; Time: 00:00:02
Fold 3 Epoch 040; Train loss: 0.1545; Time: 00:00:01
Fold 3: Evaluation at Epoch 40
HR@5       NDCG@5     HR@10      NDCG@10   
0.574503   0.447000   0.716887   0.492131  
Loss: 0.2035
Fold 3 Epoch 041; Train loss: 0.1556; Time: 00:00:01
Fold 3 Epoch 042; Train loss: 0.1495; Time: 00:00:02
Fold 3 Epoch 043; Train loss: 0.1443; Time: 00:00:02
Fold 3 Epoch 044; Train loss: 0.1461; Time: 00:00:02
Fold 3 Epoch 045; Train loss: 0.1358; Time: 00:00:02
Fold 3 Epoch 046; Train loss: 0.1342; Time: 00:00:02
Fold 3 Epoch 047; Train loss: 0.1342; Time: 00:00:02
Fold 3 Epoch 048; Train loss: 0.1296; Time: 00:00:02
Fold 3 Epoch 049; Train loss: 0.1254; Time: 00:00:02
Fold 3 Epoch 050; Train loss: 0.1275; Time: 00:00:02
Fold 3: Evaluation at Epoch 50
HR@5       NDCG@5     HR@10      NDCG@10   
0.599338   0.446731   0.754967   0.496035  
Loss: 0.1675
Fold 3 Epoch 051; Train loss: 0.1251; Time: 00:00:02
Fold 3 Epoch 052; Train loss: 0.1195; Time: 00:00:01
Fold 3 Epoch 053; Train loss: 0.1169; Time: 00:00:02
Fold 3 Epoch 054; Train loss: 0.1131; Time: 00:00:01
Fold 3 Epoch 055; Train loss: 0.1131; Time: 00:00:02
Fold 3 Epoch 056; Train loss: 0.1073; Time: 00:00:02
Fold 3 Epoch 057; Train loss: 0.1029; Time: 00:00:02
Fold 3 Epoch 058; Train loss: 0.1045; Time: 00:00:01
Fold 3 Epoch 059; Train loss: 0.1002; Time: 00:00:01
Fold 3 Epoch 060; Train loss: 0.0951; Time: 00:00:02
Fold 3: Evaluation at Epoch 60
HR@5       NDCG@5     HR@10      NDCG@10   
0.617550   0.459077   0.776490   0.509574  
Loss: 0.1400
Fold 3 Epoch 061; Train loss: 0.0939; Time: 00:00:02
Fold 3 Epoch 062; Train loss: 0.0959; Time: 00:00:02
Fold 3 Epoch 063; Train loss: 0.0912; Time: 00:00:02
Fold 3 Epoch 064; Train loss: 0.0887; Time: 00:00:02
Fold 3 Epoch 065; Train loss: 0.0861; Time: 00:00:02
Fold 3 Epoch 066; Train loss: 0.0840; Time: 00:00:02
Fold 3 Epoch 067; Train loss: 0.0827; Time: 00:00:02
Fold 3 Epoch 068; Train loss: 0.0777; Time: 00:00:02
Fold 3 Epoch 069; Train loss: 0.0768; Time: 00:00:02
Fold 3 Epoch 070; Train loss: 0.0742; Time: 00:00:02
Fold 3: Evaluation at Epoch 70
HR@5       NDCG@5     HR@10      NDCG@10   
0.614238   0.445015   0.806291   0.506683  
Loss: 0.1058
Fold 3 Epoch 071; Train loss: 0.0725; Time: 00:00:02
Fold 3 Epoch 072; Train loss: 0.0691; Time: 00:00:02
Fold 3 Epoch 073; Train loss: 0.0660; Time: 00:00:02
Fold 3 Epoch 074; Train loss: 0.0648; Time: 00:00:02
Fold 3 Epoch 075; Train loss: 0.0629; Time: 00:00:02
Fold 3 Epoch 076; Train loss: 0.0603; Time: 00:00:02
Fold 3 Epoch 077; Train loss: 0.0587; Time: 00:00:02
Fold 3 Epoch 078; Train loss: 0.0566; Time: 00:00:02
Fold 3 Epoch 079; Train loss: 0.0533; Time: 00:00:02
Fold 3 Epoch 080; Train loss: 0.0520; Time: 00:00:02
Fold 3: Evaluation at Epoch 80
HR@5       NDCG@5     HR@10      NDCG@10   
0.663907   0.454405   0.850993   0.515173  
Loss: 0.0797
Fold 3 Epoch 081; Train loss: 0.0505; Time: 00:00:02
Fold 3 Epoch 082; Train loss: 0.0495; Time: 00:00:02
Fold 3 Epoch 083; Train loss: 0.0465; Time: 00:00:02
Fold 3 Epoch 084; Train loss: 0.0442; Time: 00:00:01
Fold 3 Epoch 085; Train loss: 0.0430; Time: 00:00:01
Fold 3 Epoch 086; Train loss: 0.0418; Time: 00:00:02
Fold 3 Epoch 087; Train loss: 0.0407; Time: 00:00:02
Fold 3 Epoch 088; Train loss: 0.0377; Time: 00:00:02
Fold 3 Epoch 089; Train loss: 0.0352; Time: 00:00:02
Fold 3 Epoch 090; Train loss: 0.0335; Time: 00:00:02
Fold 3: Evaluation at Epoch 90
HR@5       NDCG@5     HR@10      NDCG@10   
0.740066   0.469395   0.867550   0.510373  
Loss: 0.0549
Fold 3 Epoch 091; Train loss: 0.0315; Time: 00:00:02
Fold 3 Epoch 092; Train loss: 0.0300; Time: 00:00:02
Fold 3 Epoch 093; Train loss: 0.0290; Time: 00:00:01
Fold 3 Epoch 094; Train loss: 0.0267; Time: 00:00:02
Fold 3 Epoch 095; Train loss: 0.0255; Time: 00:00:02
Fold 3 Epoch 096; Train loss: 0.0240; Time: 00:00:02
Fold 3 Epoch 097; Train loss: 0.0228; Time: 00:00:02
Fold 3 Epoch 098; Train loss: 0.0215; Time: 00:00:02
Fold 3 Epoch 099; Train loss: 0.0202; Time: 00:00:02
Fold 3 Epoch 100; Train loss: 0.0194; Time: 00:00:02
Fold 3: Evaluation at Epoch 100
HR@5       NDCG@5     HR@10      NDCG@10   
0.700331   0.386794   0.915563   0.456205  
Loss: 0.0343
[optimizer candidate lamb] Fold 3: HR@10 = [np.float64(0.8311258278145696), np.float64(0.7649006622516556), np.float64(0.7135761589403974), np.float64(0.7168874172185431), np.float64(0.7549668874172185), np.float64(0.7764900662251656), np.float64(0.8062913907284768), np.float64(0.8509933774834437), np.float64(0.8675496688741722), np.float64(0.9155629139072847)]
Tuning optimizer:  50%|█████     | 2/4 [07:44<07:48, 234.35s/it]
========== Fold 3 ==========
INFO:root:Using statics: seq_size = 10, genre_vocab_size = 18
Fold 3 Epoch 001; Train loss: 0.7939; Time: 00:00:02
Fold 3 Epoch 002; Train loss: 0.4509; Time: 00:00:02
Fold 3 Epoch 003; Train loss: 0.3491; Time: 00:00:02
Fold 3 Epoch 004; Train loss: 0.3108; Time: 00:00:02
Fold 3 Epoch 005; Train loss: 0.2854; Time: 00:00:02
Fold 3 Epoch 006; Train loss: 0.2684; Time: 00:00:02
Fold 3 Epoch 007; Train loss: 0.2549; Time: 00:00:01
Fold 3 Epoch 008; Train loss: 0.2449; Time: 00:00:02
Fold 3 Epoch 009; Train loss: 0.2413; Time: 00:00:02
Fold 3 Epoch 010; Train loss: 0.2316; Time: 00:00:02
Fold 3: Evaluation at Epoch 10
HR@5       NDCG@5     HR@10      NDCG@10   
0.693709   0.554182   0.802980   0.588843  
Loss: 0.3167
Fold 3 Epoch 011; Train loss: 0.2237; Time: 00:00:01
Fold 3 Epoch 012; Train loss: 0.2177; Time: 00:00:02
Fold 3 Epoch 013; Train loss: 0.2106; Time: 00:00:02
Fold 3 Epoch 014; Train loss: 0.2066; Time: 00:00:02
Fold 3 Epoch 015; Train loss: 0.2037; Time: 00:00:02
Fold 3 Epoch 016; Train loss: 0.1993; Time: 00:00:02
Fold 3 Epoch 017; Train loss: 0.1890; Time: 00:00:02
Fold 3 Epoch 018; Train loss: 0.1894; Time: 00:00:02
Fold 3 Epoch 019; Train loss: 0.1811; Time: 00:00:02
Fold 3 Epoch 020; Train loss: 0.1761; Time: 00:00:02
Fold 3: Evaluation at Epoch 20
HR@5       NDCG@5     HR@10      NDCG@10   
0.688742   0.534393   0.796358   0.569072  
Loss: 0.2343
Fold 3 Epoch 021; Train loss: 0.1757; Time: 00:00:02
Fold 3 Epoch 022; Train loss: 0.1698; Time: 00:00:02
Fold 3 Epoch 023; Train loss: 0.1667; Time: 00:00:02
Fold 3 Epoch 024; Train loss: 0.1635; Time: 00:00:02
Fold 3 Epoch 025; Train loss: 0.1572; Time: 00:00:02
Fold 3 Epoch 026; Train loss: 0.1534; Time: 00:00:02
Fold 3 Epoch 027; Train loss: 0.1485; Time: 00:00:02
Fold 3 Epoch 028; Train loss: 0.1447; Time: 00:00:02
Fold 3 Epoch 029; Train loss: 0.1436; Time: 00:00:02
Fold 3 Epoch 030; Train loss: 0.1395; Time: 00:00:02
Fold 3: Evaluation at Epoch 30
HR@5       NDCG@5     HR@10      NDCG@10   
0.682119   0.564464   0.811258   0.606745  
Loss: 0.1940
Fold 3 Epoch 031; Train loss: 0.1344; Time: 00:00:01
Fold 3 Epoch 032; Train loss: 0.1306; Time: 00:00:01
Fold 3 Epoch 033; Train loss: 0.1275; Time: 00:00:01
Fold 3 Epoch 034; Train loss: 0.1260; Time: 00:00:01
Fold 3 Epoch 035; Train loss: 0.1213; Time: 00:00:01
Fold 3 Epoch 036; Train loss: 0.1184; Time: 00:00:01
Fold 3 Epoch 037; Train loss: 0.1157; Time: 00:00:01
Fold 3 Epoch 038; Train loss: 0.1145; Time: 00:00:01
Fold 3 Epoch 039; Train loss: 0.1094; Time: 00:00:02
Fold 3 Epoch 040; Train loss: 0.1069; Time: 00:00:01
Fold 3: Evaluation at Epoch 40
HR@5       NDCG@5     HR@10      NDCG@10   
0.715232   0.577323   0.834437   0.615597  
Loss: 0.1480
Fold 3 Epoch 041; Train loss: 0.1038; Time: 00:00:02
Fold 3 Epoch 042; Train loss: 0.0985; Time: 00:00:01
Fold 3 Epoch 043; Train loss: 0.0984; Time: 00:00:01
Fold 3 Epoch 044; Train loss: 0.0951; Time: 00:00:02
Fold 3 Epoch 045; Train loss: 0.0925; Time: 00:00:02
Fold 3 Epoch 046; Train loss: 0.0869; Time: 00:00:02
Fold 3 Epoch 047; Train loss: 0.0846; Time: 00:00:02
Fold 3 Epoch 048; Train loss: 0.0833; Time: 00:00:02
Fold 3 Epoch 049; Train loss: 0.0793; Time: 00:00:02
Fold 3 Epoch 050; Train loss: 0.0787; Time: 00:00:02
Fold 3: Evaluation at Epoch 50
HR@5       NDCG@5     HR@10      NDCG@10   
0.725166   0.574651   0.832781   0.609723  
Loss: 0.1123
Fold 3 Epoch 051; Train loss: 0.0771; Time: 00:00:02
Fold 3 Epoch 052; Train loss: 0.0729; Time: 00:00:02
Fold 3 Epoch 053; Train loss: 0.0697; Time: 00:00:02
Fold 3 Epoch 054; Train loss: 0.0688; Time: 00:00:02
Fold 3 Epoch 055; Train loss: 0.0633; Time: 00:00:02
Fold 3 Epoch 056; Train loss: 0.0632; Time: 00:00:02
Fold 3 Epoch 057; Train loss: 0.0595; Time: 00:00:02
Fold 3 Epoch 058; Train loss: 0.0576; Time: 00:00:02
Fold 3 Epoch 059; Train loss: 0.0567; Time: 00:00:02
Fold 3 Epoch 060; Train loss: 0.0546; Time: 00:00:02
Fold 3: Evaluation at Epoch 60
HR@5       NDCG@5     HR@10      NDCG@10   
0.695364   0.548608   0.824503   0.590255  
Loss: 0.0806
Fold 3 Epoch 061; Train loss: 0.0513; Time: 00:00:02
Fold 3 Epoch 062; Train loss: 0.0496; Time: 00:00:02
Fold 3 Epoch 063; Train loss: 0.0482; Time: 00:00:02
Fold 3 Epoch 064; Train loss: 0.0467; Time: 00:00:02
Fold 3 Epoch 065; Train loss: 0.0442; Time: 00:00:02
Fold 3 Epoch 066; Train loss: 0.0426; Time: 00:00:01
Fold 3 Epoch 067; Train loss: 0.0406; Time: 00:00:01
Fold 3 Epoch 068; Train loss: 0.0387; Time: 00:00:02
Fold 3 Epoch 069; Train loss: 0.0373; Time: 00:00:02
Fold 3 Epoch 070; Train loss: 0.0350; Time: 00:00:02
Fold 3: Evaluation at Epoch 70
HR@5       NDCG@5     HR@10      NDCG@10   
0.667219   0.522628   0.826159   0.573870  
Loss: 0.0597
Fold 3 Epoch 071; Train loss: 0.0337; Time: 00:00:02
Fold 3 Epoch 072; Train loss: 0.0322; Time: 00:00:02
Fold 3 Epoch 073; Train loss: 0.0299; Time: 00:00:02
Fold 3 Epoch 074; Train loss: 0.0295; Time: 00:00:02
Fold 3 Epoch 075; Train loss: 0.0276; Time: 00:00:02
Fold 3 Epoch 076; Train loss: 0.0259; Time: 00:00:02
Fold 3 Epoch 077; Train loss: 0.0246; Time: 00:00:02
Fold 3 Epoch 078; Train loss: 0.0241; Time: 00:00:02
Fold 3 Epoch 079; Train loss: 0.0225; Time: 00:00:02
Fold 3 Epoch 080; Train loss: 0.0215; Time: 00:00:02
Fold 3: Evaluation at Epoch 80
HR@5       NDCG@5     HR@10      NDCG@10   
0.673841   0.521333   0.884106   0.590087  
Loss: 0.0409
Fold 3 Epoch 081; Train loss: 0.0198; Time: 00:00:02
Fold 3 Epoch 082; Train loss: 0.0186; Time: 00:00:02
Fold 3 Epoch 083; Train loss: 0.0184; Time: 00:00:02
Fold 3 Epoch 084; Train loss: 0.0170; Time: 00:00:02
Fold 3 Epoch 085; Train loss: 0.0160; Time: 00:00:02
Fold 3 Epoch 086; Train loss: 0.0149; Time: 00:00:02
Fold 3 Epoch 087; Train loss: 0.0144; Time: 00:00:02
Fold 3 Epoch 088; Train loss: 0.0134; Time: 00:00:02
Fold 3 Epoch 089; Train loss: 0.0126; Time: 00:00:02
Fold 3 Epoch 090; Train loss: 0.0122; Time: 00:00:02
Fold 3: Evaluation at Epoch 90
HR@5       NDCG@5     HR@10      NDCG@10   
0.564570   0.448553   0.900662   0.558823  
Loss: 0.0284
Fold 3 Epoch 091; Train loss: 0.0111; Time: 00:00:02
Fold 3 Epoch 092; Train loss: 0.0109; Time: 00:00:02
Fold 3 Epoch 093; Train loss: 0.0099; Time: 00:00:02
Fold 3 Epoch 094; Train loss: 0.0096; Time: 00:00:02
Fold 3 Epoch 095; Train loss: 0.0085; Time: 00:00:02
Fold 3 Epoch 096; Train loss: 0.0083; Time: 00:00:02
Fold 3 Epoch 097; Train loss: 0.0078; Time: 00:00:02
Fold 3 Epoch 098; Train loss: 0.0072; Time: 00:00:02
Fold 3 Epoch 099; Train loss: 0.0071; Time: 00:00:02
Fold 3 Epoch 100; Train loss: 0.0064; Time: 00:00:02
Fold 3: Evaluation at Epoch 100
HR@5       NDCG@5     HR@10      NDCG@10   
0.592715   0.420959   0.885762   0.518140  
Loss: 0.0195
[optimizer candidate adamw] Fold 3: HR@10 = [np.float64(0.8029801324503312), np.float64(0.7963576158940397), np.float64(0.8112582781456954), np.float64(0.8344370860927153), np.float64(0.8327814569536424), np.float64(0.8245033112582781), np.float64(0.8261589403973509), np.float64(0.8841059602649006), np.float64(0.9006622516556292), np.float64(0.8857615894039735)]
Tuning optimizer:  75%|███████▌  | 3/4 [11:52<04:00, 240.54s/it]
========== Fold 3 ==========
INFO:root:Using statics: seq_size = 10, genre_vocab_size = 18
[31mPlease check your arguments if you have upgraded adabelief-pytorch from version 0.0.5.
[31mModifications to default arguments:
[31m                           eps  weight_decouple    rectify
-----------------------  -----  -----------------  ---------
adabelief-pytorch=0.0.5  1e-08  False              False
>=0.1.0 (Current 0.2.0)  1e-16  True               True
[34mSGD better than Adam (e.g. CNN for Image Classification)    Adam better than SGD (e.g. Transformer, GAN)
----------------------------------------------------------  ----------------------------------------------
Recommended eps = 1e-8                                      Recommended eps = 1e-16
[34mFor a complete table of recommended hyperparameters, see
[34mhttps://github.com/juntang-zhuang/Adabelief-Optimizer
[32mYou can disable the log message by setting "print_change_log = False", though it is recommended to keep as a reminder.
[0m
Weight decoupling enabled in AdaBelief
Rectification enabled in AdaBelief
Fold 3 Epoch 001; Train loss: 1.0891; Time: 00:00:02
Fold 3 Epoch 002; Train loss: 1.0796; Time: 00:00:02
Fold 3 Epoch 003; Train loss: 1.0777; Time: 00:00:02
Fold 3 Epoch 004; Train loss: 1.0703; Time: 00:00:02
Fold 3 Epoch 005; Train loss: 1.0665; Time: 00:00:02
Fold 3 Epoch 006; Train loss: 1.0584; Time: 00:00:02
Fold 3 Epoch 007; Train loss: 1.0508; Time: 00:00:02
Fold 3 Epoch 008; Train loss: 1.0415; Time: 00:00:02
Fold 3 Epoch 009; Train loss: 1.0354; Time: 00:00:02
Fold 3 Epoch 010; Train loss: 1.0261; Time: 00:00:02
Fold 3: Evaluation at Epoch 10
HR@5       NDCG@5     HR@10      NDCG@10   
0.395695   0.238657   0.652318   0.323571  
Loss: 1.1787
Fold 3 Epoch 011; Train loss: 1.0218; Time: 00:00:02
Fold 3 Epoch 012; Train loss: 1.0087; Time: 00:00:02
Fold 3 Epoch 013; Train loss: 0.9993; Time: 00:00:02
Fold 3 Epoch 014; Train loss: 0.9895; Time: 00:00:02
Fold 3 Epoch 015; Train loss: 0.9784; Time: 00:00:02
Fold 3 Epoch 016; Train loss: 0.9697; Time: 00:00:02
Fold 3 Epoch 017; Train loss: 0.9590; Time: 00:00:02
Fold 3 Epoch 018; Train loss: 0.9543; Time: 00:00:02
Fold 3 Epoch 019; Train loss: 0.9427; Time: 00:00:02
Fold 3 Epoch 020; Train loss: 0.9326; Time: 00:00:02
Fold 3: Evaluation at Epoch 20
HR@5       NDCG@5     HR@10      NDCG@10   
0.423841   0.321782   0.663907   0.399390  
Loss: 1.1070
Fold 3 Epoch 021; Train loss: 0.9240; Time: 00:00:02
Fold 3 Epoch 022; Train loss: 0.9158; Time: 00:00:02
Fold 3 Epoch 023; Train loss: 0.9074; Time: 00:00:02
Fold 3 Epoch 024; Train loss: 0.8942; Time: 00:00:02
Fold 3 Epoch 025; Train loss: 0.8890; Time: 00:00:02
Fold 3 Epoch 026; Train loss: 0.8792; Time: 00:00:02
Fold 3 Epoch 027; Train loss: 0.8718; Time: 00:00:02
Fold 3 Epoch 028; Train loss: 0.8637; Time: 00:00:02
Fold 3 Epoch 029; Train loss: 0.8547; Time: 00:00:02
Fold 3 Epoch 030; Train loss: 0.8430; Time: 00:00:02
Fold 3: Evaluation at Epoch 30
HR@5       NDCG@5     HR@10      NDCG@10   
0.443709   0.342698   0.692053   0.422125  
Loss: 1.0408
Fold 3 Epoch 031; Train loss: 0.8426; Time: 00:00:02
Fold 3 Epoch 032; Train loss: 0.8329; Time: 00:00:02
Fold 3 Epoch 033; Train loss: 0.8215; Time: 00:00:02
Fold 3 Epoch 034; Train loss: 0.8138; Time: 00:00:02
Fold 3 Epoch 035; Train loss: 0.8043; Time: 00:00:02
Fold 3 Epoch 036; Train loss: 0.7949; Time: 00:00:02
Fold 3 Epoch 037; Train loss: 0.7975; Time: 00:00:02
Fold 3 Epoch 038; Train loss: 0.7872; Time: 00:00:02
Fold 3 Epoch 039; Train loss: 0.7791; Time: 00:00:02
Fold 3 Epoch 040; Train loss: 0.7741; Time: 00:00:02
Fold 3: Evaluation at Epoch 40
HR@5       NDCG@5     HR@10      NDCG@10   
0.493377   0.361896   0.728477   0.435780  
Loss: 0.9936
Fold 3 Epoch 041; Train loss: 0.7671; Time: 00:00:02
Fold 3 Epoch 042; Train loss: 0.7613; Time: 00:00:02
Fold 3 Epoch 043; Train loss: 0.7520; Time: 00:00:02
Fold 3 Epoch 044; Train loss: 0.7463; Time: 00:00:02
Fold 3 Epoch 045; Train loss: 0.7451; Time: 00:00:02
Fold 3 Epoch 046; Train loss: 0.7339; Time: 00:00:02
Fold 3 Epoch 047; Train loss: 0.7321; Time: 00:00:02
Fold 3 Epoch 048; Train loss: 0.7265; Time: 00:00:02
Fold 3 Epoch 049; Train loss: 0.7185; Time: 00:00:02
Fold 3 Epoch 050; Train loss: 0.7106; Time: 00:00:02
Fold 3: Evaluation at Epoch 50
HR@5       NDCG@5     HR@10      NDCG@10   
0.503311   0.367212   0.796358   0.459329  
Loss: 0.9406
Fold 3 Epoch 051; Train loss: 0.7064; Time: 00:00:02
Fold 3 Epoch 052; Train loss: 0.7047; Time: 00:00:02
Fold 3 Epoch 053; Train loss: 0.6952; Time: 00:00:02
Fold 3 Epoch 054; Train loss: 0.6939; Time: 00:00:02
Fold 3 Epoch 055; Train loss: 0.6859; Time: 00:00:02
Fold 3 Epoch 056; Train loss: 0.6857; Time: 00:00:02
Fold 3 Epoch 057; Train loss: 0.6761; Time: 00:00:02
Fold 3 Epoch 058; Train loss: 0.6681; Time: 00:00:02
Fold 3 Epoch 059; Train loss: 0.6718; Time: 00:00:02
Fold 3 Epoch 060; Train loss: 0.6618; Time: 00:00:02
Fold 3: Evaluation at Epoch 60
HR@5       NDCG@5     HR@10      NDCG@10   
0.521523   0.376094   0.832781   0.473398  
Loss: 0.9057
Fold 3 Epoch 061; Train loss: 0.6636; Time: 00:00:02
Fold 3 Epoch 062; Train loss: 0.6629; Time: 00:00:02
Fold 3 Epoch 063; Train loss: 0.6512; Time: 00:00:02
Fold 3 Epoch 064; Train loss: 0.6496; Time: 00:00:02
Fold 3 Epoch 065; Train loss: 0.6375; Time: 00:00:02
Fold 3 Epoch 066; Train loss: 0.6416; Time: 00:00:02
Fold 3 Epoch 067; Train loss: 0.6368; Time: 00:00:02
Fold 3 Epoch 068; Train loss: 0.6345; Time: 00:00:02
Fold 3 Epoch 069; Train loss: 0.6245; Time: 00:00:02
Fold 3 Epoch 070; Train loss: 0.6251; Time: 00:00:02
Fold 3: Evaluation at Epoch 70
HR@5       NDCG@5     HR@10      NDCG@10   
0.519868   0.382303   0.841060   0.484172  
Loss: 0.8766
Fold 3 Epoch 071; Train loss: 0.6167; Time: 00:00:02
Fold 3 Epoch 072; Train loss: 0.6191; Time: 00:00:02
Fold 3 Epoch 073; Train loss: 0.6138; Time: 00:00:01
Fold 3 Epoch 074; Train loss: 0.6028; Time: 00:00:02
Fold 3 Epoch 075; Train loss: 0.6013; Time: 00:00:01
Fold 3 Epoch 076; Train loss: 0.6011; Time: 00:00:02
Fold 3 Epoch 077; Train loss: 0.6007; Time: 00:00:02
Fold 3 Epoch 078; Train loss: 0.5974; Time: 00:00:02
Fold 3 Epoch 079; Train loss: 0.5905; Time: 00:00:02
Fold 3 Epoch 080; Train loss: 0.5876; Time: 00:00:01
Fold 3: Evaluation at Epoch 80
HR@5       NDCG@5     HR@10      NDCG@10   
0.528146   0.392930   0.827815   0.491146  
Loss: 0.8509
Fold 3 Epoch 081; Train loss: 0.5847; Time: 00:00:02
Fold 3 Epoch 082; Train loss: 0.5800; Time: 00:00:01
Fold 3 Epoch 083; Train loss: 0.5804; Time: 00:00:01
Fold 3 Epoch 084; Train loss: 0.5759; Time: 00:00:01
Fold 3 Epoch 085; Train loss: 0.5777; Time: 00:00:01
Fold 3 Epoch 086; Train loss: 0.5622; Time: 00:00:01
Fold 3 Epoch 087; Train loss: 0.5634; Time: 00:00:01
Fold 3 Epoch 088; Train loss: 0.5652; Time: 00:00:01
Fold 3 Epoch 089; Train loss: 0.5585; Time: 00:00:01
Fold 3 Epoch 090; Train loss: 0.5586; Time: 00:00:01
Fold 3: Evaluation at Epoch 90
HR@5       NDCG@5     HR@10      NDCG@10   
0.506623   0.385176   0.822848   0.490312  
Loss: 0.8076
Fold 3 Epoch 091; Train loss: 0.5571; Time: 00:00:01
Fold 3 Epoch 092; Train loss: 0.5449; Time: 00:00:01
Fold 3 Epoch 093; Train loss: 0.5397; Time: 00:00:01
Fold 3 Epoch 094; Train loss: 0.5470; Time: 00:00:02
Fold 3 Epoch 095; Train loss: 0.5423; Time: 00:00:01
Fold 3 Epoch 096; Train loss: 0.5383; Time: 00:00:01
Fold 3 Epoch 097; Train loss: 0.5359; Time: 00:00:01
Fold 3 Epoch 098; Train loss: 0.5330; Time: 00:00:02
Fold 3 Epoch 099; Train loss: 0.5289; Time: 00:00:02
Fold 3 Epoch 100; Train loss: 0.5269; Time: 00:00:02
Fold 3: Evaluation at Epoch 100
HR@5       NDCG@5     HR@10      NDCG@10   
0.577815   0.416031   0.811258   0.492277  
Loss: 0.7833
[optimizer candidate adabelief] Fold 3: HR@10 = [np.float64(0.652317880794702), np.float64(0.6639072847682119), np.float64(0.6920529801324503), np.float64(0.7284768211920529), np.float64(0.7963576158940397), np.float64(0.8327814569536424), np.float64(0.8410596026490066), np.float64(0.8278145695364238), np.float64(0.8228476821192053), np.float64(0.8112582781456954)]
Tuning optimizer: 100%|██████████| 4/4 [15:59<00:00, 243.20s/it]Tuning optimizer: 100%|██████████| 4/4 [15:59<00:00, 239.83s/it]

Best optimizer found: lamb
Tuning timesteps:   0%|          | 0/5 [00:00<?, ?it/s]
========== Fold 3 ==========
INFO:root:Using statics: seq_size = 10, genre_vocab_size = 18
Fold 3 Epoch 001; Train loss: 1.1896; Time: 00:00:02
Fold 3 Epoch 002; Train loss: 1.0033; Time: 00:00:02
Fold 3 Epoch 003; Train loss: 0.8752; Time: 00:00:01
Fold 3 Epoch 004; Train loss: 0.7764; Time: 00:00:02
Fold 3 Epoch 005; Train loss: 0.6942; Time: 00:00:01
Fold 3 Epoch 006; Train loss: 0.6259; Time: 00:00:01
Fold 3 Epoch 007; Train loss: 0.5625; Time: 00:00:01
Fold 3 Epoch 008; Train loss: 0.5025; Time: 00:00:02
Fold 3 Epoch 009; Train loss: 0.4490; Time: 00:00:01
Fold 3 Epoch 010; Train loss: 0.4052; Time: 00:00:02
Fold 3: Evaluation at Epoch 10
HR@5       NDCG@5     HR@10      NDCG@10   
0.715232   0.524292   0.867550   0.572746  
Loss: 0.5265
Fold 3 Epoch 011; Train loss: 0.3629; Time: 00:00:01
Fold 3 Epoch 012; Train loss: 0.3271; Time: 00:00:01
Fold 3 Epoch 013; Train loss: 0.2954; Time: 00:00:01
Fold 3 Epoch 014; Train loss: 0.2627; Time: 00:00:01
Fold 3 Epoch 015; Train loss: 0.2359; Time: 00:00:01
Fold 3 Epoch 016; Train loss: 0.2133; Time: 00:00:02
Fold 3 Epoch 017; Train loss: 0.1963; Time: 00:00:02
Fold 3 Epoch 018; Train loss: 0.1771; Time: 00:00:01
Fold 3 Epoch 019; Train loss: 0.1629; Time: 00:00:01
Fold 3 Epoch 020; Train loss: 0.1512; Time: 00:00:01
Fold 3: Evaluation at Epoch 20
HR@5       NDCG@5     HR@10      NDCG@10   
0.688742   0.554318   0.839404   0.602598  
Loss: 0.2331
Fold 3 Epoch 021; Train loss: 0.1364; Time: 00:00:01
Fold 3 Epoch 022; Train loss: 0.1309; Time: 00:00:02
Fold 3 Epoch 023; Train loss: 0.1187; Time: 00:00:01
Fold 3 Epoch 024; Train loss: 0.1113; Time: 00:00:02
Fold 3 Epoch 025; Train loss: 0.1047; Time: 00:00:02
Fold 3 Epoch 026; Train loss: 0.1002; Time: 00:00:02
Fold 3 Epoch 027; Train loss: 0.0951; Time: 00:00:01
Fold 3 Epoch 028; Train loss: 0.0916; Time: 00:00:01
Fold 3 Epoch 029; Train loss: 0.0869; Time: 00:00:01
Fold 3 Epoch 030; Train loss: 0.0839; Time: 00:00:02
Fold 3: Evaluation at Epoch 30
HR@5       NDCG@5     HR@10      NDCG@10   
0.644040   0.492536   0.791391   0.540278  
Loss: 0.1232
Fold 3 Epoch 031; Train loss: 0.0830; Time: 00:00:02
Fold 3 Epoch 032; Train loss: 0.0794; Time: 00:00:01
Fold 3 Epoch 033; Train loss: 0.0768; Time: 00:00:02
Fold 3 Epoch 034; Train loss: 0.0746; Time: 00:00:02
Fold 3 Epoch 035; Train loss: 0.0733; Time: 00:00:02
Fold 3 Epoch 036; Train loss: 0.0723; Time: 00:00:02
Fold 3 Epoch 037; Train loss: 0.0705; Time: 00:00:02
Fold 3 Epoch 038; Train loss: 0.0680; Time: 00:00:02
Fold 3 Epoch 039; Train loss: 0.0664; Time: 00:00:02
Fold 3 Epoch 040; Train loss: 0.0662; Time: 00:00:02
Fold 3: Evaluation at Epoch 40
HR@5       NDCG@5     HR@10      NDCG@10   
0.637417   0.482617   0.798013   0.533876  
Loss: 0.0938
Fold 3 Epoch 041; Train loss: 0.0651; Time: 00:00:02
Fold 3 Epoch 042; Train loss: 0.0634; Time: 00:00:02
Fold 3 Epoch 043; Train loss: 0.0621; Time: 00:00:02
Fold 3 Epoch 044; Train loss: 0.0616; Time: 00:00:02
Fold 3 Epoch 045; Train loss: 0.0607; Time: 00:00:02
Fold 3 Epoch 046; Train loss: 0.0592; Time: 00:00:02
Fold 3 Epoch 047; Train loss: 0.0589; Time: 00:00:02
Fold 3 Epoch 048; Train loss: 0.0570; Time: 00:00:02
Fold 3 Epoch 049; Train loss: 0.0572; Time: 00:00:02
Fold 3 Epoch 050; Train loss: 0.0557; Time: 00:00:02
Fold 3: Evaluation at Epoch 50
HR@5       NDCG@5     HR@10      NDCG@10   
0.594371   0.456412   0.725166   0.498773  
Loss: 0.0746
Fold 3 Epoch 051; Train loss: 0.0553; Time: 00:00:02
Fold 3 Epoch 052; Train loss: 0.0547; Time: 00:00:02
Fold 3 Epoch 053; Train loss: 0.0541; Time: 00:00:02
Fold 3 Epoch 054; Train loss: 0.0526; Time: 00:00:02
Fold 3 Epoch 055; Train loss: 0.0533; Time: 00:00:02
Fold 3 Epoch 056; Train loss: 0.0526; Time: 00:00:02
Fold 3 Epoch 057; Train loss: 0.0511; Time: 00:00:02
Fold 3 Epoch 058; Train loss: 0.0508; Time: 00:00:02
Fold 3 Epoch 059; Train loss: 0.0509; Time: 00:00:02
Fold 3 Epoch 060; Train loss: 0.0491; Time: 00:00:02
Fold 3: Evaluation at Epoch 60
HR@5       NDCG@5     HR@10      NDCG@10   
0.576159   0.424770   0.723510   0.472812  
Loss: 0.0728
Fold 3 Epoch 061; Train loss: 0.0502; Time: 00:00:02
Fold 3 Epoch 062; Train loss: 0.0484; Time: 00:00:02
Fold 3 Epoch 063; Train loss: 0.0473; Time: 00:00:02
Fold 3 Epoch 064; Train loss: 0.0470; Time: 00:00:02
Fold 3 Epoch 065; Train loss: 0.0465; Time: 00:00:02
Fold 3 Epoch 066; Train loss: 0.0456; Time: 00:00:02
Fold 3 Epoch 067; Train loss: 0.0452; Time: 00:00:02
Fold 3 Epoch 068; Train loss: 0.0441; Time: 00:00:02
Fold 3 Epoch 069; Train loss: 0.0432; Time: 00:00:02
Fold 3 Epoch 070; Train loss: 0.0441; Time: 00:00:02
Fold 3: Evaluation at Epoch 70
HR@5       NDCG@5     HR@10      NDCG@10   
0.587748   0.412848   0.766556   0.471168  
Loss: 0.0563
Fold 3 Epoch 071; Train loss: 0.0426; Time: 00:00:02
Fold 3 Epoch 072; Train loss: 0.0424; Time: 00:00:02
Fold 3 Epoch 073; Train loss: 0.0418; Time: 00:00:02
Fold 3 Epoch 074; Train loss: 0.0413; Time: 00:00:02
Fold 3 Epoch 075; Train loss: 0.0397; Time: 00:00:02
Fold 3 Epoch 076; Train loss: 0.0401; Time: 00:00:02
Fold 3 Epoch 077; Train loss: 0.0401; Time: 00:00:02
Fold 3 Epoch 078; Train loss: 0.0376; Time: 00:00:02
Fold 3 Epoch 079; Train loss: 0.0387; Time: 00:00:02
Fold 3 Epoch 080; Train loss: 0.0380; Time: 00:00:02
Fold 3: Evaluation at Epoch 80
HR@5       NDCG@5     HR@10      NDCG@10   
0.509934   0.345550   0.675497   0.397749  
Loss: 0.0479
Fold 3 Epoch 081; Train loss: 0.0366; Time: 00:00:02
Fold 3 Epoch 082; Train loss: 0.0354; Time: 00:00:02
Fold 3 Epoch 083; Train loss: 0.0360; Time: 00:00:02
Fold 3 Epoch 084; Train loss: 0.0354; Time: 00:00:02
Fold 3 Epoch 085; Train loss: 0.0347; Time: 00:00:02
Fold 3 Epoch 086; Train loss: 0.0344; Time: 00:00:02
Fold 3 Epoch 087; Train loss: 0.0340; Time: 00:00:02
Fold 3 Epoch 088; Train loss: 0.0341; Time: 00:00:02
Fold 3 Epoch 089; Train loss: 0.0333; Time: 00:00:02
Fold 3 Epoch 090; Train loss: 0.0344; Time: 00:00:02
Fold 3: Evaluation at Epoch 90
HR@5       NDCG@5     HR@10      NDCG@10   
0.514901   0.377302   0.710265   0.440682  
Loss: 0.0384
Fold 3 Epoch 091; Train loss: 0.0325; Time: 00:00:02
Fold 3 Epoch 092; Train loss: 0.0326; Time: 00:00:02
Fold 3 Epoch 093; Train loss: 0.0316; Time: 00:00:02
Fold 3 Epoch 094; Train loss: 0.0313; Time: 00:00:02
Fold 3 Epoch 095; Train loss: 0.0315; Time: 00:00:02
Fold 3 Epoch 096; Train loss: 0.0309; Time: 00:00:02
Fold 3 Epoch 097; Train loss: 0.0303; Time: 00:00:02
Fold 3 Epoch 098; Train loss: 0.0299; Time: 00:00:02
Fold 3 Epoch 099; Train loss: 0.0284; Time: 00:00:02
Fold 3 Epoch 100; Train loss: 0.0297; Time: 00:00:02
Fold 3: Evaluation at Epoch 100
HR@5       NDCG@5     HR@10      NDCG@10   
0.600993   0.417583   0.793046   0.479979  
Loss: 0.0343
[timesteps candidate 100] Fold 3: HR@10 = [np.float64(0.8675496688741722), np.float64(0.8394039735099338), np.float64(0.7913907284768212), np.float64(0.7980132450331126), np.float64(0.7251655629139073), np.float64(0.7235099337748344), np.float64(0.7665562913907285), np.float64(0.6754966887417219), np.float64(0.7102649006622517), np.float64(0.793046357615894)]
Tuning timesteps:  20%|██        | 1/5 [03:57<15:49, 237.29s/it]
========== Fold 3 ==========
INFO:root:Using statics: seq_size = 10, genre_vocab_size = 18
Fold 3 Epoch 001; Train loss: 1.0050; Time: 00:00:02
Fold 3 Epoch 002; Train loss: 0.8406; Time: 00:00:02
Fold 3 Epoch 003; Train loss: 0.7258; Time: 00:00:02
Fold 3 Epoch 004; Train loss: 0.6436; Time: 00:00:02
Fold 3 Epoch 005; Train loss: 0.5843; Time: 00:00:02
Fold 3 Epoch 006; Train loss: 0.5305; Time: 00:00:02
Fold 3 Epoch 007; Train loss: 0.4850; Time: 00:00:02
Fold 3 Epoch 008; Train loss: 0.4391; Time: 00:00:02
Fold 3 Epoch 009; Train loss: 0.4009; Time: 00:00:02
Fold 3 Epoch 010; Train loss: 0.3652; Time: 00:00:02
Fold 3: Evaluation at Epoch 10
HR@5       NDCG@5     HR@10      NDCG@10   
0.548013   0.450926   0.761589   0.520819  
Loss: 0.5593
Fold 3 Epoch 011; Train loss: 0.3305; Time: 00:00:02
Fold 3 Epoch 012; Train loss: 0.3096; Time: 00:00:02
Fold 3 Epoch 013; Train loss: 0.2832; Time: 00:00:02
Fold 3 Epoch 014; Train loss: 0.2620; Time: 00:00:02
Fold 3 Epoch 015; Train loss: 0.2401; Time: 00:00:02
Fold 3 Epoch 016; Train loss: 0.2303; Time: 00:00:02
Fold 3 Epoch 017; Train loss: 0.2150; Time: 00:00:02
Fold 3 Epoch 018; Train loss: 0.1955; Time: 00:00:02
Fold 3 Epoch 019; Train loss: 0.1888; Time: 00:00:02
Fold 3 Epoch 020; Train loss: 0.1748; Time: 00:00:02
Fold 3: Evaluation at Epoch 20
HR@5       NDCG@5     HR@10      NDCG@10   
0.632450   0.505618   0.764901   0.548414  
Loss: 0.2623
Fold 3 Epoch 021; Train loss: 0.1673; Time: 00:00:02
Fold 3 Epoch 022; Train loss: 0.1581; Time: 00:00:02
Fold 3 Epoch 023; Train loss: 0.1497; Time: 00:00:01
Fold 3 Epoch 024; Train loss: 0.1434; Time: 00:00:02
Fold 3 Epoch 025; Train loss: 0.1405; Time: 00:00:02
Fold 3 Epoch 026; Train loss: 0.1322; Time: 00:00:01
Fold 3 Epoch 027; Train loss: 0.1252; Time: 00:00:01
Fold 3 Epoch 028; Train loss: 0.1233; Time: 00:00:02
Fold 3 Epoch 029; Train loss: 0.1200; Time: 00:00:02
Fold 3 Epoch 030; Train loss: 0.1184; Time: 00:00:02
Fold 3: Evaluation at Epoch 30
HR@5       NDCG@5     HR@10      NDCG@10   
0.620861   0.507120   0.788079   0.560409  
Loss: 0.1590
Fold 3 Epoch 031; Train loss: 0.1140; Time: 00:00:02
Fold 3 Epoch 032; Train loss: 0.1114; Time: 00:00:02
Fold 3 Epoch 033; Train loss: 0.1061; Time: 00:00:02
Fold 3 Epoch 034; Train loss: 0.1041; Time: 00:00:02
Fold 3 Epoch 035; Train loss: 0.1041; Time: 00:00:02
Fold 3 Epoch 036; Train loss: 0.0978; Time: 00:00:02
Fold 3 Epoch 037; Train loss: 0.0963; Time: 00:00:02
Fold 3 Epoch 038; Train loss: 0.0961; Time: 00:00:02
Fold 3 Epoch 039; Train loss: 0.0932; Time: 00:00:02
Fold 3 Epoch 040; Train loss: 0.0911; Time: 00:00:02
Fold 3: Evaluation at Epoch 40
HR@5       NDCG@5     HR@10      NDCG@10   
0.663907   0.536597   0.809603   0.582992  
Loss: 0.1322
Fold 3 Epoch 041; Train loss: 0.0893; Time: 00:00:02
Fold 3 Epoch 042; Train loss: 0.0890; Time: 00:00:02
Fold 3 Epoch 043; Train loss: 0.0869; Time: 00:00:02
Fold 3 Epoch 044; Train loss: 0.0863; Time: 00:00:02
Fold 3 Epoch 045; Train loss: 0.0854; Time: 00:00:02
Fold 3 Epoch 046; Train loss: 0.0818; Time: 00:00:02
Fold 3 Epoch 047; Train loss: 0.0839; Time: 00:00:02
Fold 3 Epoch 048; Train loss: 0.0770; Time: 00:00:02
Fold 3 Epoch 049; Train loss: 0.0763; Time: 00:00:02
Fold 3 Epoch 050; Train loss: 0.0772; Time: 00:00:02
Fold 3: Evaluation at Epoch 50
HR@5       NDCG@5     HR@10      NDCG@10   
0.615894   0.512870   0.806291   0.573562  
Loss: 0.1115
Fold 3 Epoch 051; Train loss: 0.0750; Time: 00:00:02
Fold 3 Epoch 052; Train loss: 0.0736; Time: 00:00:01
Fold 3 Epoch 053; Train loss: 0.0721; Time: 00:00:02
Fold 3 Epoch 054; Train loss: 0.0714; Time: 00:00:02
Fold 3 Epoch 055; Train loss: 0.0702; Time: 00:00:01
Fold 3 Epoch 056; Train loss: 0.0674; Time: 00:00:01
Fold 3 Epoch 057; Train loss: 0.0664; Time: 00:00:01
Fold 3 Epoch 058; Train loss: 0.0672; Time: 00:00:01
Fold 3 Epoch 059; Train loss: 0.0644; Time: 00:00:02
Fold 3 Epoch 060; Train loss: 0.0633; Time: 00:00:02
Fold 3: Evaluation at Epoch 60
HR@5       NDCG@5     HR@10      NDCG@10   
0.629139   0.521240   0.793046   0.573719  
Loss: 0.0983
Fold 3 Epoch 061; Train loss: 0.0633; Time: 00:00:02
Fold 3 Epoch 062; Train loss: 0.0615; Time: 00:00:02
Fold 3 Epoch 063; Train loss: 0.0592; Time: 00:00:01
Fold 3 Epoch 064; Train loss: 0.0587; Time: 00:00:01
Fold 3 Epoch 065; Train loss: 0.0559; Time: 00:00:01
Fold 3 Epoch 066; Train loss: 0.0543; Time: 00:00:01
Fold 3 Epoch 067; Train loss: 0.0563; Time: 00:00:02
Fold 3 Epoch 068; Train loss: 0.0521; Time: 00:00:02
Fold 3 Epoch 069; Train loss: 0.0513; Time: 00:00:01
Fold 3 Epoch 070; Train loss: 0.0518; Time: 00:00:01
Fold 3: Evaluation at Epoch 70
HR@5       NDCG@5     HR@10      NDCG@10   
0.594371   0.479471   0.763245   0.532673  
Loss: 0.0749
Fold 3 Epoch 071; Train loss: 0.0488; Time: 00:00:01
Fold 3 Epoch 072; Train loss: 0.0485; Time: 00:00:01
Fold 3 Epoch 073; Train loss: 0.0481; Time: 00:00:01
Fold 3 Epoch 074; Train loss: 0.0466; Time: 00:00:01
Fold 3 Epoch 075; Train loss: 0.0448; Time: 00:00:02
Fold 3 Epoch 076; Train loss: 0.0449; Time: 00:00:02
Fold 3 Epoch 077; Train loss: 0.0426; Time: 00:00:02
Fold 3 Epoch 078; Train loss: 0.0427; Time: 00:00:02
Fold 3 Epoch 079; Train loss: 0.0414; Time: 00:00:02
Fold 3 Epoch 080; Train loss: 0.0400; Time: 00:00:01
Fold 3: Evaluation at Epoch 80
HR@5       NDCG@5     HR@10      NDCG@10   
0.567881   0.461748   0.807947   0.537754  
Loss: 0.0617
Fold 3 Epoch 081; Train loss: 0.0376; Time: 00:00:01
Fold 3 Epoch 082; Train loss: 0.0377; Time: 00:00:01
Fold 3 Epoch 083; Train loss: 0.0371; Time: 00:00:01
Fold 3 Epoch 084; Train loss: 0.0348; Time: 00:00:02
Fold 3 Epoch 085; Train loss: 0.0338; Time: 00:00:01
Fold 3 Epoch 086; Train loss: 0.0328; Time: 00:00:01
Fold 3 Epoch 087; Train loss: 0.0318; Time: 00:00:01
Fold 3 Epoch 088; Train loss: 0.0311; Time: 00:00:01
Fold 3 Epoch 089; Train loss: 0.0295; Time: 00:00:01
Fold 3 Epoch 090; Train loss: 0.0285; Time: 00:00:01
Fold 3: Evaluation at Epoch 90
HR@5       NDCG@5     HR@10      NDCG@10   
0.607616   0.436199   0.763245   0.486020  
Loss: 0.0404
Fold 3 Epoch 091; Train loss: 0.0276; Time: 00:00:01
Fold 3 Epoch 092; Train loss: 0.0275; Time: 00:00:01
Fold 3 Epoch 093; Train loss: 0.0255; Time: 00:00:01
Fold 3 Epoch 094; Train loss: 0.0249; Time: 00:00:01
Fold 3 Epoch 095; Train loss: 0.0239; Time: 00:00:01
Fold 3 Epoch 096; Train loss: 0.0228; Time: 00:00:01
Fold 3 Epoch 097; Train loss: 0.0217; Time: 00:00:01
Fold 3 Epoch 098; Train loss: 0.0207; Time: 00:00:01
Fold 3 Epoch 099; Train loss: 0.0199; Time: 00:00:01
Fold 3 Epoch 100; Train loss: 0.0188; Time: 00:00:02
Fold 3: Evaluation at Epoch 100
HR@5       NDCG@5     HR@10      NDCG@10   
0.559603   0.403237   0.764901   0.469948  
Loss: 0.0275
[timesteps candidate 200] Fold 3: HR@10 = [np.float64(0.7615894039735099), np.float64(0.7649006622516556), np.float64(0.7880794701986755), np.float64(0.8096026490066225), np.float64(0.8062913907284768), np.float64(0.793046357615894), np.float64(0.7632450331125827), np.float64(0.8079470198675497), np.float64(0.7632450331125827), np.float64(0.7649006622516556)]
Tuning timesteps:  40%|████      | 2/5 [07:38<11:23, 227.74s/it]
========== Fold 3 ==========
INFO:root:Using statics: seq_size = 10, genre_vocab_size = 18
Fold 3 Epoch 001; Train loss: 1.0668; Time: 00:00:01
Fold 3 Epoch 002; Train loss: 0.9098; Time: 00:00:01
Fold 3 Epoch 003; Train loss: 0.7990; Time: 00:00:02
Fold 3 Epoch 004; Train loss: 0.7246; Time: 00:00:02
Fold 3 Epoch 005; Train loss: 0.6693; Time: 00:00:02
Fold 3 Epoch 006; Train loss: 0.6164; Time: 00:00:01
Fold 3 Epoch 007; Train loss: 0.5738; Time: 00:00:01
Fold 3 Epoch 008; Train loss: 0.5324; Time: 00:00:01
Fold 3 Epoch 009; Train loss: 0.4956; Time: 00:00:01
Fold 3 Epoch 010; Train loss: 0.4596; Time: 00:00:01
Fold 3: Evaluation at Epoch 10
HR@5       NDCG@5     HR@10      NDCG@10   
0.695364   0.519272   0.875828   0.578969  
Loss: 0.6250
Fold 3 Epoch 011; Train loss: 0.4316; Time: 00:00:01
Fold 3 Epoch 012; Train loss: 0.4034; Time: 00:00:02
Fold 3 Epoch 013; Train loss: 0.3805; Time: 00:00:02
Fold 3 Epoch 014; Train loss: 0.3622; Time: 00:00:01
Fold 3 Epoch 015; Train loss: 0.3403; Time: 00:00:02
Fold 3 Epoch 016; Train loss: 0.3236; Time: 00:00:02
Fold 3 Epoch 017; Train loss: 0.3032; Time: 00:00:01
Fold 3 Epoch 018; Train loss: 0.2933; Time: 00:00:01
Fold 3 Epoch 019; Train loss: 0.2781; Time: 00:00:01
Fold 3 Epoch 020; Train loss: 0.2654; Time: 00:00:02
Fold 3: Evaluation at Epoch 20
HR@5       NDCG@5     HR@10      NDCG@10   
0.705298   0.530533   0.824503   0.569137  
Loss: 0.3784
Fold 3 Epoch 021; Train loss: 0.2538; Time: 00:00:01
Fold 3 Epoch 022; Train loss: 0.2433; Time: 00:00:01
Fold 3 Epoch 023; Train loss: 0.2325; Time: 00:00:01
Fold 3 Epoch 024; Train loss: 0.2264; Time: 00:00:01
Fold 3 Epoch 025; Train loss: 0.2171; Time: 00:00:02
Fold 3 Epoch 026; Train loss: 0.2110; Time: 00:00:02
Fold 3 Epoch 027; Train loss: 0.2061; Time: 00:00:02
Fold 3 Epoch 028; Train loss: 0.2019; Time: 00:00:02
Fold 3 Epoch 029; Train loss: 0.1936; Time: 00:00:02
Fold 3 Epoch 030; Train loss: 0.1831; Time: 00:00:02
Fold 3: Evaluation at Epoch 30
HR@5       NDCG@5     HR@10      NDCG@10   
0.672185   0.505868   0.827815   0.556605  
Loss: 0.2573
Fold 3 Epoch 031; Train loss: 0.1875; Time: 00:00:01
Fold 3 Epoch 032; Train loss: 0.1807; Time: 00:00:01
Fold 3 Epoch 033; Train loss: 0.1742; Time: 00:00:01
Fold 3 Epoch 034; Train loss: 0.1719; Time: 00:00:01
Fold 3 Epoch 035; Train loss: 0.1705; Time: 00:00:01
Fold 3 Epoch 036; Train loss: 0.1623; Time: 00:00:02
Fold 3 Epoch 037; Train loss: 0.1588; Time: 00:00:01
Fold 3 Epoch 038; Train loss: 0.1563; Time: 00:00:01
Fold 3 Epoch 039; Train loss: 0.1507; Time: 00:00:02
Fold 3 Epoch 040; Train loss: 0.1470; Time: 00:00:01
Fold 3: Evaluation at Epoch 40
HR@5       NDCG@5     HR@10      NDCG@10   
0.665563   0.507614   0.811258   0.554765  
Loss: 0.2101
Fold 3 Epoch 041; Train loss: 0.1462; Time: 00:00:01
Fold 3 Epoch 042; Train loss: 0.1379; Time: 00:00:02
Fold 3 Epoch 043; Train loss: 0.1401; Time: 00:00:01
Fold 3 Epoch 044; Train loss: 0.1381; Time: 00:00:01
Fold 3 Epoch 045; Train loss: 0.1296; Time: 00:00:02
Fold 3 Epoch 046; Train loss: 0.1277; Time: 00:00:02
Fold 3 Epoch 047; Train loss: 0.1268; Time: 00:00:02
Fold 3 Epoch 048; Train loss: 0.1221; Time: 00:00:01
Fold 3 Epoch 049; Train loss: 0.1216; Time: 00:00:02
Fold 3 Epoch 050; Train loss: 0.1157; Time: 00:00:02
Fold 3: Evaluation at Epoch 50
HR@5       NDCG@5     HR@10      NDCG@10   
0.657285   0.484516   0.801325   0.531179  
Loss: 0.1693
Fold 3 Epoch 051; Train loss: 0.1168; Time: 00:00:02
Fold 3 Epoch 052; Train loss: 0.1107; Time: 00:00:01
Fold 3 Epoch 053; Train loss: 0.1111; Time: 00:00:02
Fold 3 Epoch 054; Train loss: 0.1074; Time: 00:00:01
Fold 3 Epoch 055; Train loss: 0.1039; Time: 00:00:01
Fold 3 Epoch 056; Train loss: 0.1037; Time: 00:00:01
Fold 3 Epoch 057; Train loss: 0.0970; Time: 00:00:02
Fold 3 Epoch 058; Train loss: 0.0960; Time: 00:00:02
Fold 3 Epoch 059; Train loss: 0.0970; Time: 00:00:02
Fold 3 Epoch 060; Train loss: 0.0905; Time: 00:00:02
Fold 3: Evaluation at Epoch 60
HR@5       NDCG@5     HR@10      NDCG@10   
0.630795   0.399201   0.852649   0.471151  
Loss: 0.1338
Fold 3 Epoch 061; Train loss: 0.0896; Time: 00:00:02
Fold 3 Epoch 062; Train loss: 0.0882; Time: 00:00:01
Fold 3 Epoch 063; Train loss: 0.0864; Time: 00:00:01
Fold 3 Epoch 064; Train loss: 0.0847; Time: 00:00:01
Fold 3 Epoch 065; Train loss: 0.0815; Time: 00:00:01
Fold 3 Epoch 066; Train loss: 0.0782; Time: 00:00:02
Fold 3 Epoch 067; Train loss: 0.0760; Time: 00:00:02
Fold 3 Epoch 068; Train loss: 0.0736; Time: 00:00:01
Fold 3 Epoch 069; Train loss: 0.0724; Time: 00:00:01
Fold 3 Epoch 070; Train loss: 0.0703; Time: 00:00:01
Fold 3: Evaluation at Epoch 70
HR@5       NDCG@5     HR@10      NDCG@10   
0.615894   0.354154   0.859272   0.433867  
Loss: 0.1028
Fold 3 Epoch 071; Train loss: 0.0650; Time: 00:00:01
Fold 3 Epoch 072; Train loss: 0.0661; Time: 00:00:01
Fold 3 Epoch 073; Train loss: 0.0627; Time: 00:00:01
Fold 3 Epoch 074; Train loss: 0.0612; Time: 00:00:01
Fold 3 Epoch 075; Train loss: 0.0576; Time: 00:00:02
Fold 3 Epoch 076; Train loss: 0.0560; Time: 00:00:01
Fold 3 Epoch 077; Train loss: 0.0541; Time: 00:00:02
Fold 3 Epoch 078; Train loss: 0.0529; Time: 00:00:02
Fold 3 Epoch 079; Train loss: 0.0510; Time: 00:00:01
Fold 3 Epoch 080; Train loss: 0.0484; Time: 00:00:02
Fold 3: Evaluation at Epoch 80
HR@5       NDCG@5     HR@10      NDCG@10   
0.571192   0.316573   0.869205   0.413874  
Loss: 0.0733
Fold 3 Epoch 081; Train loss: 0.0464; Time: 00:00:02
Fold 3 Epoch 082; Train loss: 0.0432; Time: 00:00:02
Fold 3 Epoch 083; Train loss: 0.0433; Time: 00:00:01
Fold 3 Epoch 084; Train loss: 0.0401; Time: 00:00:02
Fold 3 Epoch 085; Train loss: 0.0381; Time: 00:00:02
Fold 3 Epoch 086; Train loss: 0.0368; Time: 00:00:02
Fold 3 Epoch 087; Train loss: 0.0357; Time: 00:00:02
Fold 3 Epoch 088; Train loss: 0.0333; Time: 00:00:01
Fold 3 Epoch 089; Train loss: 0.0318; Time: 00:00:01
Fold 3 Epoch 090; Train loss: 0.0298; Time: 00:00:02
Fold 3: Evaluation at Epoch 90
HR@5       NDCG@5     HR@10      NDCG@10   
0.680464   0.428233   0.890728   0.496985  
Loss: 0.0469
Fold 3 Epoch 091; Train loss: 0.0286; Time: 00:00:01
Fold 3 Epoch 092; Train loss: 0.0274; Time: 00:00:02
Fold 3 Epoch 093; Train loss: 0.0261; Time: 00:00:02
Fold 3 Epoch 094; Train loss: 0.0245; Time: 00:00:01
Fold 3 Epoch 095; Train loss: 0.0239; Time: 00:00:01
Fold 3 Epoch 096; Train loss: 0.0217; Time: 00:00:01
Fold 3 Epoch 097; Train loss: 0.0200; Time: 00:00:01
Fold 3 Epoch 098; Train loss: 0.0193; Time: 00:00:01
Fold 3 Epoch 099; Train loss: 0.0175; Time: 00:00:01
Fold 3 Epoch 100; Train loss: 0.0162; Time: 00:00:02
Fold 3: Evaluation at Epoch 100
HR@5       NDCG@5     HR@10      NDCG@10   
0.665563   0.372362   0.889073   0.448296  
Loss: 0.0285
[timesteps candidate 400] Fold 3: HR@10 = [np.float64(0.8758278145695364), np.float64(0.8245033112582781), np.float64(0.8278145695364238), np.float64(0.8112582781456954), np.float64(0.8013245033112583), np.float64(0.8526490066225165), np.float64(0.859271523178808), np.float64(0.8692052980132451), np.float64(0.890728476821192), np.float64(0.8890728476821192)]
Tuning timesteps:  60%|██████    | 3/5 [11:04<07:16, 218.00s/it]
========== Fold 3 ==========
INFO:root:Using statics: seq_size = 10, genre_vocab_size = 18
Fold 3 Epoch 001; Train loss: 0.9979; Time: 00:00:01
Fold 3 Epoch 002; Train loss: 0.8463; Time: 00:00:01
Fold 3 Epoch 003; Train loss: 0.7544; Time: 00:00:02
Fold 3 Epoch 004; Train loss: 0.6882; Time: 00:00:01
Fold 3 Epoch 005; Train loss: 0.6419; Time: 00:00:02
Fold 3 Epoch 006; Train loss: 0.5980; Time: 00:00:02
Fold 3 Epoch 007; Train loss: 0.5565; Time: 00:00:01
Fold 3 Epoch 008; Train loss: 0.5189; Time: 00:00:01
Fold 3 Epoch 009; Train loss: 0.4843; Time: 00:00:01
Fold 3 Epoch 010; Train loss: 0.4534; Time: 00:00:02
Fold 3: Evaluation at Epoch 10
HR@5       NDCG@5     HR@10      NDCG@10   
0.615894   0.435540   0.740066   0.475239  
Loss: 0.6350
Fold 3 Epoch 011; Train loss: 0.4292; Time: 00:00:01
Fold 3 Epoch 012; Train loss: 0.4049; Time: 00:00:01
Fold 3 Epoch 013; Train loss: 0.3824; Time: 00:00:02
Fold 3 Epoch 014; Train loss: 0.3583; Time: 00:00:02
Fold 3 Epoch 015; Train loss: 0.3494; Time: 00:00:02
Fold 3 Epoch 016; Train loss: 0.3336; Time: 00:00:02
Fold 3 Epoch 017; Train loss: 0.3140; Time: 00:00:02
Fold 3 Epoch 018; Train loss: 0.3018; Time: 00:00:02
Fold 3 Epoch 019; Train loss: 0.2902; Time: 00:00:02
Fold 3 Epoch 020; Train loss: 0.2795; Time: 00:00:01
Fold 3: Evaluation at Epoch 20
HR@5       NDCG@5     HR@10      NDCG@10   
0.650662   0.526449   0.750000   0.558425  
Loss: 0.3821
Fold 3 Epoch 021; Train loss: 0.2685; Time: 00:00:01
Fold 3 Epoch 022; Train loss: 0.2555; Time: 00:00:01
Fold 3 Epoch 023; Train loss: 0.2466; Time: 00:00:01
Fold 3 Epoch 024; Train loss: 0.2340; Time: 00:00:02
Fold 3 Epoch 025; Train loss: 0.2339; Time: 00:00:02
Fold 3 Epoch 026; Train loss: 0.2273; Time: 00:00:02
Fold 3 Epoch 027; Train loss: 0.2119; Time: 00:00:02
Fold 3 Epoch 028; Train loss: 0.2069; Time: 00:00:02
Fold 3 Epoch 029; Train loss: 0.2010; Time: 00:00:01
Fold 3 Epoch 030; Train loss: 0.1944; Time: 00:00:01
Fold 3: Evaluation at Epoch 30
HR@5       NDCG@5     HR@10      NDCG@10   
0.634106   0.526564   0.723510   0.555016  
Loss: 0.2735
Fold 3 Epoch 031; Train loss: 0.1915; Time: 00:00:02
Fold 3 Epoch 032; Train loss: 0.1853; Time: 00:00:02
Fold 3 Epoch 033; Train loss: 0.1805; Time: 00:00:01
Fold 3 Epoch 034; Train loss: 0.1756; Time: 00:00:02
Fold 3 Epoch 035; Train loss: 0.1693; Time: 00:00:02
Fold 3 Epoch 036; Train loss: 0.1682; Time: 00:00:01
Fold 3 Epoch 037; Train loss: 0.1595; Time: 00:00:01
Fold 3 Epoch 038; Train loss: 0.1575; Time: 00:00:02
Fold 3 Epoch 039; Train loss: 0.1535; Time: 00:00:01
Fold 3 Epoch 040; Train loss: 0.1486; Time: 00:00:02
Fold 3: Evaluation at Epoch 40
HR@5       NDCG@5     HR@10      NDCG@10   
0.624172   0.518120   0.728477   0.551698  
Loss: 0.2143
Fold 3 Epoch 041; Train loss: 0.1454; Time: 00:00:01
Fold 3 Epoch 042; Train loss: 0.1420; Time: 00:00:01
Fold 3 Epoch 043; Train loss: 0.1346; Time: 00:00:01
Fold 3 Epoch 044; Train loss: 0.1329; Time: 00:00:01
Fold 3 Epoch 045; Train loss: 0.1280; Time: 00:00:01
Fold 3 Epoch 046; Train loss: 0.1261; Time: 00:00:01
Fold 3 Epoch 047; Train loss: 0.1199; Time: 00:00:01
Fold 3 Epoch 048; Train loss: 0.1199; Time: 00:00:01
Fold 3 Epoch 049; Train loss: 0.1119; Time: 00:00:02
Fold 3 Epoch 050; Train loss: 0.1126; Time: 00:00:02
Fold 3: Evaluation at Epoch 50
HR@5       NDCG@5     HR@10      NDCG@10   
0.622517   0.522576   0.731788   0.557954  
Loss: 0.1754
Fold 3 Epoch 051; Train loss: 0.1089; Time: 00:00:01
Fold 3 Epoch 052; Train loss: 0.1046; Time: 00:00:01
Fold 3 Epoch 053; Train loss: 0.1019; Time: 00:00:01
Fold 3 Epoch 054; Train loss: 0.0990; Time: 00:00:02
Fold 3 Epoch 055; Train loss: 0.0936; Time: 00:00:01
Fold 3 Epoch 056; Train loss: 0.0909; Time: 00:00:01
Fold 3 Epoch 057; Train loss: 0.0903; Time: 00:00:01
Fold 3 Epoch 058; Train loss: 0.0842; Time: 00:00:02
Fold 3 Epoch 059; Train loss: 0.0838; Time: 00:00:02
Fold 3 Epoch 060; Train loss: 0.0804; Time: 00:00:01
Fold 3: Evaluation at Epoch 60
HR@5       NDCG@5     HR@10      NDCG@10   
0.655629   0.528137   0.809603   0.577720  
Loss: 0.1321
Fold 3 Epoch 061; Train loss: 0.0769; Time: 00:00:01
Fold 3 Epoch 062; Train loss: 0.0740; Time: 00:00:01
Fold 3 Epoch 063; Train loss: 0.0724; Time: 00:00:02
Fold 3 Epoch 064; Train loss: 0.0676; Time: 00:00:01
Fold 3 Epoch 065; Train loss: 0.0650; Time: 00:00:02
Fold 3 Epoch 066; Train loss: 0.0621; Time: 00:00:01
Fold 3 Epoch 067; Train loss: 0.0608; Time: 00:00:01
Fold 3 Epoch 068; Train loss: 0.0575; Time: 00:00:01
Fold 3 Epoch 069; Train loss: 0.0540; Time: 00:00:01
Fold 3 Epoch 070; Train loss: 0.0531; Time: 00:00:01
Fold 3: Evaluation at Epoch 70
HR@5       NDCG@5     HR@10      NDCG@10   
0.668874   0.534635   0.847682   0.591914  
Loss: 0.1033
Fold 3 Epoch 071; Train loss: 0.0491; Time: 00:00:01
Fold 3 Epoch 072; Train loss: 0.0468; Time: 00:00:01
Fold 3 Epoch 073; Train loss: 0.0443; Time: 00:00:01
Fold 3 Epoch 074; Train loss: 0.0424; Time: 00:00:01
Fold 3 Epoch 075; Train loss: 0.0400; Time: 00:00:02
Fold 3 Epoch 076; Train loss: 0.0383; Time: 00:00:01
Fold 3 Epoch 077; Train loss: 0.0355; Time: 00:00:01
Fold 3 Epoch 078; Train loss: 0.0340; Time: 00:00:02
Fold 3 Epoch 079; Train loss: 0.0328; Time: 00:00:01
Fold 3 Epoch 080; Train loss: 0.0296; Time: 00:00:01
Fold 3: Evaluation at Epoch 80
HR@5       NDCG@5     HR@10      NDCG@10   
0.600993   0.461940   0.834437   0.537830  
Loss: 0.0647
Fold 3 Epoch 081; Train loss: 0.0279; Time: 00:00:02
Fold 3 Epoch 082; Train loss: 0.0268; Time: 00:00:01
Fold 3 Epoch 083; Train loss: 0.0245; Time: 00:00:02
Fold 3 Epoch 084; Train loss: 0.0233; Time: 00:00:01
Fold 3 Epoch 085; Train loss: 0.0214; Time: 00:00:01
Fold 3 Epoch 086; Train loss: 0.0207; Time: 00:00:01
Fold 3 Epoch 087; Train loss: 0.0180; Time: 00:00:01
Fold 3 Epoch 088; Train loss: 0.0173; Time: 00:00:01
Fold 3 Epoch 089; Train loss: 0.0157; Time: 00:00:01
Fold 3 Epoch 090; Train loss: 0.0145; Time: 00:00:01
Fold 3: Evaluation at Epoch 90
HR@5       NDCG@5     HR@10      NDCG@10   
0.630795   0.468591   0.867550   0.547915  
Loss: 0.0390
Fold 3 Epoch 091; Train loss: 0.0136; Time: 00:00:01
Fold 3 Epoch 092; Train loss: 0.0122; Time: 00:00:01
Fold 3 Epoch 093; Train loss: 0.0110; Time: 00:00:02
Fold 3 Epoch 094; Train loss: 0.0101; Time: 00:00:02
Fold 3 Epoch 095; Train loss: 0.0091; Time: 00:00:02
Fold 3 Epoch 096; Train loss: 0.0085; Time: 00:00:01
Fold 3 Epoch 097; Train loss: 0.0076; Time: 00:00:01
Fold 3 Epoch 098; Train loss: 0.0071; Time: 00:00:01
Fold 3 Epoch 099; Train loss: 0.0064; Time: 00:00:01
Fold 3 Epoch 100; Train loss: 0.0059; Time: 00:00:01
Fold 3: Evaluation at Epoch 100
HR@5       NDCG@5     HR@10      NDCG@10   
0.422185   0.340839   0.865894   0.488388  
Loss: 0.0207
[timesteps candidate 600] Fold 3: HR@10 = [np.float64(0.7400662251655629), np.float64(0.75), np.float64(0.7235099337748344), np.float64(0.7284768211920529), np.float64(0.7317880794701986), np.float64(0.8096026490066225), np.float64(0.847682119205298), np.float64(0.8344370860927153), np.float64(0.8675496688741722), np.float64(0.8658940397350994)]
Tuning timesteps:  80%|████████  | 4/5 [14:27<03:31, 211.79s/it]
========== Fold 3 ==========
INFO:root:Using statics: seq_size = 10, genre_vocab_size = 18
Fold 3 Epoch 001; Train loss: 1.2409; Time: 00:00:01
Fold 3 Epoch 002; Train loss: 1.0625; Time: 00:00:01
Fold 3 Epoch 003; Train loss: 0.9487; Time: 00:00:01
Fold 3 Epoch 004; Train loss: 0.8642; Time: 00:00:01
Fold 3 Epoch 005; Train loss: 0.8071; Time: 00:00:02
Fold 3 Epoch 006; Train loss: 0.7496; Time: 00:00:02
Fold 3 Epoch 007; Train loss: 0.7054; Time: 00:00:02
Fold 3 Epoch 008; Train loss: 0.6617; Time: 00:00:01
Fold 3 Epoch 009; Train loss: 0.6215; Time: 00:00:01
Fold 3 Epoch 010; Train loss: 0.5875; Time: 00:00:01
Fold 3: Evaluation at Epoch 10
HR@5       NDCG@5     HR@10      NDCG@10   
0.673841   0.479889   0.799669   0.520524  
Loss: 0.7033
Fold 3 Epoch 011; Train loss: 0.5561; Time: 00:00:01
Fold 3 Epoch 012; Train loss: 0.5289; Time: 00:00:02
Fold 3 Epoch 013; Train loss: 0.4929; Time: 00:00:01
Fold 3 Epoch 014; Train loss: 0.4735; Time: 00:00:01
Fold 3 Epoch 015; Train loss: 0.4486; Time: 00:00:02
Fold 3 Epoch 016; Train loss: 0.4327; Time: 00:00:01
Fold 3 Epoch 017; Train loss: 0.4101; Time: 00:00:01
Fold 3 Epoch 018; Train loss: 0.4026; Time: 00:00:01
Fold 3 Epoch 019; Train loss: 0.3849; Time: 00:00:01
Fold 3 Epoch 020; Train loss: 0.3750; Time: 00:00:02
Fold 3: Evaluation at Epoch 20
HR@5       NDCG@5     HR@10      NDCG@10   
0.650662   0.539207   0.784768   0.582893  
Loss: 0.4490
Fold 3 Epoch 021; Train loss: 0.3570; Time: 00:00:01
Fold 3 Epoch 022; Train loss: 0.3412; Time: 00:00:01
Fold 3 Epoch 023; Train loss: 0.3341; Time: 00:00:01
Fold 3 Epoch 024; Train loss: 0.3185; Time: 00:00:01
Fold 3 Epoch 025; Train loss: 0.3103; Time: 00:00:02
Fold 3 Epoch 026; Train loss: 0.3014; Time: 00:00:01
Fold 3 Epoch 027; Train loss: 0.2958; Time: 00:00:02
Fold 3 Epoch 028; Train loss: 0.2881; Time: 00:00:02
Fold 3 Epoch 029; Train loss: 0.2756; Time: 00:00:02
Fold 3 Epoch 030; Train loss: 0.2646; Time: 00:00:01
Fold 3: Evaluation at Epoch 30
HR@5       NDCG@5     HR@10      NDCG@10   
0.620861   0.491677   0.753311   0.534597  
Loss: 0.3282
Fold 3 Epoch 031; Train loss: 0.2627; Time: 00:00:01
Fold 3 Epoch 032; Train loss: 0.2548; Time: 00:00:01
Fold 3 Epoch 033; Train loss: 0.2438; Time: 00:00:01
Fold 3 Epoch 034; Train loss: 0.2421; Time: 00:00:02
Fold 3 Epoch 035; Train loss: 0.2355; Time: 00:00:02
Fold 3 Epoch 036; Train loss: 0.2307; Time: 00:00:02
Fold 3 Epoch 037; Train loss: 0.2180; Time: 00:00:01
Fold 3 Epoch 038; Train loss: 0.2160; Time: 00:00:02
Fold 3 Epoch 039; Train loss: 0.2080; Time: 00:00:02
Fold 3 Epoch 040; Train loss: 0.2050; Time: 00:00:01
Fold 3: Evaluation at Epoch 40
HR@5       NDCG@5     HR@10      NDCG@10   
0.644040   0.522440   0.764901   0.561292  
Loss: 0.2514
Fold 3 Epoch 041; Train loss: 0.2015; Time: 00:00:02
Fold 3 Epoch 042; Train loss: 0.1942; Time: 00:00:02
Fold 3 Epoch 043; Train loss: 0.1898; Time: 00:00:01
Fold 3 Epoch 044; Train loss: 0.1856; Time: 00:00:01
Fold 3 Epoch 045; Train loss: 0.1802; Time: 00:00:02
Fold 3 Epoch 046; Train loss: 0.1776; Time: 00:00:01
Fold 3 Epoch 047; Train loss: 0.1667; Time: 00:00:01
Fold 3 Epoch 048; Train loss: 0.1626; Time: 00:00:02
Fold 3 Epoch 049; Train loss: 0.1615; Time: 00:00:01
Fold 3 Epoch 050; Train loss: 0.1542; Time: 00:00:01
Fold 3: Evaluation at Epoch 50
HR@5       NDCG@5     HR@10      NDCG@10   
0.644040   0.522733   0.761589   0.560830  
Loss: 0.1957
Fold 3 Epoch 051; Train loss: 0.1516; Time: 00:00:01
Fold 3 Epoch 052; Train loss: 0.1481; Time: 00:00:01
Fold 3 Epoch 053; Train loss: 0.1435; Time: 00:00:01
Fold 3 Epoch 054; Train loss: 0.1378; Time: 00:00:01
Fold 3 Epoch 055; Train loss: 0.1335; Time: 00:00:01
Fold 3 Epoch 056; Train loss: 0.1298; Time: 00:00:02
Fold 3 Epoch 057; Train loss: 0.1266; Time: 00:00:02
Fold 3 Epoch 058; Train loss: 0.1204; Time: 00:00:02
Fold 3 Epoch 059; Train loss: 0.1178; Time: 00:00:02
Fold 3 Epoch 060; Train loss: 0.1148; Time: 00:00:02
Fold 3: Evaluation at Epoch 60
HR@5       NDCG@5     HR@10      NDCG@10   
0.650662   0.546241   0.811258   0.598342  
Loss: 0.1480
Fold 3 Epoch 061; Train loss: 0.1102; Time: 00:00:01
Fold 3 Epoch 062; Train loss: 0.1079; Time: 00:00:01
Fold 3 Epoch 063; Train loss: 0.1047; Time: 00:00:01
Fold 3 Epoch 064; Train loss: 0.0989; Time: 00:00:01
Fold 3 Epoch 065; Train loss: 0.0942; Time: 00:00:01
Fold 3 Epoch 066; Train loss: 0.0907; Time: 00:00:01
Fold 3 Epoch 067; Train loss: 0.0887; Time: 00:00:02
Fold 3 Epoch 068; Train loss: 0.0849; Time: 00:00:01
Fold 3 Epoch 069; Train loss: 0.0833; Time: 00:00:01
Fold 3 Epoch 070; Train loss: 0.0772; Time: 00:00:01
Fold 3: Evaluation at Epoch 70
HR@5       NDCG@5     HR@10      NDCG@10   
0.680464   0.537562   0.832781   0.587178  
Loss: 0.1078
Fold 3 Epoch 071; Train loss: 0.0747; Time: 00:00:01
Fold 3 Epoch 072; Train loss: 0.0713; Time: 00:00:01
Fold 3 Epoch 073; Train loss: 0.0670; Time: 00:00:01
Fold 3 Epoch 074; Train loss: 0.0657; Time: 00:00:01
Fold 3 Epoch 075; Train loss: 0.0620; Time: 00:00:01
Fold 3 Epoch 076; Train loss: 0.0601; Time: 00:00:02
Fold 3 Epoch 077; Train loss: 0.0565; Time: 00:00:02
Fold 3 Epoch 078; Train loss: 0.0537; Time: 00:00:02
Fold 3 Epoch 079; Train loss: 0.0509; Time: 00:00:02
Fold 3 Epoch 080; Train loss: 0.0480; Time: 00:00:02
Fold 3: Evaluation at Epoch 80
HR@5       NDCG@5     HR@10      NDCG@10   
0.693709   0.518397   0.885762   0.581568  
Loss: 0.0712
Fold 3 Epoch 081; Train loss: 0.0453; Time: 00:00:01
Fold 3 Epoch 082; Train loss: 0.0431; Time: 00:00:01
Fold 3 Epoch 083; Train loss: 0.0405; Time: 00:00:02
Fold 3 Epoch 084; Train loss: 0.0381; Time: 00:00:01
Fold 3 Epoch 085; Train loss: 0.0359; Time: 00:00:01
Fold 3 Epoch 086; Train loss: 0.0335; Time: 00:00:02
Fold 3 Epoch 087; Train loss: 0.0309; Time: 00:00:02
Fold 3 Epoch 088; Train loss: 0.0290; Time: 00:00:02
Fold 3 Epoch 089; Train loss: 0.0267; Time: 00:00:01
Fold 3 Epoch 090; Train loss: 0.0250; Time: 00:00:01
Fold 3: Evaluation at Epoch 90
HR@5       NDCG@5     HR@10      NDCG@10   
0.705298   0.502115   0.894040   0.565036  
Loss: 0.0423
Fold 3 Epoch 091; Train loss: 0.0236; Time: 00:00:01
Fold 3 Epoch 092; Train loss: 0.0215; Time: 00:00:01
Fold 3 Epoch 093; Train loss: 0.0198; Time: 00:00:01
Fold 3 Epoch 094; Train loss: 0.0178; Time: 00:00:02
Fold 3 Epoch 095; Train loss: 0.0170; Time: 00:00:02
Fold 3 Epoch 096; Train loss: 0.0152; Time: 00:00:01
Fold 3 Epoch 097; Train loss: 0.0139; Time: 00:00:01
Fold 3 Epoch 098; Train loss: 0.0127; Time: 00:00:01
Fold 3 Epoch 099; Train loss: 0.0116; Time: 00:00:01
Fold 3 Epoch 100; Train loss: 0.0105; Time: 00:00:01
Fold 3: Evaluation at Epoch 100
HR@5       NDCG@5     HR@10      NDCG@10   
0.706954   0.490454   0.890728   0.553093  
Loss: 0.0222
[timesteps candidate 800] Fold 3: HR@10 = [np.float64(0.7996688741721855), np.float64(0.7847682119205298), np.float64(0.7533112582781457), np.float64(0.7649006622516556), np.float64(0.7615894039735099), np.float64(0.8112582781456954), np.float64(0.8327814569536424), np.float64(0.8857615894039735), np.float64(0.8940397350993378), np.float64(0.890728476821192)]
Tuning timesteps: 100%|██████████| 5/5 [17:56<00:00, 210.87s/it]Tuning timesteps: 100%|██████████| 5/5 [17:56<00:00, 215.25s/it]

Best timesteps found: 800
Tuning dropout_rate:   0%|          | 0/7 [00:00<?, ?it/s]
========== Fold 3 ==========
INFO:root:Using statics: seq_size = 10, genre_vocab_size = 18
Fold 3 Epoch 001; Train loss: 1.0664; Time: 00:00:02
Fold 3 Epoch 002; Train loss: 0.9166; Time: 00:00:02
Fold 3 Epoch 003; Train loss: 0.8198; Time: 00:00:02
Fold 3 Epoch 004; Train loss: 0.7518; Time: 00:00:01
Fold 3 Epoch 005; Train loss: 0.7048; Time: 00:00:02
Fold 3 Epoch 006; Train loss: 0.6596; Time: 00:00:02
Fold 3 Epoch 007; Train loss: 0.6159; Time: 00:00:02
Fold 3 Epoch 008; Train loss: 0.5847; Time: 00:00:01
Fold 3 Epoch 009; Train loss: 0.5436; Time: 00:00:01
Fold 3 Epoch 010; Train loss: 0.5171; Time: 00:00:01
Fold 3: Evaluation at Epoch 10
HR@5       NDCG@5     HR@10      NDCG@10   
0.304636   0.254222   0.677152   0.371129  
Loss: 0.6287
Fold 3 Epoch 011; Train loss: 0.4876; Time: 00:00:01
Fold 3 Epoch 012; Train loss: 0.4644; Time: 00:00:01
Fold 3 Epoch 013; Train loss: 0.4429; Time: 00:00:01
Fold 3 Epoch 014; Train loss: 0.4241; Time: 00:00:02
Fold 3 Epoch 015; Train loss: 0.4060; Time: 00:00:02
Fold 3 Epoch 016; Train loss: 0.3903; Time: 00:00:02
Fold 3 Epoch 017; Train loss: 0.3750; Time: 00:00:02
Fold 3 Epoch 018; Train loss: 0.3532; Time: 00:00:02
Fold 3 Epoch 019; Train loss: 0.3417; Time: 00:00:02
Fold 3 Epoch 020; Train loss: 0.3300; Time: 00:00:02
Fold 3: Evaluation at Epoch 20
HR@5       NDCG@5     HR@10      NDCG@10   
0.652318   0.535084   0.771523   0.573075  
Loss: 0.4068
Fold 3 Epoch 021; Train loss: 0.3197; Time: 00:00:01
Fold 3 Epoch 022; Train loss: 0.3062; Time: 00:00:01
Fold 3 Epoch 023; Train loss: 0.2941; Time: 00:00:01
Fold 3 Epoch 024; Train loss: 0.2871; Time: 00:00:02
Fold 3 Epoch 025; Train loss: 0.2796; Time: 00:00:02
Fold 3 Epoch 026; Train loss: 0.2636; Time: 00:00:02
Fold 3 Epoch 027; Train loss: 0.2611; Time: 00:00:02
Fold 3 Epoch 028; Train loss: 0.2485; Time: 00:00:02
Fold 3 Epoch 029; Train loss: 0.2440; Time: 00:00:02
Fold 3 Epoch 030; Train loss: 0.2383; Time: 00:00:02
Fold 3: Evaluation at Epoch 30
HR@5       NDCG@5     HR@10      NDCG@10   
0.635762   0.516086   0.806291   0.570522  
Loss: 0.3025
Fold 3 Epoch 031; Train loss: 0.2319; Time: 00:00:01
Fold 3 Epoch 032; Train loss: 0.2247; Time: 00:00:01
Fold 3 Epoch 033; Train loss: 0.2154; Time: 00:00:02
Fold 3 Epoch 034; Train loss: 0.2163; Time: 00:00:02
Fold 3 Epoch 035; Train loss: 0.2051; Time: 00:00:02
Fold 3 Epoch 036; Train loss: 0.1993; Time: 00:00:02
Fold 3 Epoch 037; Train loss: 0.1957; Time: 00:00:02
Fold 3 Epoch 038; Train loss: 0.1861; Time: 00:00:02
Fold 3 Epoch 039; Train loss: 0.1834; Time: 00:00:02
Fold 3 Epoch 040; Train loss: 0.1790; Time: 00:00:02
Fold 3: Evaluation at Epoch 40
HR@5       NDCG@5     HR@10      NDCG@10   
0.594371   0.503709   0.774834   0.561095  
Loss: 0.2370
Fold 3 Epoch 041; Train loss: 0.1678; Time: 00:00:02
Fold 3 Epoch 042; Train loss: 0.1702; Time: 00:00:02
Fold 3 Epoch 043; Train loss: 0.1630; Time: 00:00:02
Fold 3 Epoch 044; Train loss: 0.1578; Time: 00:00:02
Fold 3 Epoch 045; Train loss: 0.1567; Time: 00:00:02
Fold 3 Epoch 046; Train loss: 0.1489; Time: 00:00:02
Fold 3 Epoch 047; Train loss: 0.1464; Time: 00:00:02
Fold 3 Epoch 048; Train loss: 0.1416; Time: 00:00:02
Fold 3 Epoch 049; Train loss: 0.1386; Time: 00:00:02
Fold 3 Epoch 050; Train loss: 0.1338; Time: 00:00:02
Fold 3: Evaluation at Epoch 50
HR@5       NDCG@5     HR@10      NDCG@10   
0.600993   0.489378   0.754967   0.538329  
Loss: 0.1781
Fold 3 Epoch 051; Train loss: 0.1284; Time: 00:00:02
Fold 3 Epoch 052; Train loss: 0.1228; Time: 00:00:01
Fold 3 Epoch 053; Train loss: 0.1202; Time: 00:00:02
Fold 3 Epoch 054; Train loss: 0.1185; Time: 00:00:02
Fold 3 Epoch 055; Train loss: 0.1133; Time: 00:00:02
Fold 3 Epoch 056; Train loss: 0.1108; Time: 00:00:02
Fold 3 Epoch 057; Train loss: 0.1060; Time: 00:00:02
Fold 3 Epoch 058; Train loss: 0.1025; Time: 00:00:02
Fold 3 Epoch 059; Train loss: 0.0988; Time: 00:00:02
Fold 3 Epoch 060; Train loss: 0.0952; Time: 00:00:02
Fold 3: Evaluation at Epoch 60
HR@5       NDCG@5     HR@10      NDCG@10   
0.605960   0.486092   0.783113   0.541874  
Loss: 0.1309
Fold 3 Epoch 061; Train loss: 0.0899; Time: 00:00:02
Fold 3 Epoch 062; Train loss: 0.0878; Time: 00:00:02
Fold 3 Epoch 063; Train loss: 0.0853; Time: 00:00:02
Fold 3 Epoch 064; Train loss: 0.0808; Time: 00:00:02
Fold 3 Epoch 065; Train loss: 0.0781; Time: 00:00:02
Fold 3 Epoch 066; Train loss: 0.0744; Time: 00:00:02
Fold 3 Epoch 067; Train loss: 0.0712; Time: 00:00:02
Fold 3 Epoch 068; Train loss: 0.0674; Time: 00:00:02
Fold 3 Epoch 069; Train loss: 0.0661; Time: 00:00:02
Fold 3 Epoch 070; Train loss: 0.0619; Time: 00:00:02
Fold 3: Evaluation at Epoch 70
HR@5       NDCG@5     HR@10      NDCG@10   
0.579470   0.451708   0.764901   0.511612  
Loss: 0.0912
Fold 3 Epoch 071; Train loss: 0.0578; Time: 00:00:02
Fold 3 Epoch 072; Train loss: 0.0571; Time: 00:00:02
Fold 3 Epoch 073; Train loss: 0.0562; Time: 00:00:02
Fold 3 Epoch 074; Train loss: 0.0513; Time: 00:00:02
Fold 3 Epoch 075; Train loss: 0.0481; Time: 00:00:02
Fold 3 Epoch 076; Train loss: 0.0453; Time: 00:00:02
Fold 3 Epoch 077; Train loss: 0.0441; Time: 00:00:02
Fold 3 Epoch 078; Train loss: 0.0414; Time: 00:00:02
Fold 3 Epoch 079; Train loss: 0.0386; Time: 00:00:02
Fold 3 Epoch 080; Train loss: 0.0367; Time: 00:00:02
Fold 3: Evaluation at Epoch 80
HR@5       NDCG@5     HR@10      NDCG@10   
0.569536   0.420666   0.814570   0.497736  
Loss: 0.0603
Fold 3 Epoch 081; Train loss: 0.0347; Time: 00:00:02
Fold 3 Epoch 082; Train loss: 0.0321; Time: 00:00:02
Fold 3 Epoch 083; Train loss: 0.0300; Time: 00:00:02
Fold 3 Epoch 084; Train loss: 0.0285; Time: 00:00:02
Fold 3 Epoch 085; Train loss: 0.0261; Time: 00:00:02
Fold 3 Epoch 086; Train loss: 0.0238; Time: 00:00:02
Fold 3 Epoch 087; Train loss: 0.0223; Time: 00:00:02
Fold 3 Epoch 088; Train loss: 0.0213; Time: 00:00:02
Fold 3 Epoch 089; Train loss: 0.0197; Time: 00:00:01
Fold 3 Epoch 090; Train loss: 0.0176; Time: 00:00:01
Fold 3: Evaluation at Epoch 90
HR@5       NDCG@5     HR@10      NDCG@10   
0.501656   0.301577   0.852649   0.410729  
Loss: 0.0361
Fold 3 Epoch 091; Train loss: 0.0166; Time: 00:00:01
Fold 3 Epoch 092; Train loss: 0.0153; Time: 00:00:02
Fold 3 Epoch 093; Train loss: 0.0141; Time: 00:00:02
Fold 3 Epoch 094; Train loss: 0.0131; Time: 00:00:02
Fold 3 Epoch 095; Train loss: 0.0121; Time: 00:00:02
Fold 3 Epoch 096; Train loss: 0.0111; Time: 00:00:02
Fold 3 Epoch 097; Train loss: 0.0095; Time: 00:00:02
Fold 3 Epoch 098; Train loss: 0.0087; Time: 00:00:02
Fold 3 Epoch 099; Train loss: 0.0080; Time: 00:00:02
Fold 3 Epoch 100; Train loss: 0.0071; Time: 00:00:02
Fold 3: Evaluation at Epoch 100
HR@5       NDCG@5     HR@10      NDCG@10   
0.453642   0.227032   0.865894   0.358921  
Loss: 0.0199
[dropout_rate candidate 0.05] Fold 3: HR@10 = [np.float64(0.6771523178807947), np.float64(0.7715231788079471), np.float64(0.8062913907284768), np.float64(0.7748344370860927), np.float64(0.7549668874172185), np.float64(0.7831125827814569), np.float64(0.7649006622516556), np.float64(0.8145695364238411), np.float64(0.8526490066225165), np.float64(0.8658940397350994)]
Tuning dropout_rate:  14%|█▍        | 1/7 [04:07<24:43, 247.31s/it]
========== Fold 3 ==========
INFO:root:Using statics: seq_size = 10, genre_vocab_size = 18
Fold 3 Epoch 001; Train loss: 1.1317; Time: 00:00:02
Fold 3 Epoch 002; Train loss: 0.9704; Time: 00:00:02
Fold 3 Epoch 003; Train loss: 0.8682; Time: 00:00:02
Fold 3 Epoch 004; Train loss: 0.7916; Time: 00:00:02
Fold 3 Epoch 005; Train loss: 0.7380; Time: 00:00:02
Fold 3 Epoch 006; Train loss: 0.6894; Time: 00:00:02
Fold 3 Epoch 007; Train loss: 0.6408; Time: 00:00:02
Fold 3 Epoch 008; Train loss: 0.6030; Time: 00:00:02
Fold 3 Epoch 009; Train loss: 0.5725; Time: 00:00:02
Fold 3 Epoch 010; Train loss: 0.5398; Time: 00:00:02
Fold 3: Evaluation at Epoch 10
HR@5       NDCG@5     HR@10      NDCG@10   
0.768212   0.529321   0.902318   0.572918  
Loss: 0.6242
Fold 3 Epoch 011; Train loss: 0.5103; Time: 00:00:02
Fold 3 Epoch 012; Train loss: 0.4813; Time: 00:00:02
Fold 3 Epoch 013; Train loss: 0.4611; Time: 00:00:02
Fold 3 Epoch 014; Train loss: 0.4390; Time: 00:00:02
Fold 3 Epoch 015; Train loss: 0.4196; Time: 00:00:02
Fold 3 Epoch 016; Train loss: 0.4048; Time: 00:00:02
Fold 3 Epoch 017; Train loss: 0.3857; Time: 00:00:02
Fold 3 Epoch 018; Train loss: 0.3708; Time: 00:00:02
Fold 3 Epoch 019; Train loss: 0.3633; Time: 00:00:02
Fold 3 Epoch 020; Train loss: 0.3441; Time: 00:00:02
Fold 3: Evaluation at Epoch 20
HR@5       NDCG@5     HR@10      NDCG@10   
0.753311   0.549852   0.875828   0.589959  
Loss: 0.4108
Fold 3 Epoch 021; Train loss: 0.3274; Time: 00:00:02
Fold 3 Epoch 022; Train loss: 0.3161; Time: 00:00:02
Fold 3 Epoch 023; Train loss: 0.3107; Time: 00:00:02
Fold 3 Epoch 024; Train loss: 0.2959; Time: 00:00:02
Fold 3 Epoch 025; Train loss: 0.2857; Time: 00:00:02
Fold 3 Epoch 026; Train loss: 0.2788; Time: 00:00:02
Fold 3 Epoch 027; Train loss: 0.2633; Time: 00:00:02
Fold 3 Epoch 028; Train loss: 0.2675; Time: 00:00:02
Fold 3 Epoch 029; Train loss: 0.2553; Time: 00:00:02
Fold 3 Epoch 030; Train loss: 0.2450; Time: 00:00:02
Fold 3: Evaluation at Epoch 30
HR@5       NDCG@5     HR@10      NDCG@10   
0.688742   0.525729   0.829470   0.571110  
Loss: 0.2824
Fold 3 Epoch 031; Train loss: 0.2394; Time: 00:00:01
Fold 3 Epoch 032; Train loss: 0.2319; Time: 00:00:02
Fold 3 Epoch 033; Train loss: 0.2255; Time: 00:00:02
Fold 3 Epoch 034; Train loss: 0.2232; Time: 00:00:02
Fold 3 Epoch 035; Train loss: 0.2170; Time: 00:00:02
Fold 3 Epoch 036; Train loss: 0.2082; Time: 00:00:02
Fold 3 Epoch 037; Train loss: 0.2026; Time: 00:00:02
Fold 3 Epoch 038; Train loss: 0.1958; Time: 00:00:02
Fold 3 Epoch 039; Train loss: 0.1920; Time: 00:00:02
Fold 3 Epoch 040; Train loss: 0.1883; Time: 00:00:02
Fold 3: Evaluation at Epoch 40
HR@5       NDCG@5     HR@10      NDCG@10   
0.685430   0.535009   0.804636   0.572703  
Loss: 0.2282
Fold 3 Epoch 041; Train loss: 0.1803; Time: 00:00:02
Fold 3 Epoch 042; Train loss: 0.1783; Time: 00:00:02
Fold 3 Epoch 043; Train loss: 0.1731; Time: 00:00:02
Fold 3 Epoch 044; Train loss: 0.1703; Time: 00:00:01
Fold 3 Epoch 045; Train loss: 0.1630; Time: 00:00:02
Fold 3 Epoch 046; Train loss: 0.1587; Time: 00:00:01
Fold 3 Epoch 047; Train loss: 0.1532; Time: 00:00:01
Fold 3 Epoch 048; Train loss: 0.1494; Time: 00:00:01
Fold 3 Epoch 049; Train loss: 0.1441; Time: 00:00:01
Fold 3 Epoch 050; Train loss: 0.1388; Time: 00:00:01
Fold 3: Evaluation at Epoch 50
HR@5       NDCG@5     HR@10      NDCG@10   
0.668874   0.530843   0.778146   0.566464  
Loss: 0.1734
Fold 3 Epoch 051; Train loss: 0.1360; Time: 00:00:01
Fold 3 Epoch 052; Train loss: 0.1329; Time: 00:00:01
Fold 3 Epoch 053; Train loss: 0.1285; Time: 00:00:01
Fold 3 Epoch 054; Train loss: 0.1277; Time: 00:00:01
Fold 3 Epoch 055; Train loss: 0.1193; Time: 00:00:01
Fold 3 Epoch 056; Train loss: 0.1186; Time: 00:00:03
Fold 3 Epoch 057; Train loss: 0.1150; Time: 00:00:03
Fold 3 Epoch 058; Train loss: 0.1112; Time: 00:00:03
Fold 3 Epoch 059; Train loss: 0.1055; Time: 00:00:03
Fold 3 Epoch 060; Train loss: 0.1020; Time: 00:00:03
Fold 3: Evaluation at Epoch 60
HR@5       NDCG@5     HR@10      NDCG@10   
0.672185   0.537838   0.814570   0.584016  
Loss: 0.1342
Fold 3 Epoch 061; Train loss: 0.0973; Time: 00:00:03
Fold 3 Epoch 062; Train loss: 0.0955; Time: 00:00:02
Fold 3 Epoch 063; Train loss: 0.0904; Time: 00:00:02
Fold 3 Epoch 064; Train loss: 0.0892; Time: 00:00:02
Fold 3 Epoch 065; Train loss: 0.0844; Time: 00:00:02
Fold 3 Epoch 066; Train loss: 0.0822; Time: 00:00:01
Fold 3 Epoch 067; Train loss: 0.0762; Time: 00:00:02
Fold 3 Epoch 068; Train loss: 0.0734; Time: 00:00:01
Fold 3 Epoch 069; Train loss: 0.0720; Time: 00:00:01
Fold 3 Epoch 070; Train loss: 0.0684; Time: 00:00:02
Fold 3: Evaluation at Epoch 70
HR@5       NDCG@5     HR@10      NDCG@10   
0.620861   0.490797   0.821192   0.555902  
Loss: 0.0945
Fold 3 Epoch 071; Train loss: 0.0657; Time: 00:00:02
Fold 3 Epoch 072; Train loss: 0.0632; Time: 00:00:02
Fold 3 Epoch 073; Train loss: 0.0602; Time: 00:00:02
Fold 3 Epoch 074; Train loss: 0.0573; Time: 00:00:01
Fold 3 Epoch 075; Train loss: 0.0548; Time: 00:00:02
Fold 3 Epoch 076; Train loss: 0.0505; Time: 00:00:01
Fold 3 Epoch 077; Train loss: 0.0491; Time: 00:00:02
Fold 3 Epoch 078; Train loss: 0.0467; Time: 00:00:02
Fold 3 Epoch 079; Train loss: 0.0439; Time: 00:00:02
Fold 3 Epoch 080; Train loss: 0.0419; Time: 00:00:02
Fold 3: Evaluation at Epoch 80
HR@5       NDCG@5     HR@10      NDCG@10   
0.620861   0.438434   0.877483   0.523622  
Loss: 0.0614
Fold 3 Epoch 081; Train loss: 0.0388; Time: 00:00:01
Fold 3 Epoch 082; Train loss: 0.0367; Time: 00:00:01
Fold 3 Epoch 083; Train loss: 0.0342; Time: 00:00:02
Fold 3 Epoch 084; Train loss: 0.0327; Time: 00:00:02
Fold 3 Epoch 085; Train loss: 0.0304; Time: 00:00:02
Fold 3 Epoch 086; Train loss: 0.0285; Time: 00:00:02
Fold 3 Epoch 087; Train loss: 0.0269; Time: 00:00:02
Fold 3 Epoch 088; Train loss: 0.0249; Time: 00:00:02
Fold 3 Epoch 089; Train loss: 0.0231; Time: 00:00:02
Fold 3 Epoch 090; Train loss: 0.0212; Time: 00:00:02
Fold 3: Evaluation at Epoch 90
HR@5       NDCG@5     HR@10      NDCG@10   
0.518212   0.325689   0.884106   0.449154  
Loss: 0.0390
Fold 3 Epoch 091; Train loss: 0.0197; Time: 00:00:02
Fold 3 Epoch 092; Train loss: 0.0180; Time: 00:00:02
Fold 3 Epoch 093; Train loss: 0.0173; Time: 00:00:01
Fold 3 Epoch 094; Train loss: 0.0153; Time: 00:00:02
Fold 3 Epoch 095; Train loss: 0.0142; Time: 00:00:02
Fold 3 Epoch 096; Train loss: 0.0128; Time: 00:00:01
Fold 3 Epoch 097; Train loss: 0.0116; Time: 00:00:02
Fold 3 Epoch 098; Train loss: 0.0106; Time: 00:00:02
Fold 3 Epoch 099; Train loss: 0.0096; Time: 00:00:02
Fold 3 Epoch 100; Train loss: 0.0089; Time: 00:00:02
Fold 3: Evaluation at Epoch 100
HR@5       NDCG@5     HR@10      NDCG@10   
0.524834   0.307055   0.885762   0.429788  
Loss: 0.0202
[dropout_rate candidate 0.07] Fold 3: HR@10 = [np.float64(0.902317880794702), np.float64(0.8758278145695364), np.float64(0.8294701986754967), np.float64(0.804635761589404), np.float64(0.7781456953642384), np.float64(0.8145695364238411), np.float64(0.8211920529801324), np.float64(0.8774834437086093), np.float64(0.8841059602649006), np.float64(0.8857615894039735)]
Tuning dropout_rate:  29%|██▊       | 2/7 [08:17<20:44, 248.91s/it]
========== Fold 3 ==========
INFO:root:Using statics: seq_size = 10, genre_vocab_size = 18
Fold 3 Epoch 001; Train loss: 1.0661; Time: 00:00:02
Fold 3 Epoch 002; Train loss: 0.9096; Time: 00:00:02
Fold 3 Epoch 003; Train loss: 0.8065; Time: 00:00:02
Fold 3 Epoch 004; Train loss: 0.7394; Time: 00:00:02
Fold 3 Epoch 005; Train loss: 0.6912; Time: 00:00:02
Fold 3 Epoch 006; Train loss: 0.6457; Time: 00:00:02
Fold 3 Epoch 007; Train loss: 0.6032; Time: 00:00:02
Fold 3 Epoch 008; Train loss: 0.5675; Time: 00:00:02
Fold 3 Epoch 009; Train loss: 0.5375; Time: 00:00:02
Fold 3 Epoch 010; Train loss: 0.5013; Time: 00:00:02
Fold 3: Evaluation at Epoch 10
HR@5       NDCG@5     HR@10      NDCG@10   
0.658940   0.522861   0.822848   0.575567  
Loss: 0.6347
Fold 3 Epoch 011; Train loss: 0.4703; Time: 00:00:02
Fold 3 Epoch 012; Train loss: 0.4576; Time: 00:00:02
Fold 3 Epoch 013; Train loss: 0.4307; Time: 00:00:02
Fold 3 Epoch 014; Train loss: 0.4119; Time: 00:00:02
Fold 3 Epoch 015; Train loss: 0.3887; Time: 00:00:02
Fold 3 Epoch 016; Train loss: 0.3740; Time: 00:00:02
Fold 3 Epoch 017; Train loss: 0.3616; Time: 00:00:02
Fold 3 Epoch 018; Train loss: 0.3447; Time: 00:00:02
Fold 3 Epoch 019; Train loss: 0.3308; Time: 00:00:02
Fold 3 Epoch 020; Train loss: 0.3151; Time: 00:00:01
Fold 3: Evaluation at Epoch 20
HR@5       NDCG@5     HR@10      NDCG@10   
0.624172   0.498273   0.786424   0.550846  
Loss: 0.4211
Fold 3 Epoch 021; Train loss: 0.3048; Time: 00:00:02
Fold 3 Epoch 022; Train loss: 0.2867; Time: 00:00:02
Fold 3 Epoch 023; Train loss: 0.2858; Time: 00:00:02
Fold 3 Epoch 024; Train loss: 0.2756; Time: 00:00:02
Fold 3 Epoch 025; Train loss: 0.2633; Time: 00:00:02
Fold 3 Epoch 026; Train loss: 0.2551; Time: 00:00:02
Fold 3 Epoch 027; Train loss: 0.2474; Time: 00:00:02
Fold 3 Epoch 028; Train loss: 0.2427; Time: 00:00:02
Fold 3 Epoch 029; Train loss: 0.2328; Time: 00:00:02
Fold 3 Epoch 030; Train loss: 0.2277; Time: 00:00:02
Fold 3: Evaluation at Epoch 30
HR@5       NDCG@5     HR@10      NDCG@10   
0.627483   0.521287   0.759934   0.563522  
Loss: 0.2869
Fold 3 Epoch 031; Train loss: 0.2155; Time: 00:00:02
Fold 3 Epoch 032; Train loss: 0.2119; Time: 00:00:02
Fold 3 Epoch 033; Train loss: 0.2039; Time: 00:00:02
Fold 3 Epoch 034; Train loss: 0.1973; Time: 00:00:02
Fold 3 Epoch 035; Train loss: 0.1974; Time: 00:00:02
Fold 3 Epoch 036; Train loss: 0.1897; Time: 00:00:02
Fold 3 Epoch 037; Train loss: 0.1846; Time: 00:00:02
Fold 3 Epoch 038; Train loss: 0.1822; Time: 00:00:02
Fold 3 Epoch 039; Train loss: 0.1749; Time: 00:00:02
Fold 3 Epoch 040; Train loss: 0.1677; Time: 00:00:02
Fold 3: Evaluation at Epoch 40
HR@5       NDCG@5     HR@10      NDCG@10   
0.614238   0.504622   0.763245   0.552606  
Loss: 0.2300
Fold 3 Epoch 041; Train loss: 0.1666; Time: 00:00:02
Fold 3 Epoch 042; Train loss: 0.1625; Time: 00:00:02
Fold 3 Epoch 043; Train loss: 0.1528; Time: 00:00:02
Fold 3 Epoch 044; Train loss: 0.1521; Time: 00:00:02
Fold 3 Epoch 045; Train loss: 0.1497; Time: 00:00:02
Fold 3 Epoch 046; Train loss: 0.1423; Time: 00:00:02
Fold 3 Epoch 047; Train loss: 0.1414; Time: 00:00:02
Fold 3 Epoch 048; Train loss: 0.1368; Time: 00:00:02
Fold 3 Epoch 049; Train loss: 0.1301; Time: 00:00:01
Fold 3 Epoch 050; Train loss: 0.1256; Time: 00:00:02
Fold 3: Evaluation at Epoch 50
HR@5       NDCG@5     HR@10      NDCG@10   
0.619205   0.499522   0.768212   0.547502  
Loss: 0.1776
Fold 3 Epoch 051; Train loss: 0.1201; Time: 00:00:02
Fold 3 Epoch 052; Train loss: 0.1206; Time: 00:00:02
Fold 3 Epoch 053; Train loss: 0.1136; Time: 00:00:02
Fold 3 Epoch 054; Train loss: 0.1123; Time: 00:00:01
Fold 3 Epoch 055; Train loss: 0.1075; Time: 00:00:02
Fold 3 Epoch 056; Train loss: 0.1039; Time: 00:00:02
Fold 3 Epoch 057; Train loss: 0.1021; Time: 00:00:02
Fold 3 Epoch 058; Train loss: 0.0972; Time: 00:00:02
Fold 3 Epoch 059; Train loss: 0.0936; Time: 00:00:02
Fold 3 Epoch 060; Train loss: 0.0901; Time: 00:00:02
Fold 3: Evaluation at Epoch 60
HR@5       NDCG@5     HR@10      NDCG@10   
0.617550   0.491063   0.786424   0.545048  
Loss: 0.1374
Fold 3 Epoch 061; Train loss: 0.0876; Time: 00:00:02
Fold 3 Epoch 062; Train loss: 0.0835; Time: 00:00:01
Fold 3 Epoch 063; Train loss: 0.0806; Time: 00:00:01
Fold 3 Epoch 064; Train loss: 0.0766; Time: 00:00:02
Fold 3 Epoch 065; Train loss: 0.0753; Time: 00:00:01
Fold 3 Epoch 066; Train loss: 0.0725; Time: 00:00:02
Fold 3 Epoch 067; Train loss: 0.0683; Time: 00:00:02
Fold 3 Epoch 068; Train loss: 0.0667; Time: 00:00:02
Fold 3 Epoch 069; Train loss: 0.0639; Time: 00:00:02
Fold 3 Epoch 070; Train loss: 0.0613; Time: 00:00:02
Fold 3: Evaluation at Epoch 70
HR@5       NDCG@5     HR@10      NDCG@10   
0.667219   0.441468   0.867550   0.507545  
Loss: 0.0934
Fold 3 Epoch 071; Train loss: 0.0576; Time: 00:00:01
Fold 3 Epoch 072; Train loss: 0.0559; Time: 00:00:02
Fold 3 Epoch 073; Train loss: 0.0526; Time: 00:00:02
Fold 3 Epoch 074; Train loss: 0.0493; Time: 00:00:01
Fold 3 Epoch 075; Train loss: 0.0478; Time: 00:00:02
Fold 3 Epoch 076; Train loss: 0.0442; Time: 00:00:02
Fold 3 Epoch 077; Train loss: 0.0433; Time: 00:00:02
Fold 3 Epoch 078; Train loss: 0.0399; Time: 00:00:02
Fold 3 Epoch 079; Train loss: 0.0388; Time: 00:00:02
Fold 3 Epoch 080; Train loss: 0.0361; Time: 00:00:01
Fold 3: Evaluation at Epoch 80
HR@5       NDCG@5     HR@10      NDCG@10   
0.658940   0.399590   0.887417   0.477661  
Loss: 0.0626
Fold 3 Epoch 081; Train loss: 0.0336; Time: 00:00:01
Fold 3 Epoch 082; Train loss: 0.0318; Time: 00:00:02
Fold 3 Epoch 083; Train loss: 0.0299; Time: 00:00:02
Fold 3 Epoch 084; Train loss: 0.0280; Time: 00:00:01
Fold 3 Epoch 085; Train loss: 0.0262; Time: 00:00:01
Fold 3 Epoch 086; Train loss: 0.0253; Time: 00:00:02
Fold 3 Epoch 087; Train loss: 0.0234; Time: 00:00:02
Fold 3 Epoch 088; Train loss: 0.0217; Time: 00:00:02
Fold 3 Epoch 089; Train loss: 0.0201; Time: 00:00:02
Fold 3 Epoch 090; Train loss: 0.0187; Time: 00:00:02
Fold 3: Evaluation at Epoch 90
HR@5       NDCG@5     HR@10      NDCG@10   
0.637417   0.339921   0.913907   0.433556  
Loss: 0.0388
Fold 3 Epoch 091; Train loss: 0.0173; Time: 00:00:02
Fold 3 Epoch 092; Train loss: 0.0157; Time: 00:00:02
Fold 3 Epoch 093; Train loss: 0.0144; Time: 00:00:02
Fold 3 Epoch 094; Train loss: 0.0137; Time: 00:00:02
Fold 3 Epoch 095; Train loss: 0.0126; Time: 00:00:02
Fold 3 Epoch 096; Train loss: 0.0114; Time: 00:00:01
Fold 3 Epoch 097; Train loss: 0.0105; Time: 00:00:02
Fold 3 Epoch 098; Train loss: 0.0095; Time: 00:00:01
Fold 3 Epoch 099; Train loss: 0.0086; Time: 00:00:01
Fold 3 Epoch 100; Train loss: 0.0081; Time: 00:00:02
Fold 3: Evaluation at Epoch 100
HR@5       NDCG@5     HR@10      NDCG@10   
0.605960   0.323031   0.908940   0.426962  
Loss: 0.0206
[dropout_rate candidate 0.1] Fold 3: HR@10 = [np.float64(0.8228476821192053), np.float64(0.7864238410596026), np.float64(0.7599337748344371), np.float64(0.7632450331125827), np.float64(0.7682119205298014), np.float64(0.7864238410596026), np.float64(0.8675496688741722), np.float64(0.8874172185430463), np.float64(0.9139072847682119), np.float64(0.9089403973509934)]
Tuning dropout_rate:  43%|████▎     | 3/7 [12:26<16:36, 249.19s/it]
========== Fold 3 ==========
INFO:root:Using statics: seq_size = 10, genre_vocab_size = 18
Fold 3 Epoch 001; Train loss: 1.1659; Time: 00:00:02
Fold 3 Epoch 002; Train loss: 1.0045; Time: 00:00:02
Fold 3 Epoch 003; Train loss: 0.8832; Time: 00:00:02
Fold 3 Epoch 004; Train loss: 0.8026; Time: 00:00:02
Fold 3 Epoch 005; Train loss: 0.7412; Time: 00:00:02
Fold 3 Epoch 006; Train loss: 0.6974; Time: 00:00:02
Fold 3 Epoch 007; Train loss: 0.6554; Time: 00:00:02
Fold 3 Epoch 008; Train loss: 0.6106; Time: 00:00:02
Fold 3 Epoch 009; Train loss: 0.5772; Time: 00:00:02
Fold 3 Epoch 010; Train loss: 0.5430; Time: 00:00:02
Fold 3: Evaluation at Epoch 10
HR@5       NDCG@5     HR@10      NDCG@10   
0.622517   0.474870   0.761589   0.519674  
Loss: 0.6387
Fold 3 Epoch 011; Train loss: 0.5084; Time: 00:00:02
Fold 3 Epoch 012; Train loss: 0.4935; Time: 00:00:02
Fold 3 Epoch 013; Train loss: 0.4642; Time: 00:00:02
Fold 3 Epoch 014; Train loss: 0.4413; Time: 00:00:02
Fold 3 Epoch 015; Train loss: 0.4221; Time: 00:00:02
Fold 3 Epoch 016; Train loss: 0.3986; Time: 00:00:02
Fold 3 Epoch 017; Train loss: 0.3876; Time: 00:00:02
Fold 3 Epoch 018; Train loss: 0.3718; Time: 00:00:02
Fold 3 Epoch 019; Train loss: 0.3508; Time: 00:00:02
Fold 3 Epoch 020; Train loss: 0.3400; Time: 00:00:01
Fold 3: Evaluation at Epoch 20
HR@5       NDCG@5     HR@10      NDCG@10   
0.637417   0.488332   0.743377   0.522191  
Loss: 0.4220
Fold 3 Epoch 021; Train loss: 0.3357; Time: 00:00:01
Fold 3 Epoch 022; Train loss: 0.3211; Time: 00:00:01
Fold 3 Epoch 023; Train loss: 0.3133; Time: 00:00:02
Fold 3 Epoch 024; Train loss: 0.2993; Time: 00:00:02
Fold 3 Epoch 025; Train loss: 0.2908; Time: 00:00:01
Fold 3 Epoch 026; Train loss: 0.2770; Time: 00:00:02
Fold 3 Epoch 027; Train loss: 0.2718; Time: 00:00:02
Fold 3 Epoch 028; Train loss: 0.2628; Time: 00:00:02
Fold 3 Epoch 029; Train loss: 0.2545; Time: 00:00:02
Fold 3 Epoch 030; Train loss: 0.2470; Time: 00:00:01
Fold 3: Evaluation at Epoch 30
HR@5       NDCG@5     HR@10      NDCG@10   
0.630795   0.474804   0.781457   0.522622  
Loss: 0.3043
Fold 3 Epoch 031; Train loss: 0.2376; Time: 00:00:02
Fold 3 Epoch 032; Train loss: 0.2323; Time: 00:00:01
Fold 3 Epoch 033; Train loss: 0.2235; Time: 00:00:02
Fold 3 Epoch 034; Train loss: 0.2192; Time: 00:00:02
Fold 3 Epoch 035; Train loss: 0.2161; Time: 00:00:01
Fold 3 Epoch 036; Train loss: 0.2130; Time: 00:00:01
Fold 3 Epoch 037; Train loss: 0.2047; Time: 00:00:02
Fold 3 Epoch 038; Train loss: 0.1988; Time: 00:00:02
Fold 3 Epoch 039; Train loss: 0.1929; Time: 00:00:01
Fold 3 Epoch 040; Train loss: 0.1906; Time: 00:00:02
Fold 3: Evaluation at Epoch 40
HR@5       NDCG@5     HR@10      NDCG@10   
0.582781   0.454967   0.711921   0.495605  
Loss: 0.2389
Fold 3 Epoch 041; Train loss: 0.1840; Time: 00:00:02
Fold 3 Epoch 042; Train loss: 0.1773; Time: 00:00:02
Fold 3 Epoch 043; Train loss: 0.1708; Time: 00:00:02
Fold 3 Epoch 044; Train loss: 0.1699; Time: 00:00:02
Fold 3 Epoch 045; Train loss: 0.1661; Time: 00:00:02
Fold 3 Epoch 046; Train loss: 0.1607; Time: 00:00:02
Fold 3 Epoch 047; Train loss: 0.1537; Time: 00:00:02
Fold 3 Epoch 048; Train loss: 0.1516; Time: 00:00:01
Fold 3 Epoch 049; Train loss: 0.1448; Time: 00:00:01
Fold 3 Epoch 050; Train loss: 0.1424; Time: 00:00:01
Fold 3: Evaluation at Epoch 50
HR@5       NDCG@5     HR@10      NDCG@10   
0.640728   0.484206   0.802980   0.536632  
Loss: 0.1895
Fold 3 Epoch 051; Train loss: 0.1394; Time: 00:00:02
Fold 3 Epoch 052; Train loss: 0.1352; Time: 00:00:02
Fold 3 Epoch 053; Train loss: 0.1282; Time: 00:00:02
Fold 3 Epoch 054; Train loss: 0.1277; Time: 00:00:02
Fold 3 Epoch 055; Train loss: 0.1246; Time: 00:00:02
Fold 3 Epoch 056; Train loss: 0.1186; Time: 00:00:01
Fold 3 Epoch 057; Train loss: 0.1171; Time: 00:00:02
Fold 3 Epoch 058; Train loss: 0.1123; Time: 00:00:02
Fold 3 Epoch 059; Train loss: 0.1081; Time: 00:00:02
Fold 3 Epoch 060; Train loss: 0.1066; Time: 00:00:02
Fold 3: Evaluation at Epoch 60
HR@5       NDCG@5     HR@10      NDCG@10   
0.644040   0.475156   0.809603   0.528221  
Loss: 0.1411
Fold 3 Epoch 061; Train loss: 0.1013; Time: 00:00:02
Fold 3 Epoch 062; Train loss: 0.0969; Time: 00:00:02
Fold 3 Epoch 063; Train loss: 0.0922; Time: 00:00:02
Fold 3 Epoch 064; Train loss: 0.0891; Time: 00:00:02
Fold 3 Epoch 065; Train loss: 0.0893; Time: 00:00:02
Fold 3 Epoch 066; Train loss: 0.0829; Time: 00:00:02
Fold 3 Epoch 067; Train loss: 0.0814; Time: 00:00:02
Fold 3 Epoch 068; Train loss: 0.0764; Time: 00:00:02
Fold 3 Epoch 069; Train loss: 0.0749; Time: 00:00:02
Fold 3 Epoch 070; Train loss: 0.0717; Time: 00:00:01
Fold 3: Evaluation at Epoch 70
HR@5       NDCG@5     HR@10      NDCG@10   
0.687086   0.487951   0.870861   0.549330  
Loss: 0.1005
Fold 3 Epoch 071; Train loss: 0.0682; Time: 00:00:01
Fold 3 Epoch 072; Train loss: 0.0658; Time: 00:00:02
Fold 3 Epoch 073; Train loss: 0.0634; Time: 00:00:02
Fold 3 Epoch 074; Train loss: 0.0593; Time: 00:00:02
Fold 3 Epoch 075; Train loss: 0.0577; Time: 00:00:02
Fold 3 Epoch 076; Train loss: 0.0545; Time: 00:00:02
Fold 3 Epoch 077; Train loss: 0.0519; Time: 00:00:02
Fold 3 Epoch 078; Train loss: 0.0486; Time: 00:00:02
Fold 3 Epoch 079; Train loss: 0.0465; Time: 00:00:02
Fold 3 Epoch 080; Train loss: 0.0445; Time: 00:00:02
Fold 3: Evaluation at Epoch 80
HR@5       NDCG@5     HR@10      NDCG@10   
0.692053   0.491106   0.905629   0.562110  
Loss: 0.0691
Fold 3 Epoch 081; Train loss: 0.0422; Time: 00:00:02
Fold 3 Epoch 082; Train loss: 0.0401; Time: 00:00:02
Fold 3 Epoch 083; Train loss: 0.0382; Time: 00:00:02
Fold 3 Epoch 084; Train loss: 0.0356; Time: 00:00:02
Fold 3 Epoch 085; Train loss: 0.0337; Time: 00:00:02
Fold 3 Epoch 086; Train loss: 0.0311; Time: 00:00:02
Fold 3 Epoch 087; Train loss: 0.0297; Time: 00:00:02
Fold 3 Epoch 088; Train loss: 0.0283; Time: 00:00:02
Fold 3 Epoch 089; Train loss: 0.0264; Time: 00:00:02
Fold 3 Epoch 090; Train loss: 0.0236; Time: 00:00:02
Fold 3: Evaluation at Epoch 90
HR@5       NDCG@5     HR@10      NDCG@10   
0.695364   0.506123   0.907285   0.575403  
Loss: 0.0431
Fold 3 Epoch 091; Train loss: 0.0227; Time: 00:00:02
Fold 3 Epoch 092; Train loss: 0.0208; Time: 00:00:02
Fold 3 Epoch 093; Train loss: 0.0191; Time: 00:00:02
Fold 3 Epoch 094; Train loss: 0.0178; Time: 00:00:02
Fold 3 Epoch 095; Train loss: 0.0166; Time: 00:00:02
Fold 3 Epoch 096; Train loss: 0.0154; Time: 00:00:02
Fold 3 Epoch 097; Train loss: 0.0137; Time: 00:00:02
Fold 3 Epoch 098; Train loss: 0.0131; Time: 00:00:02
Fold 3 Epoch 099; Train loss: 0.0118; Time: 00:00:02
Fold 3 Epoch 100; Train loss: 0.0108; Time: 00:00:02
Fold 3: Evaluation at Epoch 100
HR@5       NDCG@5     HR@10      NDCG@10   
0.607616   0.455320   0.910596   0.555221  
Loss: 0.0239
[dropout_rate candidate 0.2] Fold 3: HR@10 = [np.float64(0.7615894039735099), np.float64(0.7433774834437086), np.float64(0.7814569536423841), np.float64(0.7119205298013245), np.float64(0.8029801324503312), np.float64(0.8096026490066225), np.float64(0.8708609271523179), np.float64(0.9056291390728477), np.float64(0.9072847682119205), np.float64(0.9105960264900662)]
Tuning dropout_rate:  57%|█████▋    | 4/7 [16:50<12:44, 254.71s/it]
========== Fold 3 ==========
INFO:root:Using statics: seq_size = 10, genre_vocab_size = 18
Fold 3 Epoch 001; Train loss: 1.0885; Time: 00:00:01
Fold 3 Epoch 002; Train loss: 0.9363; Time: 00:00:01
Fold 3 Epoch 003; Train loss: 0.8328; Time: 00:00:01
Fold 3 Epoch 004; Train loss: 0.7610; Time: 00:00:02
Fold 3 Epoch 005; Train loss: 0.7046; Time: 00:00:02
Fold 3 Epoch 006; Train loss: 0.6574; Time: 00:00:02
Fold 3 Epoch 007; Train loss: 0.6223; Time: 00:00:02
Fold 3 Epoch 008; Train loss: 0.5809; Time: 00:00:02
Fold 3 Epoch 009; Train loss: 0.5422; Time: 00:00:02
Fold 3 Epoch 010; Train loss: 0.5168; Time: 00:00:02
Fold 3: Evaluation at Epoch 10
HR@5       NDCG@5     HR@10      NDCG@10   
0.557947   0.415322   0.783113   0.487289  
Loss: 0.6857
Fold 3 Epoch 011; Train loss: 0.4871; Time: 00:00:02
Fold 3 Epoch 012; Train loss: 0.4633; Time: 00:00:02
Fold 3 Epoch 013; Train loss: 0.4450; Time: 00:00:02
Fold 3 Epoch 014; Train loss: 0.4245; Time: 00:00:02
Fold 3 Epoch 015; Train loss: 0.4000; Time: 00:00:02
Fold 3 Epoch 016; Train loss: 0.3840; Time: 00:00:02
Fold 3 Epoch 017; Train loss: 0.3699; Time: 00:00:02
Fold 3 Epoch 018; Train loss: 0.3521; Time: 00:00:02
Fold 3 Epoch 019; Train loss: 0.3439; Time: 00:00:02
Fold 3 Epoch 020; Train loss: 0.3299; Time: 00:00:02
Fold 3: Evaluation at Epoch 20
HR@5       NDCG@5     HR@10      NDCG@10   
0.624172   0.498145   0.779801   0.548172  
Loss: 0.4533
Fold 3 Epoch 021; Train loss: 0.3172; Time: 00:00:02
Fold 3 Epoch 022; Train loss: 0.3061; Time: 00:00:02
Fold 3 Epoch 023; Train loss: 0.2916; Time: 00:00:02
Fold 3 Epoch 024; Train loss: 0.2795; Time: 00:00:02
Fold 3 Epoch 025; Train loss: 0.2757; Time: 00:00:02
Fold 3 Epoch 026; Train loss: 0.2621; Time: 00:00:02
Fold 3 Epoch 027; Train loss: 0.2533; Time: 00:00:02
Fold 3 Epoch 028; Train loss: 0.2480; Time: 00:00:02
Fold 3 Epoch 029; Train loss: 0.2384; Time: 00:00:02
Fold 3 Epoch 030; Train loss: 0.2352; Time: 00:00:02
Fold 3: Evaluation at Epoch 30
HR@5       NDCG@5     HR@10      NDCG@10   
0.612583   0.513739   0.748344   0.557498  
Loss: 0.3248
Fold 3 Epoch 031; Train loss: 0.2264; Time: 00:00:02
Fold 3 Epoch 032; Train loss: 0.2174; Time: 00:00:02
Fold 3 Epoch 033; Train loss: 0.2143; Time: 00:00:02
Fold 3 Epoch 034; Train loss: 0.2111; Time: 00:00:01
Fold 3 Epoch 035; Train loss: 0.2071; Time: 00:00:01
Fold 3 Epoch 036; Train loss: 0.1968; Time: 00:00:02
Fold 3 Epoch 037; Train loss: 0.1946; Time: 00:00:01
Fold 3 Epoch 038; Train loss: 0.1858; Time: 00:00:02
Fold 3 Epoch 039; Train loss: 0.1809; Time: 00:00:02
Fold 3 Epoch 040; Train loss: 0.1772; Time: 00:00:01
Fold 3: Evaluation at Epoch 40
HR@5       NDCG@5     HR@10      NDCG@10   
0.637417   0.528798   0.754967   0.566688  
Loss: 0.2527
Fold 3 Epoch 041; Train loss: 0.1747; Time: 00:00:02
Fold 3 Epoch 042; Train loss: 0.1666; Time: 00:00:02
Fold 3 Epoch 043; Train loss: 0.1622; Time: 00:00:02
Fold 3 Epoch 044; Train loss: 0.1561; Time: 00:00:02
Fold 3 Epoch 045; Train loss: 0.1525; Time: 00:00:02
Fold 3 Epoch 046; Train loss: 0.1510; Time: 00:00:02
Fold 3 Epoch 047; Train loss: 0.1455; Time: 00:00:02
Fold 3 Epoch 048; Train loss: 0.1412; Time: 00:00:02
Fold 3 Epoch 049; Train loss: 0.1379; Time: 00:00:02
Fold 3 Epoch 050; Train loss: 0.1342; Time: 00:00:01
Fold 3: Evaluation at Epoch 50
HR@5       NDCG@5     HR@10      NDCG@10   
0.652318   0.542779   0.774834   0.581627  
Loss: 0.1895
Fold 3 Epoch 051; Train loss: 0.1283; Time: 00:00:01
Fold 3 Epoch 052; Train loss: 0.1262; Time: 00:00:02
Fold 3 Epoch 053; Train loss: 0.1228; Time: 00:00:02
Fold 3 Epoch 054; Train loss: 0.1175; Time: 00:00:02
Fold 3 Epoch 055; Train loss: 0.1124; Time: 00:00:02
Fold 3 Epoch 056; Train loss: 0.1098; Time: 00:00:02
Fold 3 Epoch 057; Train loss: 0.1069; Time: 00:00:02
Fold 3 Epoch 058; Train loss: 0.1036; Time: 00:00:01
Fold 3 Epoch 059; Train loss: 0.1012; Time: 00:00:02
Fold 3 Epoch 060; Train loss: 0.0943; Time: 00:00:01
Fold 3: Evaluation at Epoch 60
HR@5       NDCG@5     HR@10      NDCG@10   
0.662252   0.538825   0.822848   0.590546  
Loss: 0.1472
Fold 3 Epoch 061; Train loss: 0.0931; Time: 00:00:02
Fold 3 Epoch 062; Train loss: 0.0892; Time: 00:00:01
Fold 3 Epoch 063; Train loss: 0.0843; Time: 00:00:01
Fold 3 Epoch 064; Train loss: 0.0827; Time: 00:00:01
Fold 3 Epoch 065; Train loss: 0.0802; Time: 00:00:02
Fold 3 Epoch 066; Train loss: 0.0759; Time: 00:00:01
Fold 3 Epoch 067; Train loss: 0.0731; Time: 00:00:02
Fold 3 Epoch 068; Train loss: 0.0702; Time: 00:00:01
Fold 3 Epoch 069; Train loss: 0.0665; Time: 00:00:01
Fold 3 Epoch 070; Train loss: 0.0642; Time: 00:00:02
Fold 3: Evaluation at Epoch 70
HR@5       NDCG@5     HR@10      NDCG@10   
0.731788   0.556666   0.880795   0.605370  
Loss: 0.1003
Fold 3 Epoch 071; Train loss: 0.0615; Time: 00:00:01
Fold 3 Epoch 072; Train loss: 0.0579; Time: 00:00:02
Fold 3 Epoch 073; Train loss: 0.0563; Time: 00:00:02
Fold 3 Epoch 074; Train loss: 0.0529; Time: 00:00:02
Fold 3 Epoch 075; Train loss: 0.0510; Time: 00:00:02
Fold 3 Epoch 076; Train loss: 0.0492; Time: 00:00:02
Fold 3 Epoch 077; Train loss: 0.0465; Time: 00:00:02
Fold 3 Epoch 078; Train loss: 0.0436; Time: 00:00:02
Fold 3 Epoch 079; Train loss: 0.0412; Time: 00:00:02
Fold 3 Epoch 080; Train loss: 0.0387; Time: 00:00:02
Fold 3: Evaluation at Epoch 80
HR@5       NDCG@5     HR@10      NDCG@10   
0.715232   0.507514   0.903974   0.570663  
Loss: 0.0702
Fold 3 Epoch 081; Train loss: 0.0374; Time: 00:00:02
Fold 3 Epoch 082; Train loss: 0.0347; Time: 00:00:01
Fold 3 Epoch 083; Train loss: 0.0330; Time: 00:00:02
Fold 3 Epoch 084; Train loss: 0.0307; Time: 00:00:02
Fold 3 Epoch 085; Train loss: 0.0294; Time: 00:00:01
Fold 3 Epoch 086; Train loss: 0.0267; Time: 00:00:02
Fold 3 Epoch 087; Train loss: 0.0252; Time: 00:00:01
Fold 3 Epoch 088; Train loss: 0.0240; Time: 00:00:02
Fold 3 Epoch 089; Train loss: 0.0217; Time: 00:00:01
Fold 3 Epoch 090; Train loss: 0.0202; Time: 00:00:02
Fold 3: Evaluation at Epoch 90
HR@5       NDCG@5     HR@10      NDCG@10   
0.685430   0.462209   0.903974   0.535878  
Loss: 0.0419
Fold 3 Epoch 091; Train loss: 0.0189; Time: 00:00:01
Fold 3 Epoch 092; Train loss: 0.0175; Time: 00:00:01
Fold 3 Epoch 093; Train loss: 0.0161; Time: 00:00:02
Fold 3 Epoch 094; Train loss: 0.0149; Time: 00:00:02
Fold 3 Epoch 095; Train loss: 0.0136; Time: 00:00:02
Fold 3 Epoch 096; Train loss: 0.0122; Time: 00:00:01
Fold 3 Epoch 097; Train loss: 0.0111; Time: 00:00:01
Fold 3 Epoch 098; Train loss: 0.0100; Time: 00:00:02
Fold 3 Epoch 099; Train loss: 0.0092; Time: 00:00:02
Fold 3 Epoch 100; Train loss: 0.0084; Time: 00:00:02
Fold 3: Evaluation at Epoch 100
HR@5       NDCG@5     HR@10      NDCG@10   
0.678808   0.437303   0.884106   0.507814  
Loss: 0.0216
[dropout_rate candidate 0.3] Fold 3: HR@10 = [np.float64(0.7831125827814569), np.float64(0.7798013245033113), np.float64(0.7483443708609272), np.float64(0.7549668874172185), np.float64(0.7748344370860927), np.float64(0.8228476821192053), np.float64(0.8807947019867549), np.float64(0.9039735099337748), np.float64(0.9039735099337748), np.float64(0.8841059602649006)]
Tuning dropout_rate:  71%|███████▏  | 5/7 [20:56<08:23, 251.89s/it]
========== Fold 3 ==========
INFO:root:Using statics: seq_size = 10, genre_vocab_size = 18
Fold 3 Epoch 001; Train loss: 1.1105; Time: 00:00:02
Fold 3 Epoch 002; Train loss: 0.9590; Time: 00:00:02
Fold 3 Epoch 003; Train loss: 0.8524; Time: 00:00:02
Fold 3 Epoch 004; Train loss: 0.7720; Time: 00:00:02
Fold 3 Epoch 005; Train loss: 0.7196; Time: 00:00:02
Fold 3 Epoch 006; Train loss: 0.6739; Time: 00:00:01
Fold 3 Epoch 007; Train loss: 0.6308; Time: 00:00:01
Fold 3 Epoch 008; Train loss: 0.5895; Time: 00:00:02
Fold 3 Epoch 009; Train loss: 0.5547; Time: 00:00:01
Fold 3 Epoch 010; Train loss: 0.5206; Time: 00:00:01
Fold 3: Evaluation at Epoch 10
HR@5       NDCG@5     HR@10      NDCG@10   
0.706954   0.500727   0.897351   0.562269  
Loss: 0.6596
Fold 3 Epoch 011; Train loss: 0.4889; Time: 00:00:01
Fold 3 Epoch 012; Train loss: 0.4658; Time: 00:00:02
Fold 3 Epoch 013; Train loss: 0.4469; Time: 00:00:02
Fold 3 Epoch 014; Train loss: 0.4236; Time: 00:00:02
Fold 3 Epoch 015; Train loss: 0.4072; Time: 00:00:02
Fold 3 Epoch 016; Train loss: 0.3896; Time: 00:00:02
Fold 3 Epoch 017; Train loss: 0.3692; Time: 00:00:02
Fold 3 Epoch 018; Train loss: 0.3569; Time: 00:00:02
Fold 3 Epoch 019; Train loss: 0.3392; Time: 00:00:02
Fold 3 Epoch 020; Train loss: 0.3206; Time: 00:00:02
Fold 3: Evaluation at Epoch 20
HR@5       NDCG@5     HR@10      NDCG@10   
0.728477   0.504890   0.884106   0.555300  
Loss: 0.4290
Fold 3 Epoch 021; Train loss: 0.3176; Time: 00:00:02
Fold 3 Epoch 022; Train loss: 0.3025; Time: 00:00:02
Fold 3 Epoch 023; Train loss: 0.2930; Time: 00:00:03
Fold 3 Epoch 024; Train loss: 0.2881; Time: 00:00:02
Fold 3 Epoch 025; Train loss: 0.2764; Time: 00:00:02
Fold 3 Epoch 026; Train loss: 0.2686; Time: 00:00:02
Fold 3 Epoch 027; Train loss: 0.2536; Time: 00:00:02
Fold 3 Epoch 028; Train loss: 0.2557; Time: 00:00:02
Fold 3 Epoch 029; Train loss: 0.2413; Time: 00:00:02
Fold 3 Epoch 030; Train loss: 0.2346; Time: 00:00:02
Fold 3: Evaluation at Epoch 30
HR@5       NDCG@5     HR@10      NDCG@10   
0.673841   0.511816   0.836093   0.564822  
Loss: 0.3146
Fold 3 Epoch 031; Train loss: 0.2269; Time: 00:00:02
Fold 3 Epoch 032; Train loss: 0.2194; Time: 00:00:02
Fold 3 Epoch 033; Train loss: 0.2173; Time: 00:00:02
Fold 3 Epoch 034; Train loss: 0.2100; Time: 00:00:02
Fold 3 Epoch 035; Train loss: 0.2011; Time: 00:00:02
Fold 3 Epoch 036; Train loss: 0.1953; Time: 00:00:02
Fold 3 Epoch 037; Train loss: 0.1901; Time: 00:00:02
Fold 3 Epoch 038; Train loss: 0.1848; Time: 00:00:02
Fold 3 Epoch 039; Train loss: 0.1779; Time: 00:00:02
Fold 3 Epoch 040; Train loss: 0.1774; Time: 00:00:02
Fold 3: Evaluation at Epoch 40
HR@5       NDCG@5     HR@10      NDCG@10   
0.708609   0.523494   0.895695   0.584123  
Loss: 0.2376
Fold 3 Epoch 041; Train loss: 0.1726; Time: 00:00:02
Fold 3 Epoch 042; Train loss: 0.1658; Time: 00:00:01
Fold 3 Epoch 043; Train loss: 0.1631; Time: 00:00:01
Fold 3 Epoch 044; Train loss: 0.1566; Time: 00:00:02
Fold 3 Epoch 045; Train loss: 0.1517; Time: 00:00:01
Fold 3 Epoch 046; Train loss: 0.1462; Time: 00:00:02
Fold 3 Epoch 047; Train loss: 0.1445; Time: 00:00:02
Fold 3 Epoch 048; Train loss: 0.1391; Time: 00:00:01
Fold 3 Epoch 049; Train loss: 0.1351; Time: 00:00:02
Fold 3 Epoch 050; Train loss: 0.1306; Time: 00:00:02
Fold 3: Evaluation at Epoch 50
HR@5       NDCG@5     HR@10      NDCG@10   
0.688742   0.537752   0.870861   0.597567  
Loss: 0.1801
Fold 3 Epoch 051; Train loss: 0.1272; Time: 00:00:01
Fold 3 Epoch 052; Train loss: 0.1236; Time: 00:00:02
Fold 3 Epoch 053; Train loss: 0.1200; Time: 00:00:01
Fold 3 Epoch 054; Train loss: 0.1146; Time: 00:00:02
Fold 3 Epoch 055; Train loss: 0.1136; Time: 00:00:02
Fold 3 Epoch 056; Train loss: 0.1093; Time: 00:00:01
Fold 3 Epoch 057; Train loss: 0.1011; Time: 00:00:01
Fold 3 Epoch 058; Train loss: 0.1007; Time: 00:00:02
Fold 3 Epoch 059; Train loss: 0.0975; Time: 00:00:01
Fold 3 Epoch 060; Train loss: 0.0922; Time: 00:00:02
Fold 3: Evaluation at Epoch 60
HR@5       NDCG@5     HR@10      NDCG@10   
0.667219   0.539175   0.854305   0.600156  
Loss: 0.1374
Fold 3 Epoch 061; Train loss: 0.0885; Time: 00:00:02
Fold 3 Epoch 062; Train loss: 0.0849; Time: 00:00:02
Fold 3 Epoch 063; Train loss: 0.0825; Time: 00:00:02
Fold 3 Epoch 064; Train loss: 0.0784; Time: 00:00:02
Fold 3 Epoch 065; Train loss: 0.0771; Time: 00:00:02
Fold 3 Epoch 066; Train loss: 0.0724; Time: 00:00:02
Fold 3 Epoch 067; Train loss: 0.0691; Time: 00:00:02
Fold 3 Epoch 068; Train loss: 0.0658; Time: 00:00:02
Fold 3 Epoch 069; Train loss: 0.0639; Time: 00:00:02
Fold 3 Epoch 070; Train loss: 0.0610; Time: 00:00:02
Fold 3: Evaluation at Epoch 70
HR@5       NDCG@5     HR@10      NDCG@10   
0.687086   0.505726   0.887417   0.571600  
Loss: 0.0997
Fold 3 Epoch 071; Train loss: 0.0583; Time: 00:00:02
Fold 3 Epoch 072; Train loss: 0.0548; Time: 00:00:02
Fold 3 Epoch 073; Train loss: 0.0523; Time: 00:00:02
Fold 3 Epoch 074; Train loss: 0.0492; Time: 00:00:02
Fold 3 Epoch 075; Train loss: 0.0471; Time: 00:00:02
Fold 3 Epoch 076; Train loss: 0.0450; Time: 00:00:01
Fold 3 Epoch 077; Train loss: 0.0426; Time: 00:00:02
Fold 3 Epoch 078; Train loss: 0.0394; Time: 00:00:02
Fold 3 Epoch 079; Train loss: 0.0372; Time: 00:00:02
Fold 3 Epoch 080; Train loss: 0.0348; Time: 00:00:02
Fold 3: Evaluation at Epoch 80
HR@5       NDCG@5     HR@10      NDCG@10   
0.703642   0.503374   0.892384   0.564246  
Loss: 0.0665
Fold 3 Epoch 081; Train loss: 0.0335; Time: 00:00:02
Fold 3 Epoch 082; Train loss: 0.0313; Time: 00:00:02
Fold 3 Epoch 083; Train loss: 0.0289; Time: 00:00:02
Fold 3 Epoch 084; Train loss: 0.0268; Time: 00:00:02
Fold 3 Epoch 085; Train loss: 0.0253; Time: 00:00:02
Fold 3 Epoch 086; Train loss: 0.0245; Time: 00:00:02
Fold 3 Epoch 087; Train loss: 0.0225; Time: 00:00:02
Fold 3 Epoch 088; Train loss: 0.0203; Time: 00:00:02
Fold 3 Epoch 089; Train loss: 0.0193; Time: 00:00:02
Fold 3 Epoch 090; Train loss: 0.0177; Time: 00:00:03
Fold 3: Evaluation at Epoch 90
HR@5       NDCG@5     HR@10      NDCG@10   
0.663907   0.459879   0.908940   0.543361  
Loss: 0.0408
Fold 3 Epoch 091; Train loss: 0.0157; Time: 00:00:02
Fold 3 Epoch 092; Train loss: 0.0147; Time: 00:00:02
Fold 3 Epoch 093; Train loss: 0.0137; Time: 00:00:02
Fold 3 Epoch 094; Train loss: 0.0126; Time: 00:00:02
Fold 3 Epoch 095; Train loss: 0.0115; Time: 00:00:02
Fold 3 Epoch 096; Train loss: 0.0110; Time: 00:00:02
Fold 3 Epoch 097; Train loss: 0.0097; Time: 00:00:02
Fold 3 Epoch 098; Train loss: 0.0095; Time: 00:00:02
Fold 3 Epoch 099; Train loss: 0.0083; Time: 00:00:02
Fold 3 Epoch 100; Train loss: 0.0078; Time: 00:00:02
Fold 3: Evaluation at Epoch 100
HR@5       NDCG@5     HR@10      NDCG@10   
0.617550   0.419820   0.902318   0.517180  
Loss: 0.0229
[dropout_rate candidate 0.4] Fold 3: HR@10 = [np.float64(0.8973509933774835), np.float64(0.8841059602649006), np.float64(0.8360927152317881), np.float64(0.8956953642384106), np.float64(0.8708609271523179), np.float64(0.8543046357615894), np.float64(0.8874172185430463), np.float64(0.8923841059602649), np.float64(0.9089403973509934), np.float64(0.902317880794702)]
Tuning dropout_rate:  86%|████████▌ | 6/7 [25:22<04:16, 256.39s/it]
========== Fold 3 ==========
INFO:root:Using statics: seq_size = 10, genre_vocab_size = 18
Fold 3 Epoch 001; Train loss: 1.1516; Time: 00:00:02
Fold 3 Epoch 002; Train loss: 1.0090; Time: 00:00:02
Fold 3 Epoch 003; Train loss: 0.9041; Time: 00:00:02
Fold 3 Epoch 004; Train loss: 0.8305; Time: 00:00:02
Fold 3 Epoch 005; Train loss: 0.7677; Time: 00:00:02
Fold 3 Epoch 006; Train loss: 0.7242; Time: 00:00:02
Fold 3 Epoch 007; Train loss: 0.6749; Time: 00:00:02
Fold 3 Epoch 008; Train loss: 0.6304; Time: 00:00:02
Fold 3 Epoch 009; Train loss: 0.5990; Time: 00:00:02
Fold 3 Epoch 010; Train loss: 0.5633; Time: 00:00:02
Fold 3: Evaluation at Epoch 10
HR@5       NDCG@5     HR@10      NDCG@10   
0.660596   0.438750   0.857616   0.504879  
Loss: 0.6914
Fold 3 Epoch 011; Train loss: 0.5363; Time: 00:00:01
Fold 3 Epoch 012; Train loss: 0.5090; Time: 00:00:01
Fold 3 Epoch 013; Train loss: 0.4809; Time: 00:00:02
Fold 3 Epoch 014; Train loss: 0.4613; Time: 00:00:02
Fold 3 Epoch 015; Train loss: 0.4398; Time: 00:00:02
Fold 3 Epoch 016; Train loss: 0.4238; Time: 00:00:02
Fold 3 Epoch 017; Train loss: 0.4025; Time: 00:00:02
Fold 3 Epoch 018; Train loss: 0.3858; Time: 00:00:02
Fold 3 Epoch 019; Train loss: 0.3716; Time: 00:00:02
Fold 3 Epoch 020; Train loss: 0.3634; Time: 00:00:02
Fold 3: Evaluation at Epoch 20
HR@5       NDCG@5     HR@10      NDCG@10   
0.711921   0.515112   0.822848   0.551636  
Loss: 0.4658
Fold 3 Epoch 021; Train loss: 0.3497; Time: 00:00:02
Fold 3 Epoch 022; Train loss: 0.3406; Time: 00:00:02
Fold 3 Epoch 023; Train loss: 0.3242; Time: 00:00:02
Fold 3 Epoch 024; Train loss: 0.3191; Time: 00:00:02
Fold 3 Epoch 025; Train loss: 0.3025; Time: 00:00:02
Fold 3 Epoch 026; Train loss: 0.2924; Time: 00:00:02
Fold 3 Epoch 027; Train loss: 0.2838; Time: 00:00:02
Fold 3 Epoch 028; Train loss: 0.2780; Time: 00:00:02
Fold 3 Epoch 029; Train loss: 0.2739; Time: 00:00:02
Fold 3 Epoch 030; Train loss: 0.2630; Time: 00:00:02
Fold 3: Evaluation at Epoch 30
HR@5       NDCG@5     HR@10      NDCG@10   
0.660596   0.493768   0.793046   0.536625  
Loss: 0.3402
Fold 3 Epoch 031; Train loss: 0.2574; Time: 00:00:02
Fold 3 Epoch 032; Train loss: 0.2403; Time: 00:00:02
Fold 3 Epoch 033; Train loss: 0.2450; Time: 00:00:02
Fold 3 Epoch 034; Train loss: 0.2367; Time: 00:00:01
Fold 3 Epoch 035; Train loss: 0.2321; Time: 00:00:02
Fold 3 Epoch 036; Train loss: 0.2262; Time: 00:00:01
Fold 3 Epoch 037; Train loss: 0.2152; Time: 00:00:02
Fold 3 Epoch 038; Train loss: 0.2106; Time: 00:00:02
Fold 3 Epoch 039; Train loss: 0.2032; Time: 00:00:02
Fold 3 Epoch 040; Train loss: 0.2004; Time: 00:00:02
Fold 3: Evaluation at Epoch 40
HR@5       NDCG@5     HR@10      NDCG@10   
0.685430   0.499788   0.834437   0.548566  
Loss: 0.2553
Fold 3 Epoch 041; Train loss: 0.1992; Time: 00:00:02
Fold 3 Epoch 042; Train loss: 0.1899; Time: 00:00:02
Fold 3 Epoch 043; Train loss: 0.1847; Time: 00:00:02
Fold 3 Epoch 044; Train loss: 0.1808; Time: 00:00:02
Fold 3 Epoch 045; Train loss: 0.1779; Time: 00:00:02
Fold 3 Epoch 046; Train loss: 0.1742; Time: 00:00:02
Fold 3 Epoch 047; Train loss: 0.1707; Time: 00:00:02
Fold 3 Epoch 048; Train loss: 0.1621; Time: 00:00:02
Fold 3 Epoch 049; Train loss: 0.1580; Time: 00:00:02
Fold 3 Epoch 050; Train loss: 0.1533; Time: 00:00:02
Fold 3: Evaluation at Epoch 50
HR@5       NDCG@5     HR@10      NDCG@10   
0.736755   0.531472   0.890728   0.580621  
Loss: 0.2034
Fold 3 Epoch 051; Train loss: 0.1500; Time: 00:00:02
Fold 3 Epoch 052; Train loss: 0.1474; Time: 00:00:02
Fold 3 Epoch 053; Train loss: 0.1399; Time: 00:00:01
Fold 3 Epoch 054; Train loss: 0.1346; Time: 00:00:01
Fold 3 Epoch 055; Train loss: 0.1324; Time: 00:00:02
Fold 3 Epoch 056; Train loss: 0.1282; Time: 00:00:02
Fold 3 Epoch 057; Train loss: 0.1261; Time: 00:00:01
Fold 3 Epoch 058; Train loss: 0.1188; Time: 00:00:02
Fold 3 Epoch 059; Train loss: 0.1179; Time: 00:00:02
Fold 3 Epoch 060; Train loss: 0.1126; Time: 00:00:01
Fold 3: Evaluation at Epoch 60
HR@5       NDCG@5     HR@10      NDCG@10   
0.748344   0.527793   0.897351   0.576105  
Loss: 0.1526
Fold 3 Epoch 061; Train loss: 0.1102; Time: 00:00:02
Fold 3 Epoch 062; Train loss: 0.1066; Time: 00:00:02
Fold 3 Epoch 063; Train loss: 0.1015; Time: 00:00:01
Fold 3 Epoch 064; Train loss: 0.0984; Time: 00:00:01
Fold 3 Epoch 065; Train loss: 0.0953; Time: 00:00:02
Fold 3 Epoch 066; Train loss: 0.0916; Time: 00:00:02
Fold 3 Epoch 067; Train loss: 0.0885; Time: 00:00:02
Fold 3 Epoch 068; Train loss: 0.0848; Time: 00:00:02
Fold 3 Epoch 069; Train loss: 0.0828; Time: 00:00:02
Fold 3 Epoch 070; Train loss: 0.0802; Time: 00:00:02
Fold 3: Evaluation at Epoch 70
HR@5       NDCG@5     HR@10      NDCG@10   
0.764901   0.536253   0.907285   0.582890  
Loss: 0.1093
Fold 3 Epoch 071; Train loss: 0.0750; Time: 00:00:02
Fold 3 Epoch 072; Train loss: 0.0714; Time: 00:00:02
Fold 3 Epoch 073; Train loss: 0.0692; Time: 00:00:02
Fold 3 Epoch 074; Train loss: 0.0675; Time: 00:00:02
Fold 3 Epoch 075; Train loss: 0.0640; Time: 00:00:02
Fold 3 Epoch 076; Train loss: 0.0606; Time: 00:00:02
Fold 3 Epoch 077; Train loss: 0.0575; Time: 00:00:02
Fold 3 Epoch 078; Train loss: 0.0549; Time: 00:00:02
Fold 3 Epoch 079; Train loss: 0.0525; Time: 00:00:02
Fold 3 Epoch 080; Train loss: 0.0502; Time: 00:00:02
Fold 3: Evaluation at Epoch 80
HR@5       NDCG@5     HR@10      NDCG@10   
0.796358   0.516922   0.910596   0.554533  
Loss: 0.0747
Fold 3 Epoch 081; Train loss: 0.0470; Time: 00:00:02
Fold 3 Epoch 082; Train loss: 0.0444; Time: 00:00:02
Fold 3 Epoch 083; Train loss: 0.0419; Time: 00:00:01
Fold 3 Epoch 084; Train loss: 0.0401; Time: 00:00:01
Fold 3 Epoch 085; Train loss: 0.0384; Time: 00:00:02
Fold 3 Epoch 086; Train loss: 0.0356; Time: 00:00:01
Fold 3 Epoch 087; Train loss: 0.0337; Time: 00:00:02
Fold 3 Epoch 088; Train loss: 0.0313; Time: 00:00:02
Fold 3 Epoch 089; Train loss: 0.0292; Time: 00:00:02
Fold 3 Epoch 090; Train loss: 0.0277; Time: 00:00:01
Fold 3: Evaluation at Epoch 90
HR@5       NDCG@5     HR@10      NDCG@10   
0.784768   0.447730   0.877483   0.478760  
Loss: 0.0474
Fold 3 Epoch 091; Train loss: 0.0257; Time: 00:00:02
Fold 3 Epoch 092; Train loss: 0.0241; Time: 00:00:02
Fold 3 Epoch 093; Train loss: 0.0227; Time: 00:00:02
Fold 3 Epoch 094; Train loss: 0.0205; Time: 00:00:01
Fold 3 Epoch 095; Train loss: 0.0197; Time: 00:00:01
Fold 3 Epoch 096; Train loss: 0.0179; Time: 00:00:02
Fold 3 Epoch 097; Train loss: 0.0168; Time: 00:00:02
Fold 3 Epoch 098; Train loss: 0.0154; Time: 00:00:02
Fold 3 Epoch 099; Train loss: 0.0143; Time: 00:00:02
Fold 3 Epoch 100; Train loss: 0.0126; Time: 00:00:01
Fold 3: Evaluation at Epoch 100
HR@5       NDCG@5     HR@10      NDCG@10   
0.799669   0.371575   0.867550   0.394177  
Loss: 0.0266
[dropout_rate candidate 0.5] Fold 3: HR@10 = [np.float64(0.8576158940397351), np.float64(0.8228476821192053), np.float64(0.793046357615894), np.float64(0.8344370860927153), np.float64(0.890728476821192), np.float64(0.8973509933774835), np.float64(0.9072847682119205), np.float64(0.9105960264900662), np.float64(0.8774834437086093), np.float64(0.8675496688741722)]
Tuning dropout_rate: 100%|██████████| 7/7 [29:38<00:00, 256.37s/it]Tuning dropout_rate: 100%|██████████| 7/7 [29:38<00:00, 254.06s/it]

Best dropout_rate found: 0.2
Tuning l2_decay:   0%|          | 0/9 [00:00<?, ?it/s]
========== Fold 3 ==========
INFO:root:Using statics: seq_size = 10, genre_vocab_size = 18
Fold 3 Epoch 001; Train loss: 1.1977; Time: 00:00:02
Fold 3 Epoch 002; Train loss: 1.0399; Time: 00:00:02
Fold 3 Epoch 003; Train loss: 0.9255; Time: 00:00:02
Fold 3 Epoch 004; Train loss: 0.8484; Time: 00:00:02
Fold 3 Epoch 005; Train loss: 0.7958; Time: 00:00:02
Fold 3 Epoch 006; Train loss: 0.7396; Time: 00:00:02
Fold 3 Epoch 007; Train loss: 0.7019; Time: 00:00:02
Fold 3 Epoch 008; Train loss: 0.6534; Time: 00:00:02
Fold 3 Epoch 009; Train loss: 0.6259; Time: 00:00:02
Fold 3 Epoch 010; Train loss: 0.5835; Time: 00:00:02
Fold 3: Evaluation at Epoch 10
HR@5       NDCG@5     HR@10      NDCG@10   
0.698675   0.503722   0.839404   0.548141  
Loss: 0.7729
Fold 3 Epoch 011; Train loss: 0.5541; Time: 00:00:02
Fold 3 Epoch 012; Train loss: 0.5231; Time: 00:00:02
Fold 3 Epoch 013; Train loss: 0.5077; Time: 00:00:02
Fold 3 Epoch 014; Train loss: 0.4735; Time: 00:00:02
Fold 3 Epoch 015; Train loss: 0.4570; Time: 00:00:02
Fold 3 Epoch 016; Train loss: 0.4360; Time: 00:00:02
Fold 3 Epoch 017; Train loss: 0.4167; Time: 00:00:02
Fold 3 Epoch 018; Train loss: 0.3982; Time: 00:00:02
Fold 3 Epoch 019; Train loss: 0.3909; Time: 00:00:02
Fold 3 Epoch 020; Train loss: 0.3787; Time: 00:00:02
Fold 3: Evaluation at Epoch 20
HR@5       NDCG@5     HR@10      NDCG@10   
0.667219   0.543732   0.827815   0.595234  
Loss: 0.4908
Fold 3 Epoch 021; Train loss: 0.3515; Time: 00:00:02
Fold 3 Epoch 022; Train loss: 0.3450; Time: 00:00:02
Fold 3 Epoch 023; Train loss: 0.3362; Time: 00:00:02
Fold 3 Epoch 024; Train loss: 0.3243; Time: 00:00:02
Fold 3 Epoch 025; Train loss: 0.3071; Time: 00:00:02
Fold 3 Epoch 026; Train loss: 0.3034; Time: 00:00:02
Fold 3 Epoch 027; Train loss: 0.2948; Time: 00:00:02
Fold 3 Epoch 028; Train loss: 0.2856; Time: 00:00:02
Fold 3 Epoch 029; Train loss: 0.2791; Time: 00:00:02
Fold 3 Epoch 030; Train loss: 0.2715; Time: 00:00:02
Fold 3: Evaluation at Epoch 30
HR@5       NDCG@5     HR@10      NDCG@10   
0.602649   0.482903   0.789735   0.542521  
Loss: 0.3514
Fold 3 Epoch 031; Train loss: 0.2651; Time: 00:00:02
Fold 3 Epoch 032; Train loss: 0.2555; Time: 00:00:02
Fold 3 Epoch 033; Train loss: 0.2473; Time: 00:00:01
Fold 3 Epoch 034; Train loss: 0.2401; Time: 00:00:02
Fold 3 Epoch 035; Train loss: 0.2341; Time: 00:00:02
Fold 3 Epoch 036; Train loss: 0.2296; Time: 00:00:02
Fold 3 Epoch 037; Train loss: 0.2298; Time: 00:00:02
Fold 3 Epoch 038; Train loss: 0.2155; Time: 00:00:01
Fold 3 Epoch 039; Train loss: 0.2144; Time: 00:00:02
Fold 3 Epoch 040; Train loss: 0.2085; Time: 00:00:02
Fold 3: Evaluation at Epoch 40
HR@5       NDCG@5     HR@10      NDCG@10   
0.627483   0.520741   0.789735   0.572513  
Loss: 0.2751
Fold 3 Epoch 041; Train loss: 0.2023; Time: 00:00:02
Fold 3 Epoch 042; Train loss: 0.1970; Time: 00:00:02
Fold 3 Epoch 043; Train loss: 0.1946; Time: 00:00:02
Fold 3 Epoch 044; Train loss: 0.1878; Time: 00:00:01
Fold 3 Epoch 045; Train loss: 0.1800; Time: 00:00:02
Fold 3 Epoch 046; Train loss: 0.1801; Time: 00:00:02
Fold 3 Epoch 047; Train loss: 0.1692; Time: 00:00:01
Fold 3 Epoch 048; Train loss: 0.1681; Time: 00:00:02
Fold 3 Epoch 049; Train loss: 0.1656; Time: 00:00:02
Fold 3 Epoch 050; Train loss: 0.1592; Time: 00:00:02
Fold 3: Evaluation at Epoch 50
HR@5       NDCG@5     HR@10      NDCG@10   
0.660596   0.540807   0.789735   0.581943  
Loss: 0.2111
Fold 3 Epoch 051; Train loss: 0.1559; Time: 00:00:02
Fold 3 Epoch 052; Train loss: 0.1515; Time: 00:00:02
Fold 3 Epoch 053; Train loss: 0.1481; Time: 00:00:02
Fold 3 Epoch 054; Train loss: 0.1409; Time: 00:00:02
Fold 3 Epoch 055; Train loss: 0.1368; Time: 00:00:02
Fold 3 Epoch 056; Train loss: 0.1332; Time: 00:00:02
Fold 3 Epoch 057; Train loss: 0.1299; Time: 00:00:02
Fold 3 Epoch 058; Train loss: 0.1260; Time: 00:00:02
Fold 3 Epoch 059; Train loss: 0.1219; Time: 00:00:02
Fold 3 Epoch 060; Train loss: 0.1182; Time: 00:00:02
Fold 3: Evaluation at Epoch 60
HR@5       NDCG@5     HR@10      NDCG@10   
0.670530   0.554096   0.827815   0.603692  
Loss: 0.1619
Fold 3 Epoch 061; Train loss: 0.1140; Time: 00:00:02
Fold 3 Epoch 062; Train loss: 0.1119; Time: 00:00:01
Fold 3 Epoch 063; Train loss: 0.1067; Time: 00:00:02
Fold 3 Epoch 064; Train loss: 0.1038; Time: 00:00:02
Fold 3 Epoch 065; Train loss: 0.1004; Time: 00:00:01
Fold 3 Epoch 066; Train loss: 0.0963; Time: 00:00:01
Fold 3 Epoch 067; Train loss: 0.0916; Time: 00:00:02
Fold 3 Epoch 068; Train loss: 0.0899; Time: 00:00:01
Fold 3 Epoch 069; Train loss: 0.0850; Time: 00:00:02
Fold 3 Epoch 070; Train loss: 0.0820; Time: 00:00:02
Fold 3: Evaluation at Epoch 70
HR@5       NDCG@5     HR@10      NDCG@10   
0.723510   0.579699   0.877483   0.629418  
Loss: 0.1217
Fold 3 Epoch 071; Train loss: 0.0786; Time: 00:00:02
Fold 3 Epoch 072; Train loss: 0.0760; Time: 00:00:02
Fold 3 Epoch 073; Train loss: 0.0719; Time: 00:00:02
Fold 3 Epoch 074; Train loss: 0.0698; Time: 00:00:01
Fold 3 Epoch 075; Train loss: 0.0659; Time: 00:00:02
Fold 3 Epoch 076; Train loss: 0.0622; Time: 00:00:02
Fold 3 Epoch 077; Train loss: 0.0605; Time: 00:00:02
Fold 3 Epoch 078; Train loss: 0.0576; Time: 00:00:02
Fold 3 Epoch 079; Train loss: 0.0543; Time: 00:00:02
Fold 3 Epoch 080; Train loss: 0.0507; Time: 00:00:01
Fold 3: Evaluation at Epoch 80
HR@5       NDCG@5     HR@10      NDCG@10   
0.753311   0.595297   0.894040   0.641350  
Loss: 0.0793
Fold 3 Epoch 081; Train loss: 0.0491; Time: 00:00:02
Fold 3 Epoch 082; Train loss: 0.0456; Time: 00:00:01
Fold 3 Epoch 083; Train loss: 0.0440; Time: 00:00:02
Fold 3 Epoch 084; Train loss: 0.0410; Time: 00:00:02
Fold 3 Epoch 085; Train loss: 0.0384; Time: 00:00:02
Fold 3 Epoch 086; Train loss: 0.0363; Time: 00:00:02
Fold 3 Epoch 087; Train loss: 0.0336; Time: 00:00:02
Fold 3 Epoch 088; Train loss: 0.0319; Time: 00:00:02
Fold 3 Epoch 089; Train loss: 0.0300; Time: 00:00:02
Fold 3 Epoch 090; Train loss: 0.0276; Time: 00:00:02
Fold 3: Evaluation at Epoch 90
HR@5       NDCG@5     HR@10      NDCG@10   
0.802980   0.605655   0.902318   0.637925  
Loss: 0.0462
Fold 3 Epoch 091; Train loss: 0.0257; Time: 00:00:02
Fold 3 Epoch 092; Train loss: 0.0233; Time: 00:00:02
Fold 3 Epoch 093; Train loss: 0.0216; Time: 00:00:02
Fold 3 Epoch 094; Train loss: 0.0200; Time: 00:00:02
Fold 3 Epoch 095; Train loss: 0.0186; Time: 00:00:02
Fold 3 Epoch 096; Train loss: 0.0173; Time: 00:00:02
Fold 3 Epoch 097; Train loss: 0.0156; Time: 00:00:02
Fold 3 Epoch 098; Train loss: 0.0140; Time: 00:00:02
Fold 3 Epoch 099; Train loss: 0.0128; Time: 00:00:02
Fold 3 Epoch 100; Train loss: 0.0114; Time: 00:00:02
Fold 3: Evaluation at Epoch 100
HR@5       NDCG@5     HR@10      NDCG@10   
0.798013   0.594250   0.908940   0.630521  
Loss: 0.0233
[l2_decay candidate 0.1] Fold 3: HR@10 = [np.float64(0.8394039735099338), np.float64(0.8278145695364238), np.float64(0.7897350993377483), np.float64(0.7897350993377483), np.float64(0.7897350993377483), np.float64(0.8278145695364238), np.float64(0.8774834437086093), np.float64(0.8940397350993378), np.float64(0.902317880794702), np.float64(0.9089403973509934)]
Tuning l2_decay:  11%|█         | 1/9 [04:13<33:50, 253.83s/it]
========== Fold 3 ==========
INFO:root:Using statics: seq_size = 10, genre_vocab_size = 18
Fold 3 Epoch 001; Train loss: 1.1250; Time: 00:00:02
Fold 3 Epoch 002; Train loss: 0.9574; Time: 00:00:01
Fold 3 Epoch 003; Train loss: 0.8422; Time: 00:00:02
Fold 3 Epoch 004; Train loss: 0.7623; Time: 00:00:01
Fold 3 Epoch 005; Train loss: 0.7077; Time: 00:00:02
Fold 3 Epoch 006; Train loss: 0.6597; Time: 00:00:02
Fold 3 Epoch 007; Train loss: 0.6146; Time: 00:00:02
Fold 3 Epoch 008; Train loss: 0.5706; Time: 00:00:02
Fold 3 Epoch 009; Train loss: 0.5476; Time: 00:00:02
Fold 3 Epoch 010; Train loss: 0.5161; Time: 00:00:02
Fold 3: Evaluation at Epoch 10
HR@5       NDCG@5     HR@10      NDCG@10   
0.640728   0.481653   0.788079   0.529125  
Loss: 0.7260
Fold 3 Epoch 011; Train loss: 0.4864; Time: 00:00:02
Fold 3 Epoch 012; Train loss: 0.4612; Time: 00:00:02
Fold 3 Epoch 013; Train loss: 0.4359; Time: 00:00:02
Fold 3 Epoch 014; Train loss: 0.4178; Time: 00:00:02
Fold 3 Epoch 015; Train loss: 0.3976; Time: 00:00:02
Fold 3 Epoch 016; Train loss: 0.3796; Time: 00:00:01
Fold 3 Epoch 017; Train loss: 0.3611; Time: 00:00:02
Fold 3 Epoch 018; Train loss: 0.3455; Time: 00:00:02
Fold 3 Epoch 019; Train loss: 0.3296; Time: 00:00:02
Fold 3 Epoch 020; Train loss: 0.3207; Time: 00:00:01
Fold 3: Evaluation at Epoch 20
HR@5       NDCG@5     HR@10      NDCG@10   
0.639073   0.522555   0.804636   0.575220  
Loss: 0.4593
Fold 3 Epoch 021; Train loss: 0.3087; Time: 00:00:02
Fold 3 Epoch 022; Train loss: 0.2940; Time: 00:00:02
Fold 3 Epoch 023; Train loss: 0.2838; Time: 00:00:01
Fold 3 Epoch 024; Train loss: 0.2731; Time: 00:00:02
Fold 3 Epoch 025; Train loss: 0.2604; Time: 00:00:02
Fold 3 Epoch 026; Train loss: 0.2555; Time: 00:00:02
Fold 3 Epoch 027; Train loss: 0.2491; Time: 00:00:02
Fold 3 Epoch 028; Train loss: 0.2438; Time: 00:00:02
Fold 3 Epoch 029; Train loss: 0.2308; Time: 00:00:02
Fold 3 Epoch 030; Train loss: 0.2190; Time: 00:00:02
Fold 3: Evaluation at Epoch 30
HR@5       NDCG@5     HR@10      NDCG@10   
0.639073   0.506499   0.776490   0.551221  
Loss: 0.3172
Fold 3 Epoch 031; Train loss: 0.2146; Time: 00:00:01
Fold 3 Epoch 032; Train loss: 0.2067; Time: 00:00:01
Fold 3 Epoch 033; Train loss: 0.2003; Time: 00:00:02
Fold 3 Epoch 034; Train loss: 0.1998; Time: 00:00:01
Fold 3 Epoch 035; Train loss: 0.1962; Time: 00:00:01
Fold 3 Epoch 036; Train loss: 0.1861; Time: 00:00:02
Fold 3 Epoch 037; Train loss: 0.1816; Time: 00:00:02
Fold 3 Epoch 038; Train loss: 0.1754; Time: 00:00:01
Fold 3 Epoch 039; Train loss: 0.1730; Time: 00:00:02
Fold 3 Epoch 040; Train loss: 0.1675; Time: 00:00:02
Fold 3: Evaluation at Epoch 40
HR@5       NDCG@5     HR@10      NDCG@10   
0.635762   0.517511   0.781457   0.564539  
Loss: 0.2548
Fold 3 Epoch 041; Train loss: 0.1591; Time: 00:00:02
Fold 3 Epoch 042; Train loss: 0.1579; Time: 00:00:02
Fold 3 Epoch 043; Train loss: 0.1587; Time: 00:00:02
Fold 3 Epoch 044; Train loss: 0.1491; Time: 00:00:02
Fold 3 Epoch 045; Train loss: 0.1415; Time: 00:00:02
Fold 3 Epoch 046; Train loss: 0.1402; Time: 00:00:02
Fold 3 Epoch 047; Train loss: 0.1337; Time: 00:00:02
Fold 3 Epoch 048; Train loss: 0.1301; Time: 00:00:01
Fold 3 Epoch 049; Train loss: 0.1304; Time: 00:00:01
Fold 3 Epoch 050; Train loss: 0.1250; Time: 00:00:02
Fold 3: Evaluation at Epoch 50
HR@5       NDCG@5     HR@10      NDCG@10   
0.657285   0.539184   0.802980   0.585348  
Loss: 0.1931
Fold 3 Epoch 051; Train loss: 0.1182; Time: 00:00:02
Fold 3 Epoch 052; Train loss: 0.1152; Time: 00:00:02
Fold 3 Epoch 053; Train loss: 0.1117; Time: 00:00:01
Fold 3 Epoch 054; Train loss: 0.1093; Time: 00:00:01
Fold 3 Epoch 055; Train loss: 0.1037; Time: 00:00:01
Fold 3 Epoch 056; Train loss: 0.1013; Time: 00:00:02
Fold 3 Epoch 057; Train loss: 0.0966; Time: 00:00:02
Fold 3 Epoch 058; Train loss: 0.0927; Time: 00:00:02
Fold 3 Epoch 059; Train loss: 0.0879; Time: 00:00:02
Fold 3 Epoch 060; Train loss: 0.0852; Time: 00:00:02
Fold 3: Evaluation at Epoch 60
HR@5       NDCG@5     HR@10      NDCG@10   
0.683775   0.555122   0.827815   0.601371  
Loss: 0.1460
Fold 3 Epoch 061; Train loss: 0.0833; Time: 00:00:02
Fold 3 Epoch 062; Train loss: 0.0803; Time: 00:00:02
Fold 3 Epoch 063; Train loss: 0.0745; Time: 00:00:02
Fold 3 Epoch 064; Train loss: 0.0731; Time: 00:00:02
Fold 3 Epoch 065; Train loss: 0.0701; Time: 00:00:01
Fold 3 Epoch 066; Train loss: 0.0671; Time: 00:00:01
Fold 3 Epoch 067; Train loss: 0.0645; Time: 00:00:01
Fold 3 Epoch 068; Train loss: 0.0616; Time: 00:00:02
Fold 3 Epoch 069; Train loss: 0.0583; Time: 00:00:01
Fold 3 Epoch 070; Train loss: 0.0553; Time: 00:00:01
Fold 3: Evaluation at Epoch 70
HR@5       NDCG@5     HR@10      NDCG@10   
0.683775   0.552752   0.846026   0.604831  
Loss: 0.1033
Fold 3 Epoch 071; Train loss: 0.0527; Time: 00:00:02
Fold 3 Epoch 072; Train loss: 0.0502; Time: 00:00:02
Fold 3 Epoch 073; Train loss: 0.0479; Time: 00:00:01
Fold 3 Epoch 074; Train loss: 0.0458; Time: 00:00:02
Fold 3 Epoch 075; Train loss: 0.0424; Time: 00:00:02
Fold 3 Epoch 076; Train loss: 0.0400; Time: 00:00:01
Fold 3 Epoch 077; Train loss: 0.0383; Time: 00:00:01
Fold 3 Epoch 078; Train loss: 0.0360; Time: 00:00:01
Fold 3 Epoch 079; Train loss: 0.0340; Time: 00:00:02
Fold 3 Epoch 080; Train loss: 0.0319; Time: 00:00:02
Fold 3: Evaluation at Epoch 80
HR@5       NDCG@5     HR@10      NDCG@10   
0.688742   0.547273   0.882450   0.608317  
Loss: 0.0685
Fold 3 Epoch 081; Train loss: 0.0301; Time: 00:00:02
Fold 3 Epoch 082; Train loss: 0.0280; Time: 00:00:02
Fold 3 Epoch 083; Train loss: 0.0265; Time: 00:00:02
Fold 3 Epoch 084; Train loss: 0.0241; Time: 00:00:02
Fold 3 Epoch 085; Train loss: 0.0231; Time: 00:00:02
Fold 3 Epoch 086; Train loss: 0.0210; Time: 00:00:02
Fold 3 Epoch 087; Train loss: 0.0198; Time: 00:00:01
Fold 3 Epoch 088; Train loss: 0.0185; Time: 00:00:02
Fold 3 Epoch 089; Train loss: 0.0168; Time: 00:00:02
Fold 3 Epoch 090; Train loss: 0.0157; Time: 00:00:01
Fold 3: Evaluation at Epoch 90
HR@5       NDCG@5     HR@10      NDCG@10   
0.670530   0.531503   0.875828   0.595638  
Loss: 0.0409
Fold 3 Epoch 091; Train loss: 0.0150; Time: 00:00:01
Fold 3 Epoch 092; Train loss: 0.0133; Time: 00:00:02
Fold 3 Epoch 093; Train loss: 0.0125; Time: 00:00:01
Fold 3 Epoch 094; Train loss: 0.0113; Time: 00:00:02
Fold 3 Epoch 095; Train loss: 0.0107; Time: 00:00:02
Fold 3 Epoch 096; Train loss: 0.0098; Time: 00:00:02
Fold 3 Epoch 097; Train loss: 0.0087; Time: 00:00:02
Fold 3 Epoch 098; Train loss: 0.0079; Time: 00:00:02
Fold 3 Epoch 099; Train loss: 0.0071; Time: 00:00:01
Fold 3 Epoch 100; Train loss: 0.0064; Time: 00:00:01
Fold 3: Evaluation at Epoch 100
HR@5       NDCG@5     HR@10      NDCG@10   
0.579470   0.374307   0.860927   0.463247  
Loss: 0.0216
[l2_decay candidate 0.01] Fold 3: HR@10 = [np.float64(0.7880794701986755), np.float64(0.804635761589404), np.float64(0.7764900662251656), np.float64(0.7814569536423841), np.float64(0.8029801324503312), np.float64(0.8278145695364238), np.float64(0.8460264900662252), np.float64(0.8824503311258278), np.float64(0.8758278145695364), np.float64(0.8609271523178808)]
Tuning l2_decay:  22%|██▏       | 2/9 [08:18<29:00, 248.57s/it]
========== Fold 3 ==========
INFO:root:Using statics: seq_size = 10, genre_vocab_size = 18
Fold 3 Epoch 001; Train loss: 1.2036; Time: 00:00:02
Fold 3 Epoch 002; Train loss: 1.0455; Time: 00:00:02
Fold 3 Epoch 003; Train loss: 0.9310; Time: 00:00:02
Fold 3 Epoch 004; Train loss: 0.8457; Time: 00:00:01
Fold 3 Epoch 005; Train loss: 0.7943; Time: 00:00:02
Fold 3 Epoch 006; Train loss: 0.7412; Time: 00:00:02
Fold 3 Epoch 007; Train loss: 0.6935; Time: 00:00:02
Fold 3 Epoch 008; Train loss: 0.6514; Time: 00:00:02
Fold 3 Epoch 009; Train loss: 0.6105; Time: 00:00:02
Fold 3 Epoch 010; Train loss: 0.5808; Time: 00:00:02
Fold 3: Evaluation at Epoch 10
HR@5       NDCG@5     HR@10      NDCG@10   
0.655629   0.450569   0.788079   0.493614  
Loss: 0.6874
Fold 3 Epoch 011; Train loss: 0.5487; Time: 00:00:01
Fold 3 Epoch 012; Train loss: 0.5141; Time: 00:00:01
Fold 3 Epoch 013; Train loss: 0.4921; Time: 00:00:02
Fold 3 Epoch 014; Train loss: 0.4732; Time: 00:00:01
Fold 3 Epoch 015; Train loss: 0.4466; Time: 00:00:02
Fold 3 Epoch 016; Train loss: 0.4284; Time: 00:00:02
Fold 3 Epoch 017; Train loss: 0.4111; Time: 00:00:02
Fold 3 Epoch 018; Train loss: 0.3894; Time: 00:00:02
Fold 3 Epoch 019; Train loss: 0.3814; Time: 00:00:01
Fold 3 Epoch 020; Train loss: 0.3658; Time: 00:00:02
Fold 3: Evaluation at Epoch 20
HR@5       NDCG@5     HR@10      NDCG@10   
0.632450   0.487502   0.798013   0.540304  
Loss: 0.4483
Fold 3 Epoch 021; Train loss: 0.3523; Time: 00:00:01
Fold 3 Epoch 022; Train loss: 0.3417; Time: 00:00:01
Fold 3 Epoch 023; Train loss: 0.3254; Time: 00:00:01
Fold 3 Epoch 024; Train loss: 0.3131; Time: 00:00:02
Fold 3 Epoch 025; Train loss: 0.3045; Time: 00:00:02
Fold 3 Epoch 026; Train loss: 0.2978; Time: 00:00:02
Fold 3 Epoch 027; Train loss: 0.2861; Time: 00:00:02
Fold 3 Epoch 028; Train loss: 0.2736; Time: 00:00:02
Fold 3 Epoch 029; Train loss: 0.2649; Time: 00:00:02
Fold 3 Epoch 030; Train loss: 0.2623; Time: 00:00:02
Fold 3: Evaluation at Epoch 30
HR@5       NDCG@5     HR@10      NDCG@10   
0.610927   0.491128   0.748344   0.535868  
Loss: 0.3270
Fold 3 Epoch 031; Train loss: 0.2562; Time: 00:00:02
Fold 3 Epoch 032; Train loss: 0.2475; Time: 00:00:02
Fold 3 Epoch 033; Train loss: 0.2374; Time: 00:00:02
Fold 3 Epoch 034; Train loss: 0.2305; Time: 00:00:02
Fold 3 Epoch 035; Train loss: 0.2301; Time: 00:00:02
Fold 3 Epoch 036; Train loss: 0.2253; Time: 00:00:02
Fold 3 Epoch 037; Train loss: 0.2179; Time: 00:00:02
Fold 3 Epoch 038; Train loss: 0.2106; Time: 00:00:02
Fold 3 Epoch 039; Train loss: 0.2045; Time: 00:00:02
Fold 3 Epoch 040; Train loss: 0.2002; Time: 00:00:02
Fold 3: Evaluation at Epoch 40
HR@5       NDCG@5     HR@10      NDCG@10   
0.610927   0.475278   0.721854   0.510876  
Loss: 0.2595
Fold 3 Epoch 041; Train loss: 0.1960; Time: 00:00:02
Fold 3 Epoch 042; Train loss: 0.1914; Time: 00:00:01
Fold 3 Epoch 043; Train loss: 0.1878; Time: 00:00:01
Fold 3 Epoch 044; Train loss: 0.1798; Time: 00:00:01
Fold 3 Epoch 045; Train loss: 0.1784; Time: 00:00:01
Fold 3 Epoch 046; Train loss: 0.1731; Time: 00:00:01
Fold 3 Epoch 047; Train loss: 0.1707; Time: 00:00:02
Fold 3 Epoch 048; Train loss: 0.1613; Time: 00:00:02
Fold 3 Epoch 049; Train loss: 0.1573; Time: 00:00:02
Fold 3 Epoch 050; Train loss: 0.1529; Time: 00:00:01
Fold 3: Evaluation at Epoch 50
HR@5       NDCG@5     HR@10      NDCG@10   
0.582781   0.451150   0.713576   0.492545  
Loss: 0.2025
Fold 3 Epoch 051; Train loss: 0.1489; Time: 00:00:02
Fold 3 Epoch 052; Train loss: 0.1480; Time: 00:00:02
Fold 3 Epoch 053; Train loss: 0.1415; Time: 00:00:02
Fold 3 Epoch 054; Train loss: 0.1361; Time: 00:00:02
Fold 3 Epoch 055; Train loss: 0.1316; Time: 00:00:02
Fold 3 Epoch 056; Train loss: 0.1319; Time: 00:00:02
Fold 3 Epoch 057; Train loss: 0.1280; Time: 00:00:02
Fold 3 Epoch 058; Train loss: 0.1228; Time: 00:00:02
Fold 3 Epoch 059; Train loss: 0.1181; Time: 00:00:02
Fold 3 Epoch 060; Train loss: 0.1137; Time: 00:00:02
Fold 3: Evaluation at Epoch 60
HR@5       NDCG@5     HR@10      NDCG@10   
0.625828   0.483433   0.771523   0.530707  
Loss: 0.1545
Fold 3 Epoch 061; Train loss: 0.1108; Time: 00:00:02
Fold 3 Epoch 062; Train loss: 0.1067; Time: 00:00:02
Fold 3 Epoch 063; Train loss: 0.1029; Time: 00:00:02
Fold 3 Epoch 064; Train loss: 0.0991; Time: 00:00:02
Fold 3 Epoch 065; Train loss: 0.0959; Time: 00:00:02
Fold 3 Epoch 066; Train loss: 0.0938; Time: 00:00:02
Fold 3 Epoch 067; Train loss: 0.0892; Time: 00:00:02
Tuning lr:   0%|          | 0/5 [00:00<?, ?it/s]
========== Fold 3 ==========
INFO:root:Using statics: seq_size = 10, genre_vocab_size = 18
Fold 3 Epoch 001; Train loss: 672.5962; Time: 00:00:04
Fold 3 Epoch 002; Train loss: 0.6914; Time: 00:00:02
Fold 3 Epoch 003; Train loss: 0.4139; Time: 00:00:02
Fold 3 Epoch 004; Train loss: 0.4185; Time: 00:00:02
Fold 3 Epoch 005; Train loss: 0.2296; Time: 00:00:02
Fold 3 Epoch 006; Train loss: 0.1601; Time: 00:00:02
Fold 3 Epoch 007; Train loss: 0.1408; Time: 00:00:02
Fold 3 Epoch 008; Train loss: 0.1070; Time: 00:00:02
Fold 3 Epoch 009; Train loss: 0.1194; Time: 00:00:02
Fold 3 Epoch 010; Train loss: 0.0801; Time: 00:00:02
Fold 3: Evaluation at Epoch 10
HR@5       NDCG@5     HR@10      NDCG@10   
0.317881   0.193622   0.445364   0.234472  
Loss: 0.0731
Fold 3 Epoch 011; Train loss: 0.0910; Time: 00:00:01
Fold 3 Epoch 012; Train loss: 0.0660; Time: 00:00:02
Fold 3 Epoch 013; Train loss: 0.0643; Time: 00:00:02
Fold 3 Epoch 014; Train loss: 0.0531; Time: 00:00:02
Fold 3 Epoch 015; Train loss: 0.0615; Time: 00:00:02
Fold 3 Epoch 016; Train loss: 0.0541; Time: 00:00:02
Fold 3 Epoch 017; Train loss: 0.0432; Time: 00:00:02
Fold 3 Epoch 018; Train loss: 0.0523; Time: 00:00:01
Tuning lr:   0%|          | 0/5 [00:00<?, ?it/s]
========== Fold 3 ==========
INFO:root:Using statics: seq_size = 10, genre_vocab_size = 18
Fold 3 Epoch 001; Train loss: 306.4327; Time: 00:00:03
Fold 3 Epoch 002; Train loss: 0.3603; Time: 00:00:02
Fold 3 Epoch 003; Train loss: 0.4110; Time: 00:00:02
Fold 3 Epoch 004; Train loss: 0.1674; Time: 00:00:02
Fold 3 Epoch 005; Train loss: 0.1548; Time: 00:00:02
Fold 3 Epoch 006; Train loss: 0.1364; Time: 00:00:02
Fold 3 Epoch 007; Train loss: 0.1619; Time: 00:00:02
Fold 3 Epoch 008; Train loss: 0.1255; Time: 00:00:02
Fold 3 Epoch 009; Train loss: 0.1129; Time: 00:00:02
Fold 3 Epoch 010; Train loss: 0.1044; Time: 00:00:02
Fold 3: Evaluation at Epoch 10
HR@5       NDCG@5     HR@10      NDCG@10   
0.293046   0.185829   0.418874   0.226674  
Loss: 0.1159
Fold 3 Epoch 011; Train loss: 0.1031; Time: 00:00:02
Fold 3 Epoch 012; Train loss: 0.1012; Time: 00:00:03
Fold 3 Epoch 013; Train loss: 0.0895; Time: 00:00:02
Fold 3 Epoch 014; Train loss: 0.0862; Time: 00:00:02
Fold 3 Epoch 015; Train loss: 0.0839; Time: 00:00:02
Fold 3 Epoch 016; Train loss: 0.0756; Time: 00:00:02
Fold 3 Epoch 017; Train loss: 0.0698; Time: 00:00:02
Fold 3 Epoch 018; Train loss: 0.0657; Time: 00:00:02
Fold 3 Epoch 019; Train loss: 0.0675; Time: 00:00:03
Fold 3 Epoch 020; Train loss: 0.0591; Time: 00:00:02
Fold 3: Evaluation at Epoch 20
HR@5       NDCG@5     HR@10      NDCG@10   
0.311258   0.245101   0.422185   0.280605  
Loss: 0.0684
Fold 3 Epoch 021; Train loss: 0.0563; Time: 00:00:02
Fold 3 Epoch 022; Train loss: 0.0512; Time: 00:00:02
Fold 3 Epoch 023; Train loss: 0.0485; Time: 00:00:02
Fold 3 Epoch 024; Train loss: 0.0463; Time: 00:00:02
Fold 3 Epoch 025; Train loss: 0.0428; Time: 00:00:02
Fold 3 Epoch 026; Train loss: 0.0400; Time: 00:00:02
Fold 3 Epoch 027; Train loss: 0.0375; Time: 00:00:02
Fold 3 Epoch 028; Train loss: 0.0352; Time: 00:00:02
Fold 3 Epoch 029; Train loss: 0.0341; Time: 00:00:02
Fold 3 Epoch 030; Train loss: 0.0309; Time: 00:00:02
Fold 3: Evaluation at Epoch 30
HR@5       NDCG@5     HR@10      NDCG@10   
0.291391   0.155568   0.539735   0.233247  
Loss: 0.0290
Fold 3 Epoch 031; Train loss: 0.0295; Time: 00:00:02
Fold 3 Epoch 032; Train loss: 0.0285; Time: 00:00:02
Fold 3 Epoch 033; Train loss: 0.0260; Time: 00:00:02
Fold 3 Epoch 034; Train loss: 0.0224; Time: 00:00:02
Fold 3 Epoch 035; Train loss: 0.0205; Time: 00:00:02
Fold 3 Epoch 036; Train loss: 0.0191; Time: 00:00:02
Fold 3 Epoch 037; Train loss: 0.0175; Time: 00:00:02
Fold 3 Epoch 038; Train loss: 0.0142; Time: 00:00:02
Fold 3 Epoch 039; Train loss: 0.0125; Time: 00:00:02
Fold 3 Epoch 040; Train loss: 0.0102; Time: 00:00:02
Fold 3: Evaluation at Epoch 40
HR@5       NDCG@5     HR@10      NDCG@10   
0.562914   0.309310   0.887417   0.409820  
Loss: 0.0066
Fold 3 Epoch 041; Train loss: 0.0085; Time: 00:00:02
Fold 3 Epoch 042; Train loss: 0.0081; Time: 00:00:02
Fold 3 Epoch 043; Train loss: 0.0065; Time: 00:00:02
Fold 3 Epoch 044; Train loss: 0.0060; Time: 00:00:02
Fold 3 Epoch 045; Train loss: 0.0041; Time: 00:00:02
Fold 3 Epoch 046; Train loss: 0.0037; Time: 00:00:02
Fold 3 Epoch 047; Train loss: 0.0033; Time: 00:00:02
Fold 3 Epoch 048; Train loss: 0.0021; Time: 00:00:02
Fold 3 Epoch 049; Train loss: 0.0023; Time: 00:00:02
Fold 3 Epoch 050; Train loss: 0.0017; Time: 00:00:02
Fold 3: Evaluation at Epoch 50
HR@5       NDCG@5     HR@10      NDCG@10   
0.562914   0.309310   0.895695   0.424023  
Loss: 0.0018
Fold 3 Epoch 051; Train loss: 0.0018; Time: 00:00:02
Fold 3 Epoch 052; Train loss: 0.0012; Time: 00:00:02
Fold 3 Epoch 053; Train loss: 0.0013; Time: 00:00:02
Fold 3 Epoch 054; Train loss: 0.0012; Time: 00:00:02
Fold 3 Epoch 055; Train loss: 0.0010; Time: 00:00:02
Fold 3 Epoch 056; Train loss: 0.0011; Time: 00:00:02
Fold 3 Epoch 057; Train loss: 0.0009; Time: 00:00:02
Fold 3 Epoch 058; Train loss: 0.0009; Time: 00:00:02
Fold 3 Epoch 059; Train loss: 0.0009; Time: 00:00:02
Fold 3 Epoch 060; Train loss: 0.0007; Time: 00:00:02
Fold 3: Evaluation at Epoch 60
HR@5       NDCG@5     HR@10      NDCG@10   
0.564570   0.301098   0.885762   0.412656  
Loss: 0.0010
Fold 3 Epoch 061; Train loss: 0.0007; Time: 00:00:01
Fold 3 Epoch 062; Train loss: 0.0007; Time: 00:00:02
Fold 3 Epoch 063; Train loss: 0.0007; Time: 00:00:02
Fold 3 Epoch 064; Train loss: 0.0007; Time: 00:00:02
Fold 3 Epoch 065; Train loss: 0.0006; Time: 00:00:02
Fold 3 Epoch 066; Train loss: 0.0007; Time: 00:00:02
Fold 3 Epoch 067; Train loss: 0.0007; Time: 00:00:01
Fold 3 Epoch 068; Train loss: 0.0005; Time: 00:00:01
Fold 3 Epoch 069; Train loss: 0.0006; Time: 00:00:02
Fold 3 Epoch 070; Train loss: 0.0007; Time: 00:00:02
Fold 3: Evaluation at Epoch 70
HR@5       NDCG@5     HR@10      NDCG@10   
0.564570   0.301098   0.862583   0.405955  
Loss: 0.0006
Fold 3 Epoch 071; Train loss: 0.0007; Time: 00:00:02
Fold 3 Epoch 072; Train loss: 0.0005; Time: 00:00:02
Fold 3 Epoch 073; Train loss: 0.0007; Time: 00:00:02
Fold 3 Epoch 074; Train loss: 0.0006; Time: 00:00:02
Fold 3 Epoch 075; Train loss: 0.0007; Time: 00:00:02
Fold 3 Epoch 076; Train loss: 0.0006; Time: 00:00:02
Fold 3 Epoch 077; Train loss: 0.0007; Time: 00:00:02
Fold 3 Epoch 078; Train loss: 0.0007; Time: 00:00:02
Fold 3 Epoch 079; Train loss: 0.0008; Time: 00:00:02
Fold 3 Epoch 080; Train loss: 0.0009; Time: 00:00:02
Fold 3: Evaluation at Epoch 80
HR@5       NDCG@5     HR@10      NDCG@10   
0.564570   0.269883   0.862583   0.374740  
Loss: 0.0015
Fold 3 Epoch 081; Train loss: 0.0008; Time: 00:00:02
Fold 3 Epoch 082; Train loss: 0.0009; Time: 00:00:02
Fold 3 Epoch 083; Train loss: 0.0008; Time: 00:00:02
Fold 3 Epoch 084; Train loss: 0.0009; Time: 00:00:02
Fold 3 Epoch 085; Train loss: 0.0009; Time: 00:00:02
Fold 3 Epoch 086; Train loss: 0.0011; Time: 00:00:02
Fold 3 Epoch 087; Train loss: 0.0012; Time: 00:00:02
Fold 3 Epoch 088; Train loss: 0.0012; Time: 00:00:02
Fold 3 Epoch 089; Train loss: 0.0011; Time: 00:00:02
Fold 3 Epoch 090; Train loss: 0.0016; Time: 00:00:02
Fold 3: Evaluation at Epoch 90
HR@5       NDCG@5     HR@10      NDCG@10   
0.617550   0.299824   0.889073   0.381952  
Loss: 0.0015
Fold 3 Epoch 091; Train loss: 0.0016; Time: 00:00:02
Fold 3 Epoch 092; Train loss: 0.0013; Time: 00:00:02
Fold 3 Epoch 093; Train loss: 0.0017; Time: 00:00:02
Fold 3 Epoch 094; Train loss: 0.0023; Time: 00:00:02
Fold 3 Epoch 095; Train loss: 0.0023; Time: 00:00:02
Fold 3 Epoch 096; Train loss: 0.0017; Time: 00:00:02
Fold 3 Epoch 097; Train loss: 0.0021; Time: 00:00:02
Fold 3 Epoch 098; Train loss: 0.0033; Time: 00:00:02
Fold 3 Epoch 099; Train loss: 0.0037; Time: 00:00:02
Fold 3 Epoch 100; Train loss: 0.0512; Time: 00:00:02
Fold 3: Evaluation at Epoch 100
HR@5       NDCG@5     HR@10      NDCG@10   
0.256623   0.113557   0.610927   0.231429  
Loss: 0.0007
[lr candidate 0.1] Fold 3: HR@10 = [np.float64(0.41887417218543044), np.float64(0.42218543046357615), np.float64(0.5397350993377483), np.float64(0.8874172185430463), np.float64(0.8956953642384106), np.float64(0.8857615894039735), np.float64(0.8625827814569537), np.float64(0.8625827814569537), np.float64(0.8890728476821192), np.float64(0.6109271523178808)]
Tuning lr:  20%|██        | 1/5 [04:28<17:54, 268.66s/it]
========== Fold 3 ==========
INFO:root:Using statics: seq_size = 10, genre_vocab_size = 18
Fold 3 Epoch 001; Train loss: 6.7620; Time: 00:00:02
Fold 3 Epoch 002; Train loss: 0.1700; Time: 00:00:02
Fold 3 Epoch 003; Train loss: 0.1011; Time: 00:00:02
Fold 3 Epoch 004; Train loss: 0.0860; Time: 00:00:02
Fold 3 Epoch 005; Train loss: 0.0621; Time: 00:00:02
Fold 3 Epoch 006; Train loss: 0.0421; Time: 00:00:02
Fold 3 Epoch 007; Train loss: 0.0326; Time: 00:00:02
Fold 3 Epoch 008; Train loss: 0.0359; Time: 00:00:01
Fold 3 Epoch 009; Train loss: 0.0208; Time: 00:00:02
Fold 3 Epoch 010; Train loss: 0.0149; Time: 00:00:02
Fold 3: Evaluation at Epoch 10
HR@5       NDCG@5     HR@10      NDCG@10   
0.403974   0.198571   0.855960   0.343995  
Loss: 0.0093
Fold 3 Epoch 011; Train loss: 0.0124; Time: 00:00:01
Fold 3 Epoch 012; Train loss: 0.0026; Time: 00:00:02
Fold 3 Epoch 013; Train loss: 0.0005; Time: 00:00:02
Fold 3 Epoch 014; Train loss: 0.0028; Time: 00:00:02
Fold 3 Epoch 015; Train loss: 0.0004; Time: 00:00:02
Fold 3 Epoch 016; Train loss: 0.0019; Time: 00:00:02
Fold 3 Epoch 017; Train loss: 0.0007; Time: 00:00:02
Fold 3 Epoch 018; Train loss: 0.0013; Time: 00:00:02
Fold 3 Epoch 019; Train loss: 0.0004; Time: 00:00:02
Fold 3 Epoch 020; Train loss: 0.0012; Time: 00:00:02
Fold 3: Evaluation at Epoch 20
HR@5       NDCG@5     HR@10      NDCG@10   
0.562914   0.278947   0.895695   0.393928  
Loss: 0.0001
Fold 3 Epoch 021; Train loss: 0.0001; Time: 00:00:02
Fold 3 Epoch 022; Train loss: 0.0018; Time: 00:00:02
Fold 3 Epoch 023; Train loss: 0.0001; Time: 00:00:02
Fold 3 Epoch 024; Train loss: 0.0006; Time: 00:00:02
Fold 3 Epoch 025; Train loss: 0.0002; Time: 00:00:02
Fold 3 Epoch 026; Train loss: 0.0008; Time: 00:00:02
Fold 3 Epoch 027; Train loss: 0.0000; Time: 00:00:02
Fold 3 Epoch 028; Train loss: 0.0010; Time: 00:00:02
Fold 3 Epoch 029; Train loss: 0.0000; Time: 00:00:02
Fold 3 Epoch 030; Train loss: 0.0006; Time: 00:00:02
Fold 3: Evaluation at Epoch 30
HR@5       NDCG@5     HR@10      NDCG@10   
0.278146   0.121034   0.889073   0.307963  
Loss: 0.0060
Fold 3 Epoch 031; Train loss: 0.0005; Time: 00:00:02
Fold 3 Epoch 032; Train loss: 0.0000; Time: 00:00:02
Fold 3 Epoch 033; Train loss: 0.0008; Time: 00:00:02
Fold 3 Epoch 034; Train loss: 0.0000; Time: 00:00:02
Fold 3 Epoch 035; Train loss: 0.0006; Time: 00:00:02
Fold 3 Epoch 036; Train loss: 0.0002; Time: 00:00:02
Fold 3 Epoch 037; Train loss: 0.0000; Time: 00:00:02
Fold 3 Epoch 038; Train loss: 0.0011; Time: 00:00:02
Fold 3 Epoch 039; Train loss: 0.0000; Time: 00:00:02
Fold 3 Epoch 040; Train loss: 0.0000; Time: 00:00:02
Fold 3: Evaluation at Epoch 40
HR@5       NDCG@5     HR@10      NDCG@10   
0.562914   0.278947   0.885762   0.380520  
Loss: 0.0000
Fold 3 Epoch 041; Train loss: 0.0006; Time: 00:00:02
Fold 3 Epoch 042; Train loss: 0.0001; Time: 00:00:02
Fold 3 Epoch 043; Train loss: 0.0000; Time: 00:00:02
Fold 3 Epoch 044; Train loss: 0.0002; Time: 00:00:02
Fold 3 Epoch 045; Train loss: 0.0009; Time: 00:00:02
Fold 3 Epoch 046; Train loss: 0.0000; Time: 00:00:02
Fold 3 Epoch 047; Train loss: 0.0000; Time: 00:00:02
Fold 3 Epoch 048; Train loss: 0.0000; Time: 00:00:02
Fold 3 Epoch 049; Train loss: 0.0000; Time: 00:00:02
Fold 3 Epoch 050; Train loss: 0.0000; Time: 00:00:02
Fold 3: Evaluation at Epoch 50
HR@5       NDCG@5     HR@10      NDCG@10   
0.562914   0.269170   0.895695   0.384003  
Loss: 0.0000
Fold 3 Epoch 051; Train loss: 0.0000; Time: 00:00:02
Fold 3 Epoch 052; Train loss: 0.0000; Time: 00:00:02
Fold 3 Epoch 053; Train loss: 0.0000; Time: 00:00:02
Fold 3 Epoch 054; Train loss: 0.0000; Time: 00:00:02
Fold 3 Epoch 055; Train loss: 0.0000; Time: 00:00:02
Fold 3 Epoch 056; Train loss: 0.0000; Time: 00:00:02
Fold 3 Epoch 057; Train loss: 0.0000; Time: 00:00:02
Fold 3 Epoch 058; Train loss: 0.0000; Time: 00:00:02
Fold 3 Epoch 059; Train loss: 0.0000; Time: 00:00:02
Fold 3 Epoch 060; Train loss: 0.0000; Time: 00:00:02
Fold 3: Evaluation at Epoch 60
HR@5       NDCG@5     HR@10      NDCG@10   
0.056291   0.025687   0.145695   0.053685  
Loss: 0.0000
Fold 3 Epoch 061; Train loss: 0.0000; Time: 00:00:02
Fold 3 Epoch 062; Train loss: 0.0000; Time: 00:00:02
Fold 3 Epoch 063; Train loss: 0.0000; Time: 00:00:02
Fold 3 Epoch 064; Train loss: 0.0000; Time: 00:00:02
Fold 3 Epoch 065; Train loss: 0.0000; Time: 00:00:02
Fold 3 Epoch 066; Train loss: 0.0000; Time: 00:00:02
Fold 3 Epoch 067; Train loss: 0.0000; Time: 00:00:02
Fold 3 Epoch 068; Train loss: 0.0000; Time: 00:00:02
Fold 3 Epoch 069; Train loss: 0.0000; Time: 00:00:02
Fold 3 Epoch 070; Train loss: 0.0000; Time: 00:00:02
Fold 3: Evaluation at Epoch 70
HR@5       NDCG@5     HR@10      NDCG@10   
0.046358   0.026677   0.145695   0.057655  
Loss: 0.0000
Fold 3 Epoch 071; Train loss: 0.0000; Time: 00:00:02
Fold 3 Epoch 072; Train loss: 0.0000; Time: 00:00:02
Fold 3 Epoch 073; Train loss: 0.0000; Time: 00:00:02
Fold 3 Epoch 074; Train loss: 0.0000; Time: 00:00:02
Fold 3 Epoch 075; Train loss: 0.0000; Time: 00:00:02
Fold 3 Epoch 076; Train loss: 0.0000; Time: 00:00:02
Fold 3 Epoch 077; Train loss: 0.0000; Time: 00:00:02
Fold 3 Epoch 078; Train loss: 0.0000; Time: 00:00:02
Fold 3 Epoch 079; Train loss: 0.0000; Time: 00:00:02
Fold 3 Epoch 080; Train loss: 0.0000; Time: 00:00:02
Fold 3: Evaluation at Epoch 80
HR@5       NDCG@5     HR@10      NDCG@10   
0.112583   0.056328   0.895695   0.318303  
Loss: 0.0000
Fold 3 Epoch 081; Train loss: 0.0000; Time: 00:00:02
Fold 3 Epoch 082; Train loss: 0.0000; Time: 00:00:02
Fold 3 Epoch 083; Train loss: 0.0000; Time: 00:00:02
Fold 3 Epoch 084; Train loss: 0.0000; Time: 00:00:02
Fold 3 Epoch 085; Train loss: 0.0000; Time: 00:00:02
Fold 3 Epoch 086; Train loss: 0.0000; Time: 00:00:02
Fold 3 Epoch 087; Train loss: 0.0000; Time: 00:00:02
Fold 3 Epoch 088; Train loss: 0.0000; Time: 00:00:02
Fold 3 Epoch 089; Train loss: 0.0000; Time: 00:00:02
Fold 3 Epoch 090; Train loss: 0.0000; Time: 00:00:02
Fold 3: Evaluation at Epoch 90
HR@5       NDCG@5     HR@10      NDCG@10   
0.056291   0.024704   0.102649   0.039496  
Loss: 0.0000
Fold 3 Epoch 091; Train loss: 0.0000; Time: 00:00:02
Fold 3 Epoch 092; Train loss: 0.0000; Time: 00:00:02
Fold 3 Epoch 093; Train loss: 0.0000; Time: 00:00:02
Fold 3 Epoch 094; Train loss: 0.0000; Time: 00:00:02
Fold 3 Epoch 095; Train loss: 0.0000; Time: 00:00:02
Fold 3 Epoch 096; Train loss: 0.0000; Time: 00:00:02
Fold 3 Epoch 097; Train loss: 0.0000; Time: 00:00:02
Fold 3 Epoch 098; Train loss: 0.0000; Time: 00:00:02
Fold 3 Epoch 099; Train loss: 0.0000; Time: 00:00:02
Fold 3 Epoch 100; Train loss: 0.0000; Time: 00:00:02
Fold 3: Evaluation at Epoch 100
HR@5       NDCG@5     HR@10      NDCG@10   
0.048013   0.023779   0.291391   0.101204  
Loss: 0.0000
[lr candidate 0.05] Fold 3: HR@10 = [np.float64(0.8559602649006622), np.float64(0.8956953642384106), np.float64(0.8890728476821192), np.float64(0.8857615894039735), np.float64(0.8956953642384106), np.float64(0.1456953642384106), np.float64(0.1456953642384106), np.float64(0.8956953642384106), np.float64(0.10264900662251655), np.float64(0.2913907284768212)]
Tuning lr:  40%|████      | 2/5 [08:50<13:14, 264.80s/it]
========== Fold 3 ==========
INFO:root:Using statics: seq_size = 10, genre_vocab_size = 18
Fold 3 Epoch 001; Train loss: 0.2381; Time: 00:00:02
Fold 3 Epoch 002; Train loss: 0.1168; Time: 00:00:02
Fold 3 Epoch 003; Train loss: 0.0922; Time: 00:00:02
Fold 3 Epoch 004; Train loss: 0.0725; Time: 00:00:02
Fold 3 Epoch 005; Train loss: 0.0517; Time: 00:00:02
Fold 3 Epoch 006; Train loss: 0.0370; Time: 00:00:02
Fold 3 Epoch 007; Train loss: 0.0231; Time: 00:00:02
Fold 3 Epoch 008; Train loss: 0.0114; Time: 00:00:02
Fold 3 Epoch 009; Train loss: 0.0041; Time: 00:00:01
Fold 3 Epoch 010; Train loss: 0.0014; Time: 00:00:02
Fold 3: Evaluation at Epoch 10
HR@5       NDCG@5     HR@10      NDCG@10   
0.556291   0.333880   0.862583   0.435451  
Loss: 0.0020
Fold 3 Epoch 011; Train loss: 0.0005; Time: 00:00:02
Fold 3 Epoch 012; Train loss: 0.0003; Time: 00:00:02
Fold 3 Epoch 013; Train loss: 0.0002; Time: 00:00:02
Fold 3 Epoch 014; Train loss: 0.0001; Time: 00:00:02
Fold 3 Epoch 015; Train loss: 0.0001; Time: 00:00:02
Fold 3 Epoch 016; Train loss: 0.0000; Time: 00:00:02
Fold 3 Epoch 017; Train loss: 0.0000; Time: 00:00:02
Fold 3 Epoch 018; Train loss: 0.0000; Time: 00:00:02
Fold 3 Epoch 019; Train loss: 0.0000; Time: 00:00:02
Fold 3 Epoch 020; Train loss: 0.0000; Time: 00:00:02
Fold 3: Evaluation at Epoch 20
HR@5       NDCG@5     HR@10      NDCG@10   
0.556291   0.319993   0.895695   0.431163  
Loss: 0.0000
Fold 3 Epoch 021; Train loss: 0.0000; Time: 00:00:03
Fold 3 Epoch 022; Train loss: 0.0000; Time: 00:00:02
Fold 3 Epoch 023; Train loss: 0.0000; Time: 00:00:02
Fold 3 Epoch 024; Train loss: 0.0000; Time: 00:00:02
Fold 3 Epoch 025; Train loss: 0.0000; Time: 00:00:02
Fold 3 Epoch 026; Train loss: 0.0000; Time: 00:00:02
Fold 3 Epoch 027; Train loss: 0.0000; Time: 00:00:01
Fold 3 Epoch 028; Train loss: 0.0000; Time: 00:00:02
Fold 3 Epoch 029; Train loss: 0.0000; Time: 00:00:02
Fold 3 Epoch 030; Train loss: 0.0000; Time: 00:00:02
Fold 3: Evaluation at Epoch 30
HR@5       NDCG@5     HR@10      NDCG@10   
0.562914   0.322555   0.895695   0.431366  
Loss: 0.0000
Fold 3 Epoch 031; Train loss: 0.0000; Time: 00:00:02
Fold 3 Epoch 032; Train loss: 0.0000; Time: 00:00:02
Fold 3 Epoch 033; Train loss: 0.0000; Time: 00:00:02
Fold 3 Epoch 034; Train loss: 0.0000; Time: 00:00:02
Fold 3 Epoch 035; Train loss: 0.0000; Time: 00:00:02
Fold 3 Epoch 036; Train loss: 0.0000; Time: 00:00:02
Fold 3 Epoch 037; Train loss: 0.0000; Time: 00:00:02
Fold 3 Epoch 038; Train loss: 0.0000; Time: 00:00:02
Fold 3 Epoch 039; Train loss: 0.0000; Time: 00:00:02
Fold 3 Epoch 040; Train loss: 0.0000; Time: 00:00:02
Fold 3: Evaluation at Epoch 40
HR@5       NDCG@5     HR@10      NDCG@10   
0.562914   0.313630   0.895695   0.428611  
Loss: 0.0000
Fold 3 Epoch 041; Train loss: 0.0000; Time: 00:00:02
Fold 3 Epoch 042; Train loss: 0.0000; Time: 00:00:02
Fold 3 Epoch 043; Train loss: 0.0000; Time: 00:00:02
Fold 3 Epoch 044; Train loss: 0.0000; Time: 00:00:02
Fold 3 Epoch 045; Train loss: 0.0000; Time: 00:00:02
Fold 3 Epoch 046; Train loss: 0.0000; Time: 00:00:02
Fold 3 Epoch 047; Train loss: 0.0000; Time: 00:00:02
Fold 3 Epoch 048; Train loss: 0.0000; Time: 00:00:02
Fold 3 Epoch 049; Train loss: 0.0000; Time: 00:00:02
Fold 3 Epoch 050; Train loss: 0.0000; Time: 00:00:02
Fold 3: Evaluation at Epoch 50
HR@5       NDCG@5     HR@10      NDCG@10   
0.562914   0.407077   0.899007   0.523976  
Loss: 0.0000
Fold 3 Epoch 051; Train loss: 0.0000; Time: 00:00:02
Fold 3 Epoch 052; Train loss: 0.0000; Time: 00:00:02
Fold 3 Epoch 053; Train loss: 0.0000; Time: 00:00:02
Fold 3 Epoch 054; Train loss: 0.0000; Time: 00:00:02
Fold 3 Epoch 055; Train loss: 0.0000; Time: 00:00:02
Fold 3 Epoch 056; Train loss: 0.0000; Time: 00:00:02
Fold 3 Epoch 057; Train loss: 0.0000; Time: 00:00:02
Fold 3 Epoch 058; Train loss: 0.0000; Time: 00:00:02
Fold 3 Epoch 059; Train loss: 0.0000; Time: 00:00:02
Fold 3 Epoch 060; Train loss: 0.0000; Time: 00:00:02
Fold 3: Evaluation at Epoch 60
HR@5       NDCG@5     HR@10      NDCG@10   
0.819536   0.410907   0.859272   0.423081  
Loss: 0.0000
Fold 3 Epoch 061; Train loss: 0.0000; Time: 00:00:02
Fold 3 Epoch 062; Train loss: 0.0000; Time: 00:00:02
Fold 3 Epoch 063; Train loss: 0.0000; Time: 00:00:02
Fold 3 Epoch 064; Train loss: 0.0000; Time: 00:00:02
Fold 3 Epoch 065; Train loss: 0.0000; Time: 00:00:02
Fold 3 Epoch 066; Train loss: 0.0000; Time: 00:00:02
Fold 3 Epoch 067; Train loss: 0.0000; Time: 00:00:02
Fold 3 Epoch 068; Train loss: 0.0000; Time: 00:00:02
Fold 3 Epoch 069; Train loss: 0.0000; Time: 00:00:02
Fold 3 Epoch 070; Train loss: 0.0000; Time: 00:00:02
Fold 3: Evaluation at Epoch 70
HR@5       NDCG@5     HR@10      NDCG@10   
0.562914   0.313630   0.895695   0.428463  
Loss: 0.0000
Fold 3 Epoch 071; Train loss: 0.0000; Time: 00:00:02
Fold 3 Epoch 072; Train loss: 0.0000; Time: 00:00:02
Fold 3 Epoch 073; Train loss: 0.0000; Time: 00:00:02
Fold 3 Epoch 074; Train loss: 0.0000; Time: 00:00:02
Fold 3 Epoch 075; Train loss: 0.0000; Time: 00:00:02
Fold 3 Epoch 076; Train loss: 0.0000; Time: 00:00:02
Fold 3 Epoch 077; Train loss: 0.0000; Time: 00:00:02
Fold 3 Epoch 078; Train loss: 0.0000; Time: 00:00:02
Fold 3 Epoch 079; Train loss: 0.0000; Time: 00:00:02
Fold 3 Epoch 080; Train loss: 0.0000; Time: 00:00:02
Fold 3: Evaluation at Epoch 80
HR@5       NDCG@5     HR@10      NDCG@10   
0.562914   0.322555   0.895695   0.437387  
Loss: 0.0000
Fold 3 Epoch 081; Train loss: 0.0000; Time: 00:00:03
Fold 3 Epoch 082; Train loss: 0.0000; Time: 00:00:02
Fold 3 Epoch 083; Train loss: 0.0000; Time: 00:00:02
Fold 3 Epoch 084; Train loss: 0.0000; Time: 00:00:02
Fold 3 Epoch 085; Train loss: 0.0000; Time: 00:00:02
Fold 3 Epoch 086; Train loss: 0.0000; Time: 00:00:02
Fold 3 Epoch 087; Train loss: 0.0000; Time: 00:00:02
Fold 3 Epoch 088; Train loss: 0.0000; Time: 00:00:02
Fold 3 Epoch 089; Train loss: 0.0000; Time: 00:00:02
Fold 3 Epoch 090; Train loss: 0.0000; Time: 00:00:02
Fold 3: Evaluation at Epoch 90
HR@5       NDCG@5     HR@10      NDCG@10   
0.562914   0.278947   0.895695   0.393780  
Loss: 0.0000
Fold 3 Epoch 091; Train loss: 0.0000; Time: 00:00:02
Fold 3 Epoch 092; Train loss: 0.0000; Time: 00:00:02
Fold 3 Epoch 093; Train loss: 0.0000; Time: 00:00:02
Fold 3 Epoch 094; Train loss: 0.0000; Time: 00:00:02
Fold 3 Epoch 095; Train loss: 0.0000; Time: 00:00:02
Fold 3 Epoch 096; Train loss: 0.0000; Time: 00:00:02
Fold 3 Epoch 097; Train loss: 0.0000; Time: 00:00:02
Fold 3 Epoch 098; Train loss: 0.0000; Time: 00:00:02
Fold 3 Epoch 099; Train loss: 0.0000; Time: 00:00:02
Fold 3 Epoch 100; Train loss: 0.0000; Time: 00:00:02
Fold 3: Evaluation at Epoch 100
HR@5       NDCG@5     HR@10      NDCG@10   
0.562914   0.322555   0.895695   0.437387  
Loss: 0.0000
[lr candidate 0.01] Fold 3: HR@10 = [np.float64(0.8625827814569537), np.float64(0.8956953642384106), np.float64(0.8956953642384106), np.float64(0.8956953642384106), np.float64(0.8990066225165563), np.float64(0.859271523178808), np.float64(0.8956953642384106), np.float64(0.8956953642384106), np.float64(0.8956953642384106), np.float64(0.8956953642384106)]
Tuning lr:  60%|██████    | 3/5 [13:18<08:52, 266.17s/it]
========== Fold 3 ==========
INFO:root:Using statics: seq_size = 10, genre_vocab_size = 18
Fold 3 Epoch 001; Train loss: 0.3146; Time: 00:00:02
Fold 3 Epoch 002; Train loss: 0.1433; Time: 00:00:02
Fold 3 Epoch 003; Train loss: 0.1257; Time: 00:00:02
Fold 3 Epoch 004; Train loss: 0.1137; Time: 00:00:02
Fold 3 Epoch 005; Train loss: 0.0988; Time: 00:00:02
Fold 3 Epoch 006; Train loss: 0.0889; Time: 00:00:02
Fold 3 Epoch 007; Train loss: 0.0787; Time: 00:00:02
Fold 3 Epoch 008; Train loss: 0.0683; Time: 00:00:01
Fold 3 Epoch 009; Train loss: 0.0589; Time: 00:00:01
Fold 3 Epoch 010; Train loss: 0.0510; Time: 00:00:02
Fold 3: Evaluation at Epoch 10
HR@5       NDCG@5     HR@10      NDCG@10   
0.473510   0.298841   0.725166   0.381307  
Loss: 0.0574
Fold 3 Epoch 011; Train loss: 0.0432; Time: 00:00:02
Fold 3 Epoch 012; Train loss: 0.0344; Time: 00:00:02
Fold 3 Epoch 013; Train loss: 0.0271; Time: 00:00:02
Fold 3 Epoch 014; Train loss: 0.0207; Time: 00:00:02
Fold 3 Epoch 015; Train loss: 0.0149; Time: 00:00:02
Fold 3 Epoch 016; Train loss: 0.0115; Time: 00:00:02
Fold 3 Epoch 017; Train loss: 0.0091; Time: 00:00:02
Fold 3 Epoch 018; Train loss: 0.0069; Time: 00:00:02
Fold 3 Epoch 019; Train loss: 0.0051; Time: 00:00:02
Fold 3 Epoch 020; Train loss: 0.0034; Time: 00:00:02
Fold 3: Evaluation at Epoch 20
HR@5       NDCG@5     HR@10      NDCG@10   
0.650662   0.345319   0.874172   0.423255  
Loss: 0.0032
Fold 3 Epoch 021; Train loss: 0.0020; Time: 00:00:02
Fold 3 Epoch 022; Train loss: 0.0011; Time: 00:00:02
Fold 3 Epoch 023; Train loss: 0.0007; Time: 00:00:02
Fold 3 Epoch 024; Train loss: 0.0004; Time: 00:00:02
Fold 3 Epoch 025; Train loss: 0.0002; Time: 00:00:02
Fold 3 Epoch 026; Train loss: 0.0002; Time: 00:00:02
Fold 3 Epoch 027; Train loss: 0.0001; Time: 00:00:02
Fold 3 Epoch 028; Train loss: 0.0001; Time: 00:00:02
Fold 3 Epoch 029; Train loss: 0.0001; Time: 00:00:02
Fold 3 Epoch 030; Train loss: 0.0001; Time: 00:00:02
Fold 3: Evaluation at Epoch 30
HR@5       NDCG@5     HR@10      NDCG@10   
0.564570   0.278735   0.895695   0.387294  
Loss: 0.0002
Fold 3 Epoch 031; Train loss: 0.0000; Time: 00:00:02
Fold 3 Epoch 032; Train loss: 0.0000; Time: 00:00:02
Fold 3 Epoch 033; Train loss: 0.0000; Time: 00:00:02
Fold 3 Epoch 034; Train loss: 0.0000; Time: 00:00:02
Fold 3 Epoch 035; Train loss: 0.0000; Time: 00:00:02
Fold 3 Epoch 036; Train loss: 0.0000; Time: 00:00:02
Fold 3 Epoch 037; Train loss: 0.0000; Time: 00:00:02
Fold 3 Epoch 038; Train loss: 0.0000; Time: 00:00:02
Fold 3 Epoch 039; Train loss: 0.0000; Time: 00:00:02
Fold 3 Epoch 040; Train loss: 0.0000; Time: 00:00:02
Fold 3: Evaluation at Epoch 40
HR@5       NDCG@5     HR@10      NDCG@10   
0.562914   0.278095   0.895695   0.387243  
Loss: 0.0000
Fold 3 Epoch 041; Train loss: 0.0000; Time: 00:00:02
Fold 3 Epoch 042; Train loss: 0.0000; Time: 00:00:02
Fold 3 Epoch 043; Train loss: 0.0000; Time: 00:00:02
Fold 3 Epoch 044; Train loss: 0.0000; Time: 00:00:02
Fold 3 Epoch 045; Train loss: 0.0000; Time: 00:00:02
Fold 3 Epoch 046; Train loss: 0.0000; Time: 00:00:02
Fold 3 Epoch 047; Train loss: 0.0000; Time: 00:00:02
Fold 3 Epoch 048; Train loss: 0.0000; Time: 00:00:02
Fold 3 Epoch 049; Train loss: 0.0000; Time: 00:00:02
Fold 3 Epoch 050; Train loss: 0.0000; Time: 00:00:02
Fold 3: Evaluation at Epoch 50
HR@5       NDCG@5     HR@10      NDCG@10   
0.562914   0.278095   0.895695   0.387243  
Loss: 0.0000
Fold 3 Epoch 051; Train loss: 0.0000; Time: 00:00:02
Fold 3 Epoch 052; Train loss: 0.0000; Time: 00:00:02
Fold 3 Epoch 053; Train loss: 0.0000; Time: 00:00:02
Fold 3 Epoch 054; Train loss: 0.0000; Time: 00:00:02
Fold 3 Epoch 055; Train loss: 0.0000; Time: 00:00:02
Fold 3 Epoch 056; Train loss: 0.0000; Time: 00:00:02
Fold 3 Epoch 057; Train loss: 0.0000; Time: 00:00:02
Fold 3 Epoch 058; Train loss: 0.0000; Time: 00:00:02
Fold 3 Epoch 059; Train loss: 0.0000; Time: 00:00:02
Fold 3 Epoch 060; Train loss: 0.0000; Time: 00:00:02
Fold 3: Evaluation at Epoch 60
HR@5       NDCG@5     HR@10      NDCG@10   
0.564570   0.278735   0.895695   0.387294  
Loss: 0.0000
Fold 3 Epoch 061; Train loss: 0.0000; Time: 00:00:02
Fold 3 Epoch 062; Train loss: 0.0000; Time: 00:00:02
Fold 3 Epoch 063; Train loss: 0.0000; Time: 00:00:02
Fold 3 Epoch 064; Train loss: 0.0000; Time: 00:00:02
Fold 3 Epoch 065; Train loss: 0.0000; Time: 00:00:02
Fold 3 Epoch 066; Train loss: 0.0000; Time: 00:00:02
Fold 3 Epoch 067; Train loss: 0.0000; Time: 00:00:02
Fold 3 Epoch 068; Train loss: 0.0000; Time: 00:00:02
Fold 3 Epoch 069; Train loss: 0.0000; Time: 00:00:02
Fold 3 Epoch 070; Train loss: 0.0000; Time: 00:00:02
Fold 3: Evaluation at Epoch 70
HR@5       NDCG@5     HR@10      NDCG@10   
0.562914   0.278095   0.895695   0.387243  
Loss: 0.0000
Fold 3 Epoch 071; Train loss: 0.0000; Time: 00:00:02
Fold 3 Epoch 072; Train loss: 0.0000; Time: 00:00:02
Fold 3 Epoch 073; Train loss: 0.0000; Time: 00:00:02
Fold 3 Epoch 074; Train loss: 0.0000; Time: 00:00:02
Fold 3 Epoch 075; Train loss: 0.0000; Time: 00:00:02
Fold 3 Epoch 076; Train loss: 0.0000; Time: 00:00:02
Fold 3 Epoch 077; Train loss: 0.0000; Time: 00:00:02
Fold 3 Epoch 078; Train loss: 0.0000; Time: 00:00:02
Fold 3 Epoch 079; Train loss: 0.0000; Time: 00:00:02
Fold 3 Epoch 080; Train loss: 0.0000; Time: 00:00:02
Fold 3: Evaluation at Epoch 80
HR@5       NDCG@5     HR@10      NDCG@10   
0.562914   0.278095   0.862583   0.377672  
Loss: 0.0000
Fold 3 Epoch 081; Train loss: 0.0000; Time: 00:00:02
Fold 3 Epoch 082; Train loss: 0.0000; Time: 00:00:02
Fold 3 Epoch 083; Train loss: 0.0000; Time: 00:00:02
Fold 3 Epoch 084; Train loss: 0.0000; Time: 00:00:02
Fold 3 Epoch 085; Train loss: 0.0000; Time: 00:00:02
Fold 3 Epoch 086; Train loss: 0.0000; Time: 00:00:02
Fold 3 Epoch 087; Train loss: 0.0000; Time: 00:00:02
Fold 3 Epoch 088; Train loss: 0.0000; Time: 00:00:02
Fold 3 Epoch 089; Train loss: 0.0000; Time: 00:00:02
Fold 3 Epoch 090; Train loss: 0.0000; Time: 00:00:02
Fold 3: Evaluation at Epoch 90
HR@5       NDCG@5     HR@10      NDCG@10   
0.562914   0.278095   0.862583   0.377672  
Loss: 0.0000
Fold 3 Epoch 091; Train loss: 0.0000; Time: 00:00:02
Fold 3 Epoch 092; Train loss: 0.0000; Time: 00:00:02
Fold 3 Epoch 093; Train loss: 0.0000; Time: 00:00:02
Fold 3 Epoch 094; Train loss: 0.0000; Time: 00:00:02
Fold 3 Epoch 095; Train loss: 0.0000; Time: 00:00:02
Fold 3 Epoch 096; Train loss: 0.0000; Time: 00:00:02
Fold 3 Epoch 097; Train loss: 0.0000; Time: 00:00:02
Fold 3 Epoch 098; Train loss: 0.0000; Time: 00:00:02
Fold 3 Epoch 099; Train loss: 0.0000; Time: 00:00:02
Fold 3 Epoch 100; Train loss: 0.0000; Time: 00:00:02
Fold 3: Evaluation at Epoch 100
HR@5       NDCG@5     HR@10      NDCG@10   
0.562914   0.278095   0.862583   0.377672  
Loss: 0.0000
[lr candidate 0.005] Fold 3: HR@10 = [np.float64(0.7251655629139073), np.float64(0.8741721854304636), np.float64(0.8956953642384106), np.float64(0.8956953642384106), np.float64(0.8956953642384106), np.float64(0.8956953642384106), np.float64(0.8956953642384106), np.float64(0.8625827814569537), np.float64(0.8625827814569537), np.float64(0.8625827814569537)]
Tuning lr:  80%|████████  | 4/5 [17:41<04:24, 264.93s/it]
========== Fold 3 ==========
INFO:root:Using statics: seq_size = 10, genre_vocab_size = 18
Fold 3 Epoch 001; Train loss: 0.6027; Time: 00:00:02
Fold 3 Epoch 002; Train loss: 0.2838; Time: 00:00:02
Fold 3 Epoch 003; Train loss: 0.2081; Time: 00:00:02
Fold 3 Epoch 004; Train loss: 0.1765; Time: 00:00:02
Fold 3 Epoch 005; Train loss: 0.1605; Time: 00:00:02
Fold 3 Epoch 006; Train loss: 0.1508; Time: 00:00:02
Fold 3 Epoch 007; Train loss: 0.1416; Time: 00:00:02
Fold 3 Epoch 008; Train loss: 0.1372; Time: 00:00:02
Fold 3 Epoch 009; Train loss: 0.1337; Time: 00:00:02
Fold 3 Epoch 010; Train loss: 0.1307; Time: 00:00:02
Fold 3: Evaluation at Epoch 10
HR@5       NDCG@5     HR@10      NDCG@10   
0.675497   0.533543   0.774834   0.565701  
Loss: 0.1646
Fold 3 Epoch 011; Train loss: 0.1244; Time: 00:00:02
Fold 3 Epoch 012; Train loss: 0.1190; Time: 00:00:02
Fold 3 Epoch 013; Train loss: 0.1201; Time: 00:00:02
Fold 3 Epoch 014; Train loss: 0.1172; Time: 00:00:02
Fold 3 Epoch 015; Train loss: 0.1111; Time: 00:00:02
Fold 3 Epoch 016; Train loss: 0.1118; Time: 00:00:02
Fold 3 Epoch 017; Train loss: 0.1087; Time: 00:00:02
Fold 3 Epoch 018; Train loss: 0.1037; Time: 00:00:02
Fold 3 Epoch 019; Train loss: 0.1009; Time: 00:00:02
Fold 3 Epoch 020; Train loss: 0.1019; Time: 00:00:02
Fold 3: Evaluation at Epoch 20
HR@5       NDCG@5     HR@10      NDCG@10   
0.693709   0.546246   0.819536   0.586421  
Loss: 0.1230
Fold 3 Epoch 021; Train loss: 0.1000; Time: 00:00:02
Fold 3 Epoch 022; Train loss: 0.0964; Time: 00:00:02
Fold 3 Epoch 023; Train loss: 0.0925; Time: 00:00:02
Fold 3 Epoch 024; Train loss: 0.0910; Time: 00:00:02
Fold 3 Epoch 025; Train loss: 0.0896; Time: 00:00:02
Fold 3 Epoch 026; Train loss: 0.0877; Time: 00:00:02
Fold 3 Epoch 027; Train loss: 0.0827; Time: 00:00:02
Fold 3 Epoch 028; Train loss: 0.0813; Time: 00:00:02
Fold 3 Epoch 029; Train loss: 0.0771; Time: 00:00:02
Fold 3 Epoch 030; Train loss: 0.0759; Time: 00:00:02
Fold 3: Evaluation at Epoch 30
HR@5       NDCG@5     HR@10      NDCG@10   
0.705298   0.555602   0.816225   0.591775  
Loss: 0.1022
Fold 3 Epoch 031; Train loss: 0.0750; Time: 00:00:02
Fold 3 Epoch 032; Train loss: 0.0724; Time: 00:00:02
Fold 3 Epoch 033; Train loss: 0.0708; Time: 00:00:02
Fold 3 Epoch 034; Train loss: 0.0686; Time: 00:00:02
Fold 3 Epoch 035; Train loss: 0.0675; Time: 00:00:02
Fold 3 Epoch 036; Train loss: 0.0631; Time: 00:00:02
Fold 3 Epoch 037; Train loss: 0.0631; Time: 00:00:02
Fold 3 Epoch 038; Train loss: 0.0604; Time: 00:00:02
Fold 3 Epoch 039; Train loss: 0.0572; Time: 00:00:02
Fold 3 Epoch 040; Train loss: 0.0547; Time: 00:00:02
Fold 3: Evaluation at Epoch 40
HR@5       NDCG@5     HR@10      NDCG@10   
0.701987   0.524872   0.846026   0.571542  
Loss: 0.0759
Fold 3 Epoch 041; Train loss: 0.0527; Time: 00:00:02
Fold 3 Epoch 042; Train loss: 0.0510; Time: 00:00:02
Fold 3 Epoch 043; Train loss: 0.0498; Time: 00:00:02
Fold 3 Epoch 044; Train loss: 0.0474; Time: 00:00:02
Fold 3 Epoch 045; Train loss: 0.0455; Time: 00:00:02
Fold 3 Epoch 046; Train loss: 0.0440; Time: 00:00:02
Fold 3 Epoch 047; Train loss: 0.0417; Time: 00:00:02
Fold 3 Epoch 048; Train loss: 0.0392; Time: 00:00:02
Fold 3 Epoch 049; Train loss: 0.0368; Time: 00:00:02
Fold 3 Epoch 050; Train loss: 0.0349; Time: 00:00:02
Fold 3: Evaluation at Epoch 50
HR@5       NDCG@5     HR@10      NDCG@10   
0.690397   0.512494   0.827815   0.557598  
Loss: 0.0471
Fold 3 Epoch 051; Train loss: 0.0338; Time: 00:00:02
Fold 3 Epoch 052; Train loss: 0.0318; Time: 00:00:02
Fold 3 Epoch 053; Train loss: 0.0294; Time: 00:00:02
Fold 3 Epoch 054; Train loss: 0.0277; Time: 00:00:02
Fold 3 Epoch 055; Train loss: 0.0261; Time: 00:00:01
Fold 3 Epoch 056; Train loss: 0.0243; Time: 00:00:02
Fold 3 Epoch 057; Train loss: 0.0219; Time: 00:00:02
Fold 3 Epoch 058; Train loss: 0.0210; Time: 00:00:02
Fold 3 Epoch 059; Train loss: 0.0188; Time: 00:00:02
Fold 3 Epoch 060; Train loss: 0.0184; Time: 00:00:02
Fold 3: Evaluation at Epoch 60
HR@5       NDCG@5     HR@10      NDCG@10   
0.675497   0.471335   0.841060   0.523929  
Loss: 0.0263
Fold 3 Epoch 061; Train loss: 0.0167; Time: 00:00:02
Fold 3 Epoch 062; Train loss: 0.0154; Time: 00:00:02
Fold 3 Epoch 063; Train loss: 0.0139; Time: 00:00:02
Fold 3 Epoch 064; Train loss: 0.0128; Time: 00:00:02
Fold 3 Epoch 065; Train loss: 0.0118; Time: 00:00:01
Fold 3 Epoch 066; Train loss: 0.0108; Time: 00:00:02
Fold 3 Epoch 067; Train loss: 0.0097; Time: 00:00:02
Fold 3 Epoch 068; Train loss: 0.0088; Time: 00:00:02
Fold 3 Epoch 069; Train loss: 0.0080; Time: 00:00:02
Fold 3 Epoch 070; Train loss: 0.0072; Time: 00:00:02
Fold 3: Evaluation at Epoch 70
HR@5       NDCG@5     HR@10      NDCG@10   
0.566225   0.313364   0.903974   0.422161  
Loss: 0.0114
Fold 3 Epoch 071; Train loss: 0.0065; Time: 00:00:02
Fold 3 Epoch 072; Train loss: 0.0058; Time: 00:00:02
Fold 3 Epoch 073; Train loss: 0.0052; Time: 00:00:02
Fold 3 Epoch 074; Train loss: 0.0046; Time: 00:00:02
Fold 3 Epoch 075; Train loss: 0.0042; Time: 00:00:02
Fold 3 Epoch 076; Train loss: 0.0037; Time: 00:00:02
Fold 3 Epoch 077; Train loss: 0.0033; Time: 00:00:02
Fold 3 Epoch 078; Train loss: 0.0030; Time: 00:00:02
Fold 3 Epoch 079; Train loss: 0.0027; Time: 00:00:02
Fold 3 Epoch 080; Train loss: 0.0024; Time: 00:00:02
Fold 3: Evaluation at Epoch 80
HR@5       NDCG@5     HR@10      NDCG@10   
0.344371   0.152587   0.900662   0.331133  
Loss: 0.0047
Fold 3 Epoch 081; Train loss: 0.0022; Time: 00:00:02
Fold 3 Epoch 082; Train loss: 0.0019; Time: 00:00:02
Fold 3 Epoch 083; Train loss: 0.0017; Time: 00:00:02
Fold 3 Epoch 084; Train loss: 0.0016; Time: 00:00:02
Fold 3 Epoch 085; Train loss: 0.0014; Time: 00:00:02
Fold 3 Epoch 086; Train loss: 0.0013; Time: 00:00:02
Fold 3 Epoch 087; Train loss: 0.0012; Time: 00:00:02
Fold 3 Epoch 088; Train loss: 0.0011; Time: 00:00:02
Fold 3 Epoch 089; Train loss: 0.0009; Time: 00:00:02
Fold 3 Epoch 090; Train loss: 0.0009; Time: 00:00:02
Fold 3: Evaluation at Epoch 90
HR@5       NDCG@5     HR@10      NDCG@10   
0.278146   0.117123   0.897351   0.322799  
Loss: 0.0022
Fold 3 Epoch 091; Train loss: 0.0008; Time: 00:00:02
Fold 3 Epoch 092; Train loss: 0.0007; Time: 00:00:02
Fold 3 Epoch 093; Train loss: 0.0007; Time: 00:00:02
Fold 3 Epoch 094; Train loss: 0.0006; Time: 00:00:02
Fold 3 Epoch 095; Train loss: 0.0005; Time: 00:00:02
Fold 3 Epoch 096; Train loss: 0.0005; Time: 00:00:02
Fold 3 Epoch 097; Train loss: 0.0005; Time: 00:00:02
Fold 3 Epoch 098; Train loss: 0.0004; Time: 00:00:02
Fold 3 Epoch 099; Train loss: 0.0004; Time: 00:00:02
Fold 3 Epoch 100; Train loss: 0.0004; Time: 00:00:02
Fold 3: Evaluation at Epoch 100
HR@5       NDCG@5     HR@10      NDCG@10   
0.278146   0.125902   0.902318   0.333014  
Loss: 0.0011
[lr candidate 0.001] Fold 3: HR@10 = [np.float64(0.7748344370860927), np.float64(0.8195364238410596), np.float64(0.8162251655629139), np.float64(0.8460264900662252), np.float64(0.8278145695364238), np.float64(0.8410596026490066), np.float64(0.9039735099337748), np.float64(0.9006622516556292), np.float64(0.8973509933774835), np.float64(0.902317880794702)]
Tuning lr: 100%|██████████| 5/5 [22:05<00:00, 264.47s/it]Tuning lr: 100%|██████████| 5/5 [22:05<00:00, 265.05s/it]

Best learning rate found: 0.001
Tuning optimizer:   0%|          | 0/4 [00:00<?, ?it/s]
========== Fold 3 ==========
INFO:root:Using statics: seq_size = 10, genre_vocab_size = 18
Fold 3 Epoch 001; Train loss: 0.6555; Time: 00:00:02
Fold 3 Epoch 002; Train loss: 0.2976; Time: 00:00:02
Fold 3 Epoch 003; Train loss: 0.2187; Time: 00:00:02
Fold 3 Epoch 004; Train loss: 0.1845; Time: 00:00:02
Fold 3 Epoch 005; Train loss: 0.1691; Time: 00:00:02
Fold 3 Epoch 006; Train loss: 0.1564; Time: 00:00:02
Fold 3 Epoch 007; Train loss: 0.1472; Time: 00:00:02
Fold 3 Epoch 008; Train loss: 0.1420; Time: 00:00:02
Fold 3 Epoch 009; Train loss: 0.1364; Time: 00:00:02
Fold 3 Epoch 010; Train loss: 0.1329; Time: 00:00:02
Fold 3: Evaluation at Epoch 10
HR@5       NDCG@5     HR@10      NDCG@10   
0.523179   0.434312   0.663907   0.478975  
Loss: 0.1823
Fold 3 Epoch 011; Train loss: 0.1292; Time: 00:00:01
Fold 3 Epoch 012; Train loss: 0.1241; Time: 00:00:02
Fold 3 Epoch 013; Train loss: 0.1226; Time: 00:00:02
Fold 3 Epoch 014; Train loss: 0.1195; Time: 00:00:02
Fold 3 Epoch 015; Train loss: 0.1158; Time: 00:00:02
Fold 3 Epoch 016; Train loss: 0.1142; Time: 00:00:02
Fold 3 Epoch 017; Train loss: 0.1144; Time: 00:00:02
Fold 3 Epoch 018; Train loss: 0.1081; Time: 00:00:02
Fold 3 Epoch 019; Train loss: 0.1056; Time: 00:00:02
Fold 3 Epoch 020; Train loss: 0.1061; Time: 00:00:02
Fold 3: Evaluation at Epoch 20
HR@5       NDCG@5     HR@10      NDCG@10   
0.519868   0.430088   0.645695   0.470085  
Loss: 0.1400
Fold 3 Epoch 021; Train loss: 0.1011; Time: 00:00:02
Fold 3 Epoch 022; Train loss: 0.0975; Time: 00:00:02
Fold 3 Epoch 023; Train loss: 0.0986; Time: 00:00:02
Fold 3 Epoch 024; Train loss: 0.0911; Time: 00:00:02
Fold 3 Epoch 025; Train loss: 0.0904; Time: 00:00:02
Fold 3 Epoch 026; Train loss: 0.0899; Time: 00:00:02
Fold 3 Epoch 027; Train loss: 0.0872; Time: 00:00:02
Fold 3 Epoch 028; Train loss: 0.0822; Time: 00:00:02
Fold 3 Epoch 029; Train loss: 0.0801; Time: 00:00:02
Fold 3 Epoch 030; Train loss: 0.0780; Time: 00:00:02
Fold 3: Evaluation at Epoch 30
HR@5       NDCG@5     HR@10      NDCG@10   
0.500000   0.349533   0.688742   0.410251  
Loss: 0.1129
Fold 3 Epoch 031; Train loss: 0.0783; Time: 00:00:02
Fold 3 Epoch 032; Train loss: 0.0746; Time: 00:00:02
Fold 3 Epoch 033; Train loss: 0.0741; Time: 00:00:02
Fold 3 Epoch 034; Train loss: 0.0718; Time: 00:00:02
Fold 3 Epoch 035; Train loss: 0.0671; Time: 00:00:02
Fold 3 Epoch 036; Train loss: 0.0657; Time: 00:00:02
Fold 3 Epoch 037; Train loss: 0.0641; Time: 00:00:02
Fold 3 Epoch 038; Train loss: 0.0621; Time: 00:00:02
Fold 3 Epoch 039; Train loss: 0.0613; Time: 00:00:02
Fold 3 Epoch 040; Train loss: 0.0586; Time: 00:00:01
Fold 3: Evaluation at Epoch 40
HR@5       NDCG@5     HR@10      NDCG@10   
0.415563   0.312693   0.624172   0.378440  
Loss: 0.0743
Fold 3 Epoch 041; Train loss: 0.0558; Time: 00:00:02
Fold 3 Epoch 042; Train loss: 0.0519; Time: 00:00:02
Fold 3 Epoch 043; Train loss: 0.0511; Time: 00:00:02
Fold 3 Epoch 044; Train loss: 0.0504; Time: 00:00:02
Fold 3 Epoch 045; Train loss: 0.0476; Time: 00:00:02
Fold 3 Epoch 046; Train loss: 0.0452; Time: 00:00:02
Fold 3 Epoch 047; Train loss: 0.0433; Time: 00:00:02
Fold 3 Epoch 048; Train loss: 0.0422; Time: 00:00:02
Fold 3 Epoch 049; Train loss: 0.0396; Time: 00:00:02
Fold 3 Epoch 050; Train loss: 0.0379; Time: 00:00:02
Fold 3: Evaluation at Epoch 50
HR@5       NDCG@5     HR@10      NDCG@10   
0.501656   0.348812   0.693709   0.410014  
Loss: 0.0473
Fold 3 Epoch 051; Train loss: 0.0355; Time: 00:00:02
Fold 3 Epoch 052; Train loss: 0.0344; Time: 00:00:02
Fold 3 Epoch 053; Train loss: 0.0324; Time: 00:00:02
Fold 3 Epoch 054; Train loss: 0.0313; Time: 00:00:02
Fold 3 Epoch 055; Train loss: 0.0287; Time: 00:00:02
Fold 3 Epoch 056; Train loss: 0.0270; Time: 00:00:02
Fold 3 Epoch 057; Train loss: 0.0257; Time: 00:00:02
Fold 3 Epoch 058; Train loss: 0.0239; Time: 00:00:02
Fold 3 Epoch 059; Train loss: 0.0219; Time: 00:00:02
Fold 3 Epoch 060; Train loss: 0.0205; Time: 00:00:02
Fold 3: Evaluation at Epoch 60
HR@5       NDCG@5     HR@10      NDCG@10   
0.549669   0.374995   0.758278   0.442058  
Loss: 0.0271
Fold 3 Epoch 061; Train loss: 0.0196; Time: 00:00:02
Fold 3 Epoch 062; Train loss: 0.0175; Time: 00:00:02
Fold 3 Epoch 063; Train loss: 0.0164; Time: 00:00:02
Fold 3 Epoch 064; Train loss: 0.0148; Time: 00:00:02
Fold 3 Epoch 065; Train loss: 0.0136; Time: 00:00:02
Fold 3 Epoch 066; Train loss: 0.0126; Time: 00:00:02
Fold 3 Epoch 067; Train loss: 0.0120; Time: 00:00:02
Fold 3 Epoch 068; Train loss: 0.0104; Time: 00:00:02
Fold 3 Epoch 069; Train loss: 0.0096; Time: 00:00:02
Fold 3 Epoch 070; Train loss: 0.0085; Time: 00:00:02
Fold 3: Evaluation at Epoch 70
HR@5       NDCG@5     HR@10      NDCG@10   
0.488411   0.363477   0.894040   0.500254  
Loss: 0.0138
Fold 3 Epoch 071; Train loss: 0.0077; Time: 00:00:02
Fold 3 Epoch 072; Train loss: 0.0068; Time: 00:00:02
Fold 3 Epoch 073; Train loss: 0.0060; Time: 00:00:02
Fold 3 Epoch 074; Train loss: 0.0054; Time: 00:00:02
Fold 3 Epoch 075; Train loss: 0.0049; Time: 00:00:02
Fold 3 Epoch 076; Train loss: 0.0043; Time: 00:00:02
Fold 3 Epoch 077; Train loss: 0.0038; Time: 00:00:02
Fold 3 Epoch 078; Train loss: 0.0034; Time: 00:00:02
Fold 3 Epoch 079; Train loss: 0.0030; Time: 00:00:02
Fold 3 Epoch 080; Train loss: 0.0028; Time: 00:00:02
Fold 3: Evaluation at Epoch 80
HR@5       NDCG@5     HR@10      NDCG@10   
0.564570   0.400430   0.882450   0.505170  
Loss: 0.0054
Fold 3 Epoch 081; Train loss: 0.0024; Time: 00:00:02
Fold 3 Epoch 082; Train loss: 0.0022; Time: 00:00:02
Fold 3 Epoch 083; Train loss: 0.0019; Time: 00:00:02
Fold 3 Epoch 084; Train loss: 0.0017; Time: 00:00:02
Fold 3 Epoch 085; Train loss: 0.0015; Time: 00:00:02
Fold 3 Epoch 086; Train loss: 0.0014; Time: 00:00:02
Fold 3 Epoch 087; Train loss: 0.0012; Time: 00:00:02
Fold 3 Epoch 088; Train loss: 0.0011; Time: 00:00:02
Fold 3 Epoch 089; Train loss: 0.0010; Time: 00:00:02
Fold 3 Epoch 090; Train loss: 0.0009; Time: 00:00:02
Fold 3: Evaluation at Epoch 90
HR@5       NDCG@5     HR@10      NDCG@10   
0.562914   0.404049   0.862583   0.509174  
Loss: 0.0025
Fold 3 Epoch 091; Train loss: 0.0008; Time: 00:00:02
Fold 3 Epoch 092; Train loss: 0.0007; Time: 00:00:02
Fold 3 Epoch 093; Train loss: 0.0007; Time: 00:00:02
Fold 3 Epoch 094; Train loss: 0.0006; Time: 00:00:02
Fold 3 Epoch 095; Train loss: 0.0006; Time: 00:00:02
Fold 3 Epoch 096; Train loss: 0.0005; Time: 00:00:02
Fold 3 Epoch 097; Train loss: 0.0005; Time: 00:00:01
Fold 3 Epoch 098; Train loss: 0.0004; Time: 00:00:02
Fold 3 Epoch 099; Train loss: 0.0004; Time: 00:00:02
Fold 3 Epoch 100; Train loss: 0.0004; Time: 00:00:02
Fold 3: Evaluation at Epoch 100
HR@5       NDCG@5     HR@10      NDCG@10   
0.566225   0.398147   0.862583   0.502259  
Loss: 0.0012
[optimizer candidate nadam] Fold 3: HR@10 = [np.float64(0.6639072847682119), np.float64(0.6456953642384106), np.float64(0.6887417218543046), np.float64(0.6241721854304636), np.float64(0.6937086092715232), np.float64(0.7582781456953642), np.float64(0.8940397350993378), np.float64(0.8824503311258278), np.float64(0.8625827814569537), np.float64(0.8625827814569537)]
Tuning optimizer:  25%|██▌       | 1/4 [04:24<13:12, 264.16s/it]
========== Fold 3 ==========
INFO:root:Using statics: seq_size = 10, genre_vocab_size = 18
Fold 3 Epoch 001; Train loss: 1.1319; Time: 00:00:02
Fold 3 Epoch 002; Train loss: 0.9272; Time: 00:00:02
Fold 3 Epoch 003; Train loss: 0.7906; Time: 00:00:03
Fold 3 Epoch 004; Train loss: 0.6859; Time: 00:00:04
Fold 3 Epoch 005; Train loss: 0.5994; Time: 00:00:03
Fold 3 Epoch 006; Train loss: 0.5253; Time: 00:00:03
Fold 3 Epoch 007; Train loss: 0.4621; Time: 00:00:02
Fold 3 Epoch 008; Train loss: 0.4078; Time: 00:00:02
Fold 3 Epoch 009; Train loss: 0.3623; Time: 00:00:02
Fold 3 Epoch 010; Train loss: 0.3189; Time: 00:00:02
Fold 3: Evaluation at Epoch 10
HR@5       NDCG@5     HR@10      NDCG@10   
0.738411   0.568750   0.817881   0.594700  
Loss: 0.4437
Fold 3 Epoch 011; Train loss: 0.2889; Time: 00:00:02
Fold 3 Epoch 012; Train loss: 0.2589; Time: 00:00:02
Fold 3 Epoch 013; Train loss: 0.2386; Time: 00:00:03
Fold 3 Epoch 014; Train loss: 0.2167; Time: 00:00:03
Fold 3 Epoch 015; Train loss: 0.1999; Time: 00:00:02
Fold 3 Epoch 016; Train loss: 0.1862; Time: 00:00:02
Fold 3 Epoch 017; Train loss: 0.1772; Time: 00:00:02
Fold 3 Epoch 018; Train loss: 0.1678; Time: 00:00:03
Fold 3 Epoch 019; Train loss: 0.1567; Time: 00:00:02
Fold 3 Epoch 020; Train loss: 0.1480; Time: 00:00:02
Fold 3: Evaluation at Epoch 20
HR@5       NDCG@5     HR@10      NDCG@10   
0.625828   0.479051   0.751656   0.519553  
Loss: 0.2093
Fold 3 Epoch 021; Train loss: 0.1411; Time: 00:00:02
Fold 3 Epoch 022; Train loss: 0.1382; Time: 00:00:02
Fold 3 Epoch 023; Train loss: 0.1314; Time: 00:00:02
Fold 3 Epoch 024; Train loss: 0.1267; Time: 00:00:03
Fold 3 Epoch 025; Train loss: 0.1258; Time: 00:00:02
Fold 3 Epoch 026; Train loss: 0.1205; Time: 00:00:03
Fold 3 Epoch 027; Train loss: 0.1146; Time: 00:00:03
Fold 3 Epoch 028; Train loss: 0.1120; Time: 00:00:02
Fold 3 Epoch 029; Train loss: 0.1114; Time: 00:00:02
Fold 3 Epoch 030; Train loss: 0.1061; Time: 00:00:03
Fold 3: Evaluation at Epoch 30
HR@5       NDCG@5     HR@10      NDCG@10   
0.615894   0.473939   0.736755   0.512560  
Loss: 0.1402
Fold 3 Epoch 031; Train loss: 0.1042; Time: 00:00:02
Fold 3 Epoch 032; Train loss: 0.1009; Time: 00:00:03
Fold 3 Epoch 033; Train loss: 0.0966; Time: 00:00:02
Fold 3 Epoch 034; Train loss: 0.0952; Time: 00:00:02
Fold 3 Epoch 035; Train loss: 0.0938; Time: 00:00:02
Fold 3 Epoch 036; Train loss: 0.0911; Time: 00:00:02
Fold 3 Epoch 037; Train loss: 0.0867; Time: 00:00:02
Fold 3 Epoch 038; Train loss: 0.0857; Time: 00:00:02
Fold 3 Epoch 039; Train loss: 0.0842; Time: 00:00:03
Fold 3 Epoch 040; Train loss: 0.0820; Time: 00:00:02
Fold 3: Evaluation at Epoch 40
HR@5       NDCG@5     HR@10      NDCG@10   
0.591060   0.457754   0.735099   0.504218  
Loss: 0.1108
Fold 3 Epoch 041; Train loss: 0.0786; Time: 00:00:02
Fold 3 Epoch 042; Train loss: 0.0778; Time: 00:00:02
Fold 3 Epoch 043; Train loss: 0.0758; Time: 00:00:02
Fold 3 Epoch 044; Train loss: 0.0739; Time: 00:00:02
Fold 3 Epoch 045; Train loss: 0.0711; Time: 00:00:02
Fold 3 Epoch 046; Train loss: 0.0710; Time: 00:00:03
Fold 3 Epoch 047; Train loss: 0.0687; Time: 00:00:02
Fold 3 Epoch 048; Train loss: 0.0650; Time: 00:00:02
Fold 3 Epoch 049; Train loss: 0.0620; Time: 00:00:02
Fold 3 Epoch 050; Train loss: 0.0617; Time: 00:00:02
Fold 3: Evaluation at Epoch 50
HR@5       NDCG@5     HR@10      NDCG@10   
0.591060   0.427370   0.756623   0.479577  
Loss: 0.0848
Fold 3 Epoch 051; Train loss: 0.0616; Time: 00:00:02
Fold 3 Epoch 052; Train loss: 0.0592; Time: 00:00:02
Fold 3 Epoch 053; Train loss: 0.0565; Time: 00:00:03
Fold 3 Epoch 054; Train loss: 0.0556; Time: 00:00:02
Fold 3 Epoch 055; Train loss: 0.0522; Time: 00:00:02
Fold 3 Epoch 056; Train loss: 0.0516; Time: 00:00:02
Fold 3 Epoch 057; Train loss: 0.0500; Time: 00:00:03
Fold 3 Epoch 058; Train loss: 0.0480; Time: 00:00:03
Fold 3 Epoch 059; Train loss: 0.0463; Time: 00:00:02
Fold 3 Epoch 060; Train loss: 0.0441; Time: 00:00:03
Fold 3: Evaluation at Epoch 60
HR@5       NDCG@5     HR@10      NDCG@10   
0.591060   0.400892   0.764901   0.456536  
Loss: 0.0605
Fold 3 Epoch 061; Train loss: 0.0428; Time: 00:00:03
Fold 3 Epoch 062; Train loss: 0.0415; Time: 00:00:02
Fold 3 Epoch 063; Train loss: 0.0396; Time: 00:00:02
Fold 3 Epoch 064; Train loss: 0.0377; Time: 00:00:02
Fold 3 Epoch 065; Train loss: 0.0365; Time: 00:00:02
Fold 3 Epoch 066; Train loss: 0.0344; Time: 00:00:02
Fold 3 Epoch 067; Train loss: 0.0335; Time: 00:00:03
Fold 3 Epoch 068; Train loss: 0.0314; Time: 00:00:04
Fold 3 Epoch 069; Train loss: 0.0292; Time: 00:00:03
Fold 3 Epoch 070; Train loss: 0.0283; Time: 00:00:02
Fold 3: Evaluation at Epoch 70
HR@5       NDCG@5     HR@10      NDCG@10   
0.632450   0.378342   0.796358   0.431125  
Loss: 0.0418
Fold 3 Epoch 071; Train loss: 0.0262; Time: 00:00:02
Fold 3 Epoch 072; Train loss: 0.0249; Time: 00:00:02
Fold 3 Epoch 073; Train loss: 0.0241; Time: 00:00:02
Fold 3 Epoch 074; Train loss: 0.0218; Time: 00:00:02
Fold 3 Epoch 075; Train loss: 0.0210; Time: 00:00:02
Fold 3 Epoch 076; Train loss: 0.0193; Time: 00:00:02
Fold 3 Epoch 077; Train loss: 0.0178; Time: 00:00:03
Fold 3 Epoch 078; Train loss: 0.0168; Time: 00:00:02
Fold 3 Epoch 079; Train loss: 0.0152; Time: 00:00:02
Fold 3 Epoch 080; Train loss: 0.0145; Time: 00:00:03
Fold 3: Evaluation at Epoch 80
HR@5       NDCG@5     HR@10      NDCG@10   
0.571192   0.330576   0.867550   0.425779  
Loss: 0.0240
Fold 3 Epoch 081; Train loss: 0.0129; Time: 00:00:02
Fold 3 Epoch 082; Train loss: 0.0113; Time: 00:00:03
Fold 3 Epoch 083; Train loss: 0.0105; Time: 00:00:02
Fold 3 Epoch 084; Train loss: 0.0092; Time: 00:00:02
Fold 3 Epoch 085; Train loss: 0.0085; Time: 00:00:02
Fold 3 Epoch 086; Train loss: 0.0071; Time: 00:00:02
Fold 3 Epoch 087; Train loss: 0.0064; Time: 00:00:02
Fold 3 Epoch 088; Train loss: 0.0052; Time: 00:00:02
Fold 3 Epoch 089; Train loss: 0.0043; Time: 00:00:02
Fold 3 Epoch 090; Train loss: 0.0034; Time: 00:00:03
Fold 3: Evaluation at Epoch 90
HR@5       NDCG@5     HR@10      NDCG@10   
0.299669   0.169541   0.902318   0.361021  
Loss: 0.0119
Fold 3 Epoch 091; Train loss: 0.0027; Time: 00:00:03
Fold 3 Epoch 092; Train loss: 0.0021; Time: 00:00:03
Fold 3 Epoch 093; Train loss: 0.0016; Time: 00:00:02
Fold 3 Epoch 094; Train loss: 0.0013; Time: 00:00:02
Fold 3 Epoch 095; Train loss: 0.0010; Time: 00:00:02
Fold 3 Epoch 096; Train loss: 0.0008; Time: 00:00:03
Fold 3 Epoch 097; Train loss: 0.0007; Time: 00:00:02
Fold 3 Epoch 098; Train loss: 0.0005; Time: 00:00:02
Fold 3 Epoch 099; Train loss: 0.0005; Time: 00:00:03
Fold 3 Epoch 100; Train loss: 0.0003; Time: 00:00:02
Fold 3: Evaluation at Epoch 100
HR@5       NDCG@5     HR@10      NDCG@10   
0.279801   0.129729   0.640728   0.243045  
Loss: 0.0030
[optimizer candidate lamb] Fold 3: HR@10 = [np.float64(0.8178807947019867), np.float64(0.7516556291390728), np.float64(0.7367549668874173), np.float64(0.7350993377483444), np.float64(0.7566225165562914), np.float64(0.7649006622516556), np.float64(0.7963576158940397), np.float64(0.8675496688741722), np.float64(0.902317880794702), np.float64(0.640728476821192)]
Tuning optimizer:  50%|█████     | 2/4 [09:25<09:32, 286.28s/it]
========== Fold 3 ==========
INFO:root:Using statics: seq_size = 10, genre_vocab_size = 18
Fold 3 Epoch 001; Train loss: 0.6042; Time: 00:00:01
Fold 3 Epoch 002; Train loss: 0.2802; Time: 00:00:02
Fold 3 Epoch 003; Train loss: 0.2072; Time: 00:00:02
Fold 3 Epoch 004; Train loss: 0.1782; Time: 00:00:02
Fold 3 Epoch 005; Train loss: 0.1620; Time: 00:00:02
Fold 3 Epoch 006; Train loss: 0.1527; Time: 00:00:02
Fold 3 Epoch 007; Train loss: 0.1503; Time: 00:00:02
Fold 3 Epoch 008; Train loss: 0.1429; Time: 00:00:02
Fold 3 Epoch 009; Train loss: 0.1423; Time: 00:00:02
Fold 3 Epoch 010; Train loss: 0.1325; Time: 00:00:02
Fold 3: Evaluation at Epoch 10
HR@5       NDCG@5     HR@10      NDCG@10   
0.693709   0.553212   0.814570   0.592288  
Loss: 0.1932
Fold 3 Epoch 011; Train loss: 0.1344; Time: 00:00:02
Fold 3 Epoch 012; Train loss: 0.1300; Time: 00:00:02
Fold 3 Epoch 013; Train loss: 0.1273; Time: 00:00:02
Fold 3 Epoch 014; Train loss: 0.1278; Time: 00:00:02
Fold 3 Epoch 015; Train loss: 0.1238; Time: 00:00:02
Fold 3 Epoch 016; Train loss: 0.1231; Time: 00:00:02
Fold 3 Epoch 017; Train loss: 0.1213; Time: 00:00:02
Fold 3 Epoch 018; Train loss: 0.1174; Time: 00:00:02
Fold 3 Epoch 019; Train loss: 0.1152; Time: 00:00:02
Fold 3 Epoch 020; Train loss: 0.1115; Time: 00:00:02
Fold 3: Evaluation at Epoch 20
HR@5       NDCG@5     HR@10      NDCG@10   
0.701987   0.551888   0.801325   0.583734  
Loss: 0.1461
Fold 3 Epoch 021; Train loss: 0.1117; Time: 00:00:02
Fold 3 Epoch 022; Train loss: 0.1080; Time: 00:00:02
Fold 3 Epoch 023; Train loss: 0.1062; Time: 00:00:02
Fold 3 Epoch 024; Train loss: 0.1039; Time: 00:00:02
Fold 3 Epoch 025; Train loss: 0.1008; Time: 00:00:02
Fold 3 Epoch 026; Train loss: 0.1020; Time: 00:00:02
Fold 3 Epoch 027; Train loss: 0.1010; Time: 00:00:02
Fold 3 Epoch 028; Train loss: 0.0957; Time: 00:00:02
Fold 3 Epoch 029; Train loss: 0.0904; Time: 00:00:02
Fold 3 Epoch 030; Train loss: 0.0894; Time: 00:00:02
Fold 3: Evaluation at Epoch 30
HR@5       NDCG@5     HR@10      NDCG@10   
0.548013   0.399951   0.692053   0.445658  
Loss: 0.1289
Fold 3 Epoch 031; Train loss: 0.0881; Time: 00:00:02
Fold 3 Epoch 032; Train loss: 0.0879; Time: 00:00:02
Fold 3 Epoch 033; Train loss: 0.0850; Time: 00:00:03
Fold 3 Epoch 034; Train loss: 0.0844; Time: 00:00:03
Fold 3 Epoch 035; Train loss: 0.0802; Time: 00:00:02
Fold 3 Epoch 036; Train loss: 0.0801; Time: 00:00:02
Fold 3 Epoch 037; Train loss: 0.0772; Time: 00:00:02
Fold 3 Epoch 038; Train loss: 0.0772; Time: 00:00:02
Fold 3 Epoch 039; Train loss: 0.0730; Time: 00:00:02
Fold 3 Epoch 040; Train loss: 0.0705; Time: 00:00:02
Fold 3: Evaluation at Epoch 40
HR@5       NDCG@5     HR@10      NDCG@10   
0.581126   0.451214   0.750000   0.506312  
Loss: 0.1024
Fold 3 Epoch 041; Train loss: 0.0705; Time: 00:00:02
Fold 3 Epoch 042; Train loss: 0.0677; Time: 00:00:02
Fold 3 Epoch 043; Train loss: 0.0655; Time: 00:00:02
Fold 3 Epoch 044; Train loss: 0.0640; Time: 00:00:02
Fold 3 Epoch 045; Train loss: 0.0627; Time: 00:00:02
Fold 3 Epoch 046; Train loss: 0.0600; Time: 00:00:02
Fold 3 Epoch 047; Train loss: 0.0574; Time: 00:00:02
Fold 3 Epoch 048; Train loss: 0.0572; Time: 00:00:02
Fold 3 Epoch 049; Train loss: 0.0539; Time: 00:00:02
Fold 3 Epoch 050; Train loss: 0.0532; Time: 00:00:02
Fold 3: Evaluation at Epoch 50
HR@5       NDCG@5     HR@10      NDCG@10   
0.663907   0.540008   0.804636   0.585199  
Loss: 0.0764
Fold 3 Epoch 051; Train loss: 0.0510; Time: 00:00:02
Fold 3 Epoch 052; Train loss: 0.0495; Time: 00:00:02
Fold 3 Epoch 053; Train loss: 0.0472; Time: 00:00:02
Fold 3 Epoch 054; Train loss: 0.0456; Time: 00:00:02
Fold 3 Epoch 055; Train loss: 0.0440; Time: 00:00:02
Fold 3 Epoch 056; Train loss: 0.0421; Time: 00:00:02
Fold 3 Epoch 057; Train loss: 0.0395; Time: 00:00:02
Fold 3 Epoch 058; Train loss: 0.0388; Time: 00:00:02
Fold 3 Epoch 059; Train loss: 0.0373; Time: 00:00:02
Fold 3 Epoch 060; Train loss: 0.0360; Time: 00:00:02
Fold 3: Evaluation at Epoch 60
HR@5       NDCG@5     HR@10      NDCG@10   
0.615894   0.490603   0.794702   0.548176  
Loss: 0.0545
Fold 3 Epoch 061; Train loss: 0.0340; Time: 00:00:02
Fold 3 Epoch 062; Train loss: 0.0318; Time: 00:00:02
Fold 3 Epoch 063; Train loss: 0.0304; Time: 00:00:02
Fold 3 Epoch 064; Train loss: 0.0291; Time: 00:00:02
Fold 3 Epoch 065; Train loss: 0.0275; Time: 00:00:02
Fold 3 Epoch 066; Train loss: 0.0261; Time: 00:00:02
Fold 3 Epoch 067; Train loss: 0.0241; Time: 00:00:02
Fold 3 Epoch 068; Train loss: 0.0225; Time: 00:00:02
Fold 3 Epoch 069; Train loss: 0.0213; Time: 00:00:02
Fold 3 Epoch 070; Train loss: 0.0205; Time: 00:00:02
Fold 3: Evaluation at Epoch 70
HR@5       NDCG@5     HR@10      NDCG@10   
0.617550   0.479404   0.836093   0.550680  
Loss: 0.0397
Fold 3 Epoch 071; Train loss: 0.0194; Time: 00:00:02
Fold 3 Epoch 072; Train loss: 0.0178; Time: 00:00:02
Fold 3 Epoch 073; Train loss: 0.0170; Time: 00:00:02
Fold 3 Epoch 074; Train loss: 0.0158; Time: 00:00:02
Fold 3 Epoch 075; Train loss: 0.0148; Time: 00:00:02
Fold 3 Epoch 076; Train loss: 0.0141; Time: 00:00:02
Fold 3 Epoch 077; Train loss: 0.0136; Time: 00:00:02
Fold 3 Epoch 078; Train loss: 0.0125; Time: 00:00:02
Fold 3 Epoch 079; Train loss: 0.0115; Time: 00:00:02
Fold 3 Epoch 080; Train loss: 0.0108; Time: 00:00:02
Fold 3: Evaluation at Epoch 80
HR@5       NDCG@5     HR@10      NDCG@10   
0.504967   0.392117   0.879139   0.510183  
Loss: 0.0249
Fold 3 Epoch 081; Train loss: 0.0099; Time: 00:00:01
Fold 3 Epoch 082; Train loss: 0.0092; Time: 00:00:02
Fold 3 Epoch 083; Train loss: 0.0088; Time: 00:00:02
Fold 3 Epoch 084; Train loss: 0.0078; Time: 00:00:02
Fold 3 Epoch 085; Train loss: 0.0073; Time: 00:00:02
Fold 3 Epoch 086; Train loss: 0.0066; Time: 00:00:02
Fold 3 Epoch 087; Train loss: 0.0060; Time: 00:00:02
Fold 3 Epoch 088; Train loss: 0.0057; Time: 00:00:02
Fold 3 Epoch 089; Train loss: 0.0051; Time: 00:00:02
Fold 3 Epoch 090; Train loss: 0.0047; Time: 00:00:02
Fold 3: Evaluation at Epoch 90
HR@5       NDCG@5     HR@10      NDCG@10   
0.481788   0.271388   0.885762   0.401038  
Loss: 0.0163
Fold 3 Epoch 091; Train loss: 0.0042; Time: 00:00:02
Fold 3 Epoch 092; Train loss: 0.0039; Time: 00:00:02
Fold 3 Epoch 093; Train loss: 0.0036; Time: 00:00:02
Fold 3 Epoch 094; Train loss: 0.0034; Time: 00:00:02
Fold 3 Epoch 095; Train loss: 0.0030; Time: 00:00:02
Fold 3 Epoch 096; Train loss: 0.0029; Time: 00:00:02
Fold 3 Epoch 097; Train loss: 0.0026; Time: 00:00:02
Fold 3 Epoch 098; Train loss: 0.0025; Time: 00:00:02
Fold 3 Epoch 099; Train loss: 0.0023; Time: 00:00:02
Fold 3 Epoch 100; Train loss: 0.0022; Time: 00:00:02
Fold 3: Evaluation at Epoch 100
HR@5       NDCG@5     HR@10      NDCG@10   
0.314570   0.135715   0.880795   0.326973  
Loss: 0.0113
[optimizer candidate adamw] Fold 3: HR@10 = [np.float64(0.8145695364238411), np.float64(0.8013245033112583), np.float64(0.6920529801324503), np.float64(0.75), np.float64(0.804635761589404), np.float64(0.7947019867549668), np.float64(0.8360927152317881), np.float64(0.8791390728476821), np.float64(0.8857615894039735), np.float64(0.8807947019867549)]
Tuning optimizer:  75%|███████▌  | 3/4 [13:52<04:37, 277.26s/it]
========== Fold 3 ==========
INFO:root:Using statics: seq_size = 10, genre_vocab_size = 18
[31mPlease check your arguments if you have upgraded adabelief-pytorch from version 0.0.5.
[31mModifications to default arguments:
[31m                           eps  weight_decouple    rectify
-----------------------  -----  -----------------  ---------
adabelief-pytorch=0.0.5  1e-08  False              False
>=0.1.0 (Current 0.2.0)  1e-16  True               True
[34mSGD better than Adam (e.g. CNN for Image Classification)    Adam better than SGD (e.g. Transformer, GAN)
----------------------------------------------------------  ----------------------------------------------
Recommended eps = 1e-8                                      Recommended eps = 1e-16
[34mFor a complete table of recommended hyperparameters, see
[34mhttps://github.com/juntang-zhuang/Adabelief-Optimizer
[32mYou can disable the log message by setting "print_change_log = False", though it is recommended to keep as a reminder.
[0m
Weight decoupling enabled in AdaBelief
Rectification enabled in AdaBelief
Fold 3 Epoch 001; Train loss: 1.0853; Time: 00:00:02
Fold 3 Epoch 002; Train loss: 1.0707; Time: 00:00:02
Fold 3 Epoch 003; Train loss: 1.0478; Time: 00:00:02
Fold 3 Epoch 004; Train loss: 1.0240; Time: 00:00:02
Fold 3 Epoch 005; Train loss: 0.9984; Time: 00:00:02
Fold 3 Epoch 006; Train loss: 0.9714; Time: 00:00:02
Fold 3 Epoch 007; Train loss: 0.9424; Time: 00:00:02
Fold 3 Epoch 008; Train loss: 0.9112; Time: 00:00:02
Fold 3 Epoch 009; Train loss: 0.8852; Time: 00:00:02
Fold 3 Epoch 010; Train loss: 0.8586; Time: 00:00:02
Fold 3: Evaluation at Epoch 10
HR@5       NDCG@5     HR@10      NDCG@10   
0.435430   0.334520   0.667219   0.408534  
Loss: 1.0428
Fold 3 Epoch 011; Train loss: 0.8320; Time: 00:00:02
Fold 3 Epoch 012; Train loss: 0.8075; Time: 00:00:02
Fold 3 Epoch 013; Train loss: 0.7851; Time: 00:00:02
Fold 3 Epoch 014; Train loss: 0.7594; Time: 00:00:02
Fold 3 Epoch 015; Train loss: 0.7405; Time: 00:00:03
Fold 3 Epoch 016; Train loss: 0.7192; Time: 00:00:02
Fold 3 Epoch 017; Train loss: 0.6997; Time: 00:00:02
Fold 3 Epoch 018; Train loss: 0.6826; Time: 00:00:02
Fold 3 Epoch 019; Train loss: 0.6661; Time: 00:00:02
Fold 3 Epoch 020; Train loss: 0.6490; Time: 00:00:02
Fold 3: Evaluation at Epoch 20
HR@5       NDCG@5     HR@10      NDCG@10   
0.503311   0.366321   0.791391   0.456735  
Loss: 0.8911
Fold 3 Epoch 021; Train loss: 0.6319; Time: 00:00:02
Fold 3 Epoch 022; Train loss: 0.6203; Time: 00:00:02
Fold 3 Epoch 023; Train loss: 0.6050; Time: 00:00:02
Fold 3 Epoch 024; Train loss: 0.5883; Time: 00:00:02
Fold 3 Epoch 025; Train loss: 0.5776; Time: 00:00:02
Fold 3 Epoch 026; Train loss: 0.5634; Time: 00:00:02
Fold 3 Epoch 027; Train loss: 0.5524; Time: 00:00:02
Fold 3 Epoch 028; Train loss: 0.5429; Time: 00:00:02
Fold 3 Epoch 029; Train loss: 0.5264; Time: 00:00:02
Fold 3 Epoch 030; Train loss: 0.5208; Time: 00:00:02
Fold 3: Evaluation at Epoch 30
HR@5       NDCG@5     HR@10      NDCG@10   
0.514901   0.382283   0.824503   0.480747  
Loss: 0.7847
Fold 3 Epoch 031; Train loss: 0.5085; Time: 00:00:02
Fold 3 Epoch 032; Train loss: 0.5006; Time: 00:00:03
Fold 3 Epoch 033; Train loss: 0.4886; Time: 00:00:02
Fold 3 Epoch 034; Train loss: 0.4796; Time: 00:00:02
Fold 3 Epoch 035; Train loss: 0.4704; Time: 00:00:02
Fold 3 Epoch 036; Train loss: 0.4637; Time: 00:00:02
Fold 3 Epoch 037; Train loss: 0.4555; Time: 00:00:02
Fold 3 Epoch 038; Train loss: 0.4470; Time: 00:00:02
Fold 3 Epoch 039; Train loss: 0.4373; Time: 00:00:02
Fold 3 Epoch 040; Train loss: 0.4326; Time: 00:00:02
Fold 3: Evaluation at Epoch 40
HR@5       NDCG@5     HR@10      NDCG@10   
0.571192   0.412950   0.814570   0.492444  
Loss: 0.7061
Fold 3 Epoch 041; Train loss: 0.4247; Time: 00:00:02
Fold 3 Epoch 042; Train loss: 0.4203; Time: 00:00:02
Fold 3 Epoch 043; Train loss: 0.4118; Time: 00:00:02
Fold 3 Epoch 044; Train loss: 0.4074; Time: 00:00:02
Fold 3 Epoch 045; Train loss: 0.3992; Time: 00:00:02
Fold 3 Epoch 046; Train loss: 0.3937; Time: 00:00:02
Fold 3 Epoch 047; Train loss: 0.3906; Time: 00:00:02
Fold 3 Epoch 048; Train loss: 0.3842; Time: 00:00:03
Fold 3 Epoch 049; Train loss: 0.3824; Time: 00:00:02
Fold 3 Epoch 050; Train loss: 0.3734; Time: 00:00:02
Fold 3: Evaluation at Epoch 50
HR@5       NDCG@5     HR@10      NDCG@10   
0.612583   0.443267   0.786424   0.499838  
Loss: 0.6265
Fold 3 Epoch 051; Train loss: 0.3704; Time: 00:00:02
Fold 3 Epoch 052; Train loss: 0.3653; Time: 00:00:02
Fold 3 Epoch 053; Train loss: 0.3622; Time: 00:00:02
Fold 3 Epoch 054; Train loss: 0.3580; Time: 00:00:02
Fold 3 Epoch 055; Train loss: 0.3485; Time: 00:00:02
Fold 3 Epoch 056; Train loss: 0.3510; Time: 00:00:02
Fold 3 Epoch 057; Train loss: 0.3438; Time: 00:00:02
Fold 3 Epoch 058; Train loss: 0.3402; Time: 00:00:02
Fold 3 Epoch 059; Train loss: 0.3415; Time: 00:00:02
Fold 3 Epoch 060; Train loss: 0.3351; Time: 00:00:02
Fold 3: Evaluation at Epoch 60
HR@5       NDCG@5     HR@10      NDCG@10   
0.605960   0.444419   0.763245   0.495321  
Loss: 0.5779
Fold 3 Epoch 061; Train loss: 0.3294; Time: 00:00:02
Fold 3 Epoch 062; Train loss: 0.3316; Time: 00:00:02
Fold 3 Epoch 063; Train loss: 0.3250; Time: 00:00:02
Fold 3 Epoch 064; Train loss: 0.3231; Time: 00:00:02
Fold 3 Epoch 065; Train loss: 0.3177; Time: 00:00:02
Fold 3 Epoch 066; Train loss: 0.3166; Time: 00:00:02
Fold 3 Epoch 067; Train loss: 0.3134; Time: 00:00:02
Fold 3 Epoch 068; Train loss: 0.3128; Time: 00:00:02
Fold 3 Epoch 069; Train loss: 0.3053; Time: 00:00:02
Fold 3 Epoch 070; Train loss: 0.3084; Time: 00:00:02
Fold 3: Evaluation at Epoch 70
HR@5       NDCG@5     HR@10      NDCG@10   
0.602649   0.445963   0.745033   0.492017  
Loss: 0.5249
Fold 3 Epoch 071; Train loss: 0.3015; Time: 00:00:02
Fold 3 Epoch 072; Train loss: 0.2976; Time: 00:00:02
Fold 3 Epoch 073; Train loss: 0.2968; Time: 00:00:02
Fold 3 Epoch 074; Train loss: 0.2997; Time: 00:00:02
Fold 3 Epoch 075; Train loss: 0.2933; Time: 00:00:02
Fold 3 Epoch 076; Train loss: 0.2892; Time: 00:00:02
Fold 3 Epoch 077; Train loss: 0.2899; Time: 00:00:01
Fold 3 Epoch 078; Train loss: 0.2891; Time: 00:00:02
Fold 3 Epoch 079; Train loss: 0.2880; Time: 00:00:02
Fold 3 Epoch 080; Train loss: 0.2795; Time: 00:00:02
Fold 3: Evaluation at Epoch 80
HR@5       NDCG@5     HR@10      NDCG@10   
0.596026   0.443464   0.736755   0.488689  
Loss: 0.4866
Fold 3 Epoch 081; Train loss: 0.2828; Time: 00:00:02
Fold 3 Epoch 082; Train loss: 0.2808; Time: 00:00:02
Fold 3 Epoch 083; Train loss: 0.2785; Time: 00:00:02
Fold 3 Epoch 084; Train loss: 0.2767; Time: 00:00:02
Fold 3 Epoch 085; Train loss: 0.2746; Time: 00:00:02
Fold 3 Epoch 086; Train loss: 0.2708; Time: 00:00:02
Fold 3 Epoch 087; Train loss: 0.2714; Time: 00:00:02
Fold 3 Epoch 088; Train loss: 0.2700; Time: 00:00:01
Fold 3 Epoch 089; Train loss: 0.2681; Time: 00:00:02
Fold 3 Epoch 090; Train loss: 0.2681; Time: 00:00:02
Fold 3: Evaluation at Epoch 90
HR@5       NDCG@5     HR@10      NDCG@10   
0.614238   0.449776   0.726821   0.485914  
Loss: 0.4380
Fold 3 Epoch 091; Train loss: 0.2646; Time: 00:00:02
Fold 3 Epoch 092; Train loss: 0.2638; Time: 00:00:02
Fold 3 Epoch 093; Train loss: 0.2612; Time: 00:00:02
Fold 3 Epoch 094; Train loss: 0.2628; Time: 00:00:02
Fold 3 Epoch 095; Train loss: 0.2607; Time: 00:00:02
Fold 3 Epoch 096; Train loss: 0.2554; Time: 00:00:02
Fold 3 Epoch 097; Train loss: 0.2585; Time: 00:00:02
Fold 3 Epoch 098; Train loss: 0.2578; Time: 00:00:02
Fold 3 Epoch 099; Train loss: 0.2533; Time: 00:00:02
Fold 3 Epoch 100; Train loss: 0.2541; Time: 00:00:02
Fold 3: Evaluation at Epoch 100
HR@5       NDCG@5     HR@10      NDCG@10   
0.615894   0.451670   0.723510   0.485967  
Loss: 0.4187
[optimizer candidate adabelief] Fold 3: HR@10 = [np.float64(0.6672185430463576), np.float64(0.7913907284768212), np.float64(0.8245033112582781), np.float64(0.8145695364238411), np.float64(0.7864238410596026), np.float64(0.7632450331125827), np.float64(0.7450331125827815), np.float64(0.7367549668874173), np.float64(0.7268211920529801), np.float64(0.7235099337748344)]
Tuning optimizer: 100%|██████████| 4/4 [18:31<00:00, 278.13s/it]Tuning optimizer: 100%|██████████| 4/4 [18:31<00:00, 277.98s/it]

Best optimizer found: adamw
Tuning timesteps:   0%|          | 0/5 [00:00<?, ?it/s]
========== Fold 3 ==========
INFO:root:Using statics: seq_size = 10, genre_vocab_size = 18
Fold 3 Epoch 001; Train loss: 0.5940; Time: 00:00:02
Fold 3 Epoch 002; Train loss: 0.2167; Time: 00:00:02
Fold 3 Epoch 003; Train loss: 0.1453; Time: 00:00:02
Fold 3 Epoch 004; Train loss: 0.1201; Time: 00:00:02
Fold 3 Epoch 005; Train loss: 0.1068; Time: 00:00:02
Fold 3 Epoch 006; Train loss: 0.0963; Time: 00:00:02
Fold 3 Epoch 007; Train loss: 0.0915; Time: 00:00:02
Fold 3 Epoch 008; Train loss: 0.0897; Time: 00:00:02
Fold 3 Epoch 009; Train loss: 0.0870; Time: 00:00:02
Fold 3 Epoch 010; Train loss: 0.0823; Time: 00:00:02
Fold 3: Evaluation at Epoch 10
HR@5       NDCG@5     HR@10      NDCG@10   
0.670530   0.512160   0.791391   0.551326  
Loss: 0.1189
Fold 3 Epoch 011; Train loss: 0.0810; Time: 00:00:02
Fold 3 Epoch 012; Train loss: 0.0799; Time: 00:00:02
Fold 3 Epoch 013; Train loss: 0.0773; Time: 00:00:02
Fold 3 Epoch 014; Train loss: 0.0767; Time: 00:00:02
Fold 3 Epoch 015; Train loss: 0.0761; Time: 00:00:02
Fold 3 Epoch 016; Train loss: 0.0727; Time: 00:00:02
Fold 3 Epoch 017; Train loss: 0.0743; Time: 00:00:02
Fold 3 Epoch 018; Train loss: 0.0706; Time: 00:00:02
Fold 3 Epoch 019; Train loss: 0.0730; Time: 00:00:02
Fold 3 Epoch 020; Train loss: 0.0727; Time: 00:00:02
Fold 3: Evaluation at Epoch 20
HR@5       NDCG@5     HR@10      NDCG@10   
0.634106   0.487451   0.764901   0.530074  
Loss: 0.0953
Fold 3 Epoch 021; Train loss: 0.0709; Time: 00:00:02
Fold 3 Epoch 022; Train loss: 0.0702; Time: 00:00:02
Fold 3 Epoch 023; Train loss: 0.0696; Time: 00:00:02
Fold 3 Epoch 024; Train loss: 0.0685; Time: 00:00:02
Fold 3 Epoch 025; Train loss: 0.0673; Time: 00:00:02
Fold 3 Epoch 026; Train loss: 0.0679; Time: 00:00:02
Fold 3 Epoch 027; Train loss: 0.0664; Time: 00:00:02
Fold 3 Epoch 028; Train loss: 0.0667; Time: 00:00:01
Fold 3 Epoch 029; Train loss: 0.0660; Time: 00:00:02
Fold 3 Epoch 030; Train loss: 0.0657; Time: 00:00:02
Fold 3: Evaluation at Epoch 30
HR@5       NDCG@5     HR@10      NDCG@10   
0.612583   0.400388   0.786424   0.456154  
Loss: 0.0865
Fold 3 Epoch 031; Train loss: 0.0646; Time: 00:00:02
Fold 3 Epoch 032; Train loss: 0.0629; Time: 00:00:02
Fold 3 Epoch 033; Train loss: 0.0619; Time: 00:00:02
Fold 3 Epoch 034; Train loss: 0.0622; Time: 00:00:02
Fold 3 Epoch 035; Train loss: 0.0622; Time: 00:00:02
Fold 3 Epoch 036; Train loss: 0.0622; Time: 00:00:02
Fold 3 Epoch 037; Train loss: 0.0617; Time: 00:00:02
Fold 3 Epoch 038; Train loss: 0.0606; Time: 00:00:02
Fold 3 Epoch 039; Train loss: 0.0593; Time: 00:00:02
Fold 3 Epoch 040; Train loss: 0.0595; Time: 00:00:02
Fold 3: Evaluation at Epoch 40
HR@5       NDCG@5     HR@10      NDCG@10   
0.667219   0.503992   0.804636   0.547757  
Loss: 0.0758
Fold 3 Epoch 041; Train loss: 0.0582; Time: 00:00:02
Fold 3 Epoch 042; Train loss: 0.0584; Time: 00:00:02
Fold 3 Epoch 043; Train loss: 0.0588; Time: 00:00:02
Fold 3 Epoch 044; Train loss: 0.0571; Time: 00:00:02
Fold 3 Epoch 045; Train loss: 0.0559; Time: 00:00:02
Fold 3 Epoch 046; Train loss: 0.0543; Time: 00:00:02
Fold 3 Epoch 047; Train loss: 0.0543; Time: 00:00:02
Fold 3 Epoch 048; Train loss: 0.0545; Time: 00:00:02
Fold 3 Epoch 049; Train loss: 0.0530; Time: 00:00:02
Fold 3 Epoch 050; Train loss: 0.0515; Time: 00:00:02
Fold 3: Evaluation at Epoch 50
HR@5       NDCG@5     HR@10      NDCG@10   
0.577815   0.410528   0.764901   0.469683  
Loss: 0.0731
Fold 3 Epoch 051; Train loss: 0.0520; Time: 00:00:02
Fold 3 Epoch 052; Train loss: 0.0517; Time: 00:00:02
Fold 3 Epoch 053; Train loss: 0.0493; Time: 00:00:03
Fold 3 Epoch 054; Train loss: 0.0490; Time: 00:00:02
Fold 3 Epoch 055; Train loss: 0.0488; Time: 00:00:02
Fold 3 Epoch 056; Train loss: 0.0487; Time: 00:00:02
Fold 3 Epoch 057; Train loss: 0.0472; Time: 00:00:02
Fold 3 Epoch 058; Train loss: 0.0467; Time: 00:00:02
Fold 3 Epoch 059; Train loss: 0.0467; Time: 00:00:02
Fold 3 Epoch 060; Train loss: 0.0453; Time: 00:00:02
Fold 3: Evaluation at Epoch 60
HR@5       NDCG@5     HR@10      NDCG@10   
0.566225   0.384353   0.761589   0.446918  
Loss: 0.0609
Fold 3 Epoch 061; Train loss: 0.0455; Time: 00:00:02
Fold 3 Epoch 062; Train loss: 0.0434; Time: 00:00:03
Fold 3 Epoch 063; Train loss: 0.0433; Time: 00:00:02
Fold 3 Epoch 064; Train loss: 0.0423; Time: 00:00:02
Fold 3 Epoch 065; Train loss: 0.0422; Time: 00:00:02
Fold 3 Epoch 066; Train loss: 0.0403; Time: 00:00:02
Fold 3 Epoch 067; Train loss: 0.0394; Time: 00:00:02
Fold 3 Epoch 068; Train loss: 0.0398; Time: 00:00:02
Fold 3 Epoch 069; Train loss: 0.0395; Time: 00:00:01
Fold 3 Epoch 070; Train loss: 0.0380; Time: 00:00:02
Fold 3: Evaluation at Epoch 70
HR@5       NDCG@5     HR@10      NDCG@10   
0.460265   0.316375   0.675497   0.386370  
Loss: 0.0536
Fold 3 Epoch 071; Train loss: 0.0381; Time: 00:00:02
Fold 3 Epoch 072; Train loss: 0.0376; Time: 00:00:02
Fold 3 Epoch 073; Train loss: 0.0355; Time: 00:00:02
Fold 3 Epoch 074; Train loss: 0.0361; Time: 00:00:02
Fold 3 Epoch 075; Train loss: 0.0354; Time: 00:00:02
Fold 3 Epoch 076; Train loss: 0.0350; Time: 00:00:02
Fold 3 Epoch 077; Train loss: 0.0343; Time: 00:00:02
Fold 3 Epoch 078; Train loss: 0.0341; Time: 00:00:02
Fold 3 Epoch 079; Train loss: 0.0336; Time: 00:00:02
Fold 3 Epoch 080; Train loss: 0.0334; Time: 00:00:03
Fold 3: Evaluation at Epoch 80
HR@5       NDCG@5     HR@10      NDCG@10   
0.672185   0.462776   0.859272   0.523715  
Loss: 0.0444
Fold 3 Epoch 081; Train loss: 0.0329; Time: 00:00:02
Fold 3 Epoch 082; Train loss: 0.0314; Time: 00:00:02
Fold 3 Epoch 083; Train loss: 0.0304; Time: 00:00:02
Fold 3 Epoch 084; Train loss: 0.0312; Time: 00:00:02
Fold 3 Epoch 085; Train loss: 0.0297; Time: 00:00:02
Fold 3 Epoch 086; Train loss: 0.0292; Time: 00:00:02
Fold 3 Epoch 087; Train loss: 0.0287; Time: 00:00:02
Fold 3 Epoch 088; Train loss: 0.0282; Time: 00:00:02
Fold 3 Epoch 089; Train loss: 0.0277; Time: 00:00:02
Fold 3 Epoch 090; Train loss: 0.0274; Time: 00:00:02
Fold 3: Evaluation at Epoch 90
HR@5       NDCG@5     HR@10      NDCG@10   
0.620861   0.391087   0.864238   0.470783  
Loss: 0.0371
Fold 3 Epoch 091; Train loss: 0.0270; Time: 00:00:02
Fold 3 Epoch 092; Train loss: 0.0257; Time: 00:00:02
Fold 3 Epoch 093; Train loss: 0.0257; Time: 00:00:02
Fold 3 Epoch 094; Train loss: 0.0249; Time: 00:00:02
Fold 3 Epoch 095; Train loss: 0.0242; Time: 00:00:02
Fold 3 Epoch 096; Train loss: 0.0229; Time: 00:00:02
Fold 3 Epoch 097; Train loss: 0.0230; Time: 00:00:02
Fold 3 Epoch 098; Train loss: 0.0226; Time: 00:00:02
Fold 3 Epoch 099; Train loss: 0.0217; Time: 00:00:02
Fold 3 Epoch 100; Train loss: 0.0211; Time: 00:00:02
Fold 3: Evaluation at Epoch 100
HR@5       NDCG@5     HR@10      NDCG@10   
0.485099   0.277770   0.753311   0.365440  
Loss: 0.0298
[timesteps candidate 100] Fold 3: HR@10 = [np.float64(0.7913907284768212), np.float64(0.7649006622516556), np.float64(0.7864238410596026), np.float64(0.804635761589404), np.float64(0.7649006622516556), np.float64(0.7615894039735099), np.float64(0.6754966887417219), np.float64(0.859271523178808), np.float64(0.8642384105960265), np.float64(0.7533112582781457)]
Tuning timesteps:  20%|██        | 1/5 [04:17<17:09, 257.43s/it]
========== Fold 3 ==========
INFO:root:Using statics: seq_size = 10, genre_vocab_size = 18
Fold 3 Epoch 001; Train loss: 0.5410; Time: 00:00:02
Fold 3 Epoch 002; Train loss: 0.2548; Time: 00:00:02
Fold 3 Epoch 003; Train loss: 0.1915; Time: 00:00:02
Fold 3 Epoch 004; Train loss: 0.1638; Time: 00:00:02
Fold 3 Epoch 005; Train loss: 0.1510; Time: 00:00:02
Fold 3 Epoch 006; Train loss: 0.1466; Time: 00:00:02
Fold 3 Epoch 007; Train loss: 0.1377; Time: 00:00:02
Fold 3 Epoch 008; Train loss: 0.1333; Time: 00:00:02
Fold 3 Epoch 009; Train loss: 0.1265; Time: 00:00:02
Fold 3 Epoch 010; Train loss: 0.1279; Time: 00:00:02
Fold 3: Evaluation at Epoch 10
HR@5       NDCG@5     HR@10      NDCG@10   
0.599338   0.493951   0.768212   0.548852  
Loss: 0.1730
Fold 3 Epoch 011; Train loss: 0.1233; Time: 00:00:01
Fold 3 Epoch 012; Train loss: 0.1207; Time: 00:00:02
Fold 3 Epoch 013; Train loss: 0.1197; Time: 00:00:02
Fold 3 Epoch 014; Train loss: 0.1140; Time: 00:00:02
Fold 3 Epoch 015; Train loss: 0.1138; Time: 00:00:02
Fold 3 Epoch 016; Train loss: 0.1137; Time: 00:00:02
Fold 3 Epoch 017; Train loss: 0.1090; Time: 00:00:02
Fold 3 Epoch 018; Train loss: 0.1089; Time: 00:00:02
Fold 3 Epoch 019; Train loss: 0.1061; Time: 00:00:02
Fold 3 Epoch 020; Train loss: 0.1055; Time: 00:00:02
Fold 3: Evaluation at Epoch 20
HR@5       NDCG@5     HR@10      NDCG@10   
0.509934   0.419127   0.667219   0.469028  
Loss: 0.1463
Fold 3 Epoch 021; Train loss: 0.1021; Time: 00:00:02
Fold 3 Epoch 022; Train loss: 0.0987; Time: 00:00:02
Fold 3 Epoch 023; Train loss: 0.0988; Time: 00:00:01
Fold 3 Epoch 024; Train loss: 0.0948; Time: 00:00:02
Fold 3 Epoch 025; Train loss: 0.0928; Time: 00:00:02
Fold 3 Epoch 026; Train loss: 0.0916; Time: 00:00:02
Fold 3 Epoch 027; Train loss: 0.0911; Time: 00:00:02
Fold 3 Epoch 028; Train loss: 0.0870; Time: 00:00:02
Fold 3 Epoch 029; Train loss: 0.0848; Time: 00:00:02
Fold 3 Epoch 030; Train loss: 0.0830; Time: 00:00:02
Fold 3: Evaluation at Epoch 30
HR@5       NDCG@5     HR@10      NDCG@10   
0.529801   0.433764   0.687086   0.484102  
Loss: 0.1206
Fold 3 Epoch 031; Train loss: 0.0807; Time: 00:00:02
Fold 3 Epoch 032; Train loss: 0.0795; Time: 00:00:01
Fold 3 Epoch 033; Train loss: 0.0766; Time: 00:00:02
Fold 3 Epoch 034; Train loss: 0.0738; Time: 00:00:02
Fold 3 Epoch 035; Train loss: 0.0728; Time: 00:00:01
Fold 3 Epoch 036; Train loss: 0.0703; Time: 00:00:02
Fold 3 Epoch 037; Train loss: 0.0680; Time: 00:00:02
Fold 3 Epoch 038; Train loss: 0.0655; Time: 00:00:02
Fold 3 Epoch 039; Train loss: 0.0665; Time: 00:00:02
Fold 3 Epoch 040; Train loss: 0.0640; Time: 00:00:02
Fold 3: Evaluation at Epoch 40
HR@5       NDCG@5     HR@10      NDCG@10   
0.604305   0.512797   0.802980   0.576572  
Loss: 0.0984
Fold 3 Epoch 041; Train loss: 0.0627; Time: 00:00:02
Fold 3 Epoch 042; Train loss: 0.0588; Time: 00:00:02
Fold 3 Epoch 043; Train loss: 0.0574; Time: 00:00:02
Fold 3 Epoch 044; Train loss: 0.0560; Time: 00:00:02
Fold 3 Epoch 045; Train loss: 0.0534; Time: 00:00:02
Fold 3 Epoch 046; Train loss: 0.0507; Time: 00:00:02
Fold 3 Epoch 047; Train loss: 0.0496; Time: 00:00:02
Fold 3 Epoch 048; Train loss: 0.0468; Time: 00:00:02
Fold 3 Epoch 049; Train loss: 0.0461; Time: 00:00:02
Fold 3 Epoch 050; Train loss: 0.0444; Time: 00:00:02
Fold 3: Evaluation at Epoch 50
HR@5       NDCG@5     HR@10      NDCG@10   
0.584437   0.467714   0.751656   0.522691  
Loss: 0.0681
Fold 3 Epoch 051; Train loss: 0.0419; Time: 00:00:02
Fold 3 Epoch 052; Train loss: 0.0398; Time: 00:00:02
Fold 3 Epoch 053; Train loss: 0.0386; Time: 00:00:02
Fold 3 Epoch 054; Train loss: 0.0363; Time: 00:00:02
Fold 3 Epoch 055; Train loss: 0.0343; Time: 00:00:02
Fold 3 Epoch 056; Train loss: 0.0332; Time: 00:00:02
Fold 3 Epoch 057; Train loss: 0.0314; Time: 00:00:02
Fold 3 Epoch 058; Train loss: 0.0303; Time: 00:00:02
Fold 3 Epoch 059; Train loss: 0.0284; Time: 00:00:02
Fold 3 Epoch 060; Train loss: 0.0274; Time: 00:00:02
Fold 3: Evaluation at Epoch 60
HR@5       NDCG@5     HR@10      NDCG@10   
0.543046   0.341236   0.745033   0.406998  
Loss: 0.0488
Fold 3 Epoch 061; Train loss: 0.0256; Time: 00:00:02
Fold 3 Epoch 062; Train loss: 0.0237; Time: 00:00:02
Fold 3 Epoch 063; Train loss: 0.0226; Time: 00:00:02
Fold 3 Epoch 064; Train loss: 0.0215; Time: 00:00:02
Fold 3 Epoch 065; Train loss: 0.0201; Time: 00:00:02
Fold 3 Epoch 066; Train loss: 0.0186; Time: 00:00:02
Fold 3 Epoch 067; Train loss: 0.0174; Time: 00:00:02
Fold 3 Epoch 068; Train loss: 0.0161; Time: 00:00:02
Fold 3 Epoch 069; Train loss: 0.0151; Time: 00:00:02
Fold 3 Epoch 070; Train loss: 0.0140; Time: 00:00:02
Fold 3: Evaluation at Epoch 70
HR@5       NDCG@5     HR@10      NDCG@10   
0.461921   0.253607   0.756623   0.344406  
Loss: 0.0297
Fold 3 Epoch 071; Train loss: 0.0130; Time: 00:00:02
Fold 3 Epoch 072; Train loss: 0.0121; Time: 00:00:02
Fold 3 Epoch 073; Train loss: 0.0115; Time: 00:00:02
Fold 3 Epoch 074; Train loss: 0.0105; Time: 00:00:02
Fold 3 Epoch 075; Train loss: 0.0098; Time: 00:00:02
Fold 3 Epoch 076; Train loss: 0.0093; Time: 00:00:02
Fold 3 Epoch 077; Train loss: 0.0084; Time: 00:00:02
Fold 3 Epoch 078; Train loss: 0.0079; Time: 00:00:02
Fold 3 Epoch 079; Train loss: 0.0072; Time: 00:00:02
Fold 3 Epoch 080; Train loss: 0.0067; Time: 00:00:02
Fold 3: Evaluation at Epoch 80
HR@5       NDCG@5     HR@10      NDCG@10   
0.144040   0.098546   0.677152   0.268778  
Loss: 0.0169
Fold 3 Epoch 081; Train loss: 0.0062; Time: 00:00:02
Fold 3 Epoch 082; Train loss: 0.0057; Time: 00:00:02
Fold 3 Epoch 083; Train loss: 0.0054; Time: 00:00:02
Fold 3 Epoch 084; Train loss: 0.0049; Time: 00:00:02
Fold 3 Epoch 085; Train loss: 0.0046; Time: 00:00:02
Fold 3 Epoch 086; Train loss: 0.0043; Time: 00:00:02
Fold 3 Epoch 087; Train loss: 0.0040; Time: 00:00:02
Fold 3 Epoch 088; Train loss: 0.0038; Time: 00:00:02
Fold 3 Epoch 089; Train loss: 0.0035; Time: 00:00:02
Fold 3 Epoch 090; Train loss: 0.0033; Time: 00:00:02
Fold 3: Evaluation at Epoch 90
HR@5       NDCG@5     HR@10      NDCG@10   
0.109272   0.081777   0.895695   0.322549  
Loss: 0.0106
Fold 3 Epoch 091; Train loss: 0.0032; Time: 00:00:02
Fold 3 Epoch 092; Train loss: 0.0029; Time: 00:00:02
Fold 3 Epoch 093; Train loss: 0.0028; Time: 00:00:02
Fold 3 Epoch 094; Train loss: 0.0026; Time: 00:00:02
Fold 3 Epoch 095; Train loss: 0.0024; Time: 00:00:02
Fold 3 Epoch 096; Train loss: 0.0023; Time: 00:00:02
Fold 3 Epoch 097; Train loss: 0.0022; Time: 00:00:02
Fold 3 Epoch 098; Train loss: 0.0021; Time: 00:00:02
Fold 3 Epoch 099; Train loss: 0.0020; Time: 00:00:02
Fold 3 Epoch 100; Train loss: 0.0019; Time: 00:00:02
Fold 3: Evaluation at Epoch 100
HR@5       NDCG@5     HR@10      NDCG@10   
0.110927   0.082148   0.889073   0.326605  
Loss: 0.0074
[timesteps candidate 200] Fold 3: HR@10 = [np.float64(0.7682119205298014), np.float64(0.6672185430463576), np.float64(0.6870860927152318), np.float64(0.8029801324503312), np.float64(0.7516556291390728), np.float64(0.7450331125827815), np.float64(0.7566225165562914), np.float64(0.6771523178807947), np.float64(0.8956953642384106), np.float64(0.8890728476821192)]
Tuning timesteps:  40%|████      | 2/5 [08:37<12:57, 259.12s/it]
========== Fold 3 ==========
INFO:root:Using statics: seq_size = 10, genre_vocab_size = 18
Fold 3 Epoch 001; Train loss: 0.6419; Time: 00:00:02
Fold 3 Epoch 002; Train loss: 0.3726; Time: 00:00:02
Fold 3 Epoch 003; Train loss: 0.3065; Time: 00:00:02
Fold 3 Epoch 004; Train loss: 0.2743; Time: 00:00:02
Fold 3 Epoch 005; Train loss: 0.2569; Time: 00:00:02
Fold 3 Epoch 006; Train loss: 0.2460; Time: 00:00:02
Fold 3 Epoch 007; Train loss: 0.2407; Time: 00:00:02
Fold 3 Epoch 008; Train loss: 0.2264; Time: 00:00:02
Fold 3 Epoch 009; Train loss: 0.2201; Time: 00:00:02
Fold 3 Epoch 010; Train loss: 0.2129; Time: 00:00:02
Fold 3: Evaluation at Epoch 10
HR@5       NDCG@5     HR@10      NDCG@10   
0.673841   0.524947   0.814570   0.570420  
Loss: 0.2760
Fold 3 Epoch 011; Train loss: 0.2046; Time: 00:00:02
Fold 3 Epoch 012; Train loss: 0.2004; Time: 00:00:02
Fold 3 Epoch 013; Train loss: 0.1961; Time: 00:00:02
Fold 3 Epoch 014; Train loss: 0.1863; Time: 00:00:02
Fold 3 Epoch 015; Train loss: 0.1810; Time: 00:00:02
Fold 3 Epoch 016; Train loss: 0.1702; Time: 00:00:02
Fold 3 Epoch 017; Train loss: 0.1685; Time: 00:00:02
Fold 3 Epoch 018; Train loss: 0.1623; Time: 00:00:02
Fold 3 Epoch 019; Train loss: 0.1571; Time: 00:00:02
Fold 3 Epoch 020; Train loss: 0.1529; Time: 00:00:02
Fold 3: Evaluation at Epoch 20
HR@5       NDCG@5     HR@10      NDCG@10   
0.650662   0.524686   0.788079   0.568609  
Loss: 0.2035
Fold 3 Epoch 021; Train loss: 0.1485; Time: 00:00:02
Fold 3 Epoch 022; Train loss: 0.1454; Time: 00:00:02
Fold 3 Epoch 023; Train loss: 0.1381; Time: 00:00:02
Fold 3 Epoch 024; Train loss: 0.1322; Time: 00:00:02
Fold 3 Epoch 025; Train loss: 0.1274; Time: 00:00:02
Fold 3 Epoch 026; Train loss: 0.1233; Time: 00:00:02
Fold 3 Epoch 027; Train loss: 0.1192; Time: 00:00:02
Fold 3 Epoch 028; Train loss: 0.1136; Time: 00:00:02
Fold 3 Epoch 029; Train loss: 0.1077; Time: 00:00:02
Fold 3 Epoch 030; Train loss: 0.1057; Time: 00:00:02
Fold 3: Evaluation at Epoch 30
HR@5       NDCG@5     HR@10      NDCG@10   
0.652318   0.382600   0.854305   0.449855  
Loss: 0.1494
Fold 3 Epoch 031; Train loss: 0.0999; Time: 00:00:02
Fold 3 Epoch 032; Train loss: 0.0977; Time: 00:00:02
Fold 3 Epoch 033; Train loss: 0.0921; Time: 00:00:02
Fold 3 Epoch 034; Train loss: 0.0863; Time: 00:00:02
Fold 3 Epoch 035; Train loss: 0.0843; Time: 00:00:02
Fold 3 Epoch 036; Train loss: 0.0786; Time: 00:00:02
Fold 3 Epoch 037; Train loss: 0.0748; Time: 00:00:02
Fold 3 Epoch 038; Train loss: 0.0729; Time: 00:00:02
Fold 3 Epoch 039; Train loss: 0.0696; Time: 00:00:03
Fold 3 Epoch 040; Train loss: 0.0635; Time: 00:00:02
Fold 3: Evaluation at Epoch 40
HR@5       NDCG@5     HR@10      NDCG@10   
0.617550   0.336754   0.855960   0.415231  
Loss: 0.1019
Fold 3 Epoch 041; Train loss: 0.0631; Time: 00:00:02
Fold 3 Epoch 042; Train loss: 0.0594; Time: 00:00:02
Fold 3 Epoch 043; Train loss: 0.0570; Time: 00:00:02
Fold 3 Epoch 044; Train loss: 0.0537; Time: 00:00:02
Fold 3 Epoch 045; Train loss: 0.0499; Time: 00:00:02
Fold 3 Epoch 046; Train loss: 0.0483; Time: 00:00:02
Fold 3 Epoch 047; Train loss: 0.0449; Time: 00:00:02
Fold 3 Epoch 048; Train loss: 0.0427; Time: 00:00:02
Fold 3 Epoch 049; Train loss: 0.0409; Time: 00:00:02
Fold 3 Epoch 050; Train loss: 0.0385; Time: 00:00:02
Fold 3: Evaluation at Epoch 50
HR@5       NDCG@5     HR@10      NDCG@10   
0.609272   0.309722   0.872517   0.397797  
Loss: 0.0635
Fold 3 Epoch 051; Train loss: 0.0364; Time: 00:00:02
Fold 3 Epoch 052; Train loss: 0.0343; Time: 00:00:02
Fold 3 Epoch 053; Train loss: 0.0325; Time: 00:00:02
Fold 3 Epoch 054; Train loss: 0.0310; Time: 00:00:02
Fold 3 Epoch 055; Train loss: 0.0288; Time: 00:00:02
Fold 3 Epoch 056; Train loss: 0.0274; Time: 00:00:02
Fold 3 Epoch 057; Train loss: 0.0256; Time: 00:00:02
Fold 3 Epoch 058; Train loss: 0.0245; Time: 00:00:02
Fold 3 Epoch 059; Train loss: 0.0234; Time: 00:00:02
Fold 3 Epoch 060; Train loss: 0.0219; Time: 00:00:02
Fold 3: Evaluation at Epoch 60
HR@5       NDCG@5     HR@10      NDCG@10   
0.658940   0.359156   0.899007   0.438989  
Loss: 0.0404
Fold 3 Epoch 061; Train loss: 0.0206; Time: 00:00:02
Fold 3 Epoch 062; Train loss: 0.0195; Time: 00:00:02
Fold 3 Epoch 063; Train loss: 0.0187; Time: 00:00:02
Fold 3 Epoch 064; Train loss: 0.0173; Time: 00:00:02
Fold 3 Epoch 065; Train loss: 0.0165; Time: 00:00:02
Fold 3 Epoch 066; Train loss: 0.0157; Time: 00:00:02
Fold 3 Epoch 067; Train loss: 0.0145; Time: 00:00:02
Fold 3 Epoch 068; Train loss: 0.0137; Time: 00:00:02
Fold 3 Epoch 069; Train loss: 0.0130; Time: 00:00:02
Fold 3 Epoch 070; Train loss: 0.0125; Time: 00:00:02
Fold 3: Evaluation at Epoch 70
HR@5       NDCG@5     HR@10      NDCG@10   
0.551325   0.297480   0.899007   0.418170  
Loss: 0.0277
Fold 3 Epoch 071; Train loss: 0.0117; Time: 00:00:02
Fold 3 Epoch 072; Train loss: 0.0109; Time: 00:00:02
Fold 3 Epoch 073; Train loss: 0.0103; Time: 00:00:02
Fold 3 Epoch 074; Train loss: 0.0096; Time: 00:00:02
Fold 3 Epoch 075; Train loss: 0.0092; Time: 00:00:02
Fold 3 Epoch 076; Train loss: 0.0086; Time: 00:00:02
Fold 3 Epoch 077; Train loss: 0.0081; Time: 00:00:02
Fold 3 Epoch 078; Train loss: 0.0077; Time: 00:00:02
Fold 3 Epoch 079; Train loss: 0.0073; Time: 00:00:02
Fold 3 Epoch 080; Train loss: 0.0068; Time: 00:00:02
Fold 3: Evaluation at Epoch 80
HR@5       NDCG@5     HR@10      NDCG@10   
0.521523   0.248078   0.882450   0.373845  
Loss: 0.0198
Fold 3 Epoch 081; Train loss: 0.0065; Time: 00:00:02
Fold 3 Epoch 082; Train loss: 0.0061; Time: 00:00:02
Fold 3 Epoch 083; Train loss: 0.0057; Time: 00:00:02
Fold 3 Epoch 084; Train loss: 0.0054; Time: 00:00:02
Fold 3 Epoch 085; Train loss: 0.0051; Time: 00:00:02
Fold 3 Epoch 086; Train loss: 0.0049; Time: 00:00:02
Fold 3 Epoch 087; Train loss: 0.0046; Time: 00:00:02
Fold 3 Epoch 088; Train loss: 0.0043; Time: 00:00:02
Fold 3 Epoch 089; Train loss: 0.0042; Time: 00:00:02
Fold 3 Epoch 090; Train loss: 0.0039; Time: 00:00:02
Fold 3: Evaluation at Epoch 90
HR@5       NDCG@5     HR@10      NDCG@10   
0.405629   0.201433   0.880795   0.364866  
Loss: 0.0150
Fold 3 Epoch 091; Train loss: 0.0037; Time: 00:00:02
Fold 3 Epoch 092; Train loss: 0.0035; Time: 00:00:02
Fold 3 Epoch 093; Train loss: 0.0033; Time: 00:00:02
Fold 3 Epoch 094; Train loss: 0.0032; Time: 00:00:02
Fold 3 Epoch 095; Train loss: 0.0030; Time: 00:00:02
Fold 3 Epoch 096; Train loss: 0.0029; Time: 00:00:02
Fold 3 Epoch 097; Train loss: 0.0027; Time: 00:00:02
Fold 3 Epoch 098; Train loss: 0.0026; Time: 00:00:02
Fold 3 Epoch 099; Train loss: 0.0025; Time: 00:00:02
Fold 3 Epoch 100; Train loss: 0.0023; Time: 00:00:02
Fold 3: Evaluation at Epoch 100
HR@5       NDCG@5     HR@10      NDCG@10   
0.291391   0.137500   0.882450   0.337303  
Loss: 0.0115
[timesteps candidate 400] Fold 3: HR@10 = [np.float64(0.8145695364238411), np.float64(0.7880794701986755), np.float64(0.8543046357615894), np.float64(0.8559602649006622), np.float64(0.8725165562913907), np.float64(0.8990066225165563), np.float64(0.8990066225165563), np.float64(0.8824503311258278), np.float64(0.8807947019867549), np.float64(0.8824503311258278)]
Tuning timesteps:  60%|██████    | 3/5 [13:22<09:02, 271.05s/it]
========== Fold 3 ==========
INFO:root:Using statics: seq_size = 10, genre_vocab_size = 18
Fold 3 Epoch 001; Train loss: 0.6415; Time: 00:00:02
Fold 3 Epoch 002; Train loss: 0.4079; Time: 00:00:01
Fold 3 Epoch 003; Train loss: 0.3485; Time: 00:00:02
Fold 3 Epoch 004; Train loss: 0.3195; Time: 00:00:02
Fold 3 Epoch 005; Train loss: 0.2942; Time: 00:00:02
Fold 3 Epoch 006; Train loss: 0.2838; Time: 00:00:02
Fold 3 Epoch 007; Train loss: 0.2707; Time: 00:00:02
Fold 3 Epoch 008; Train loss: 0.2534; Time: 00:00:02
Fold 3 Epoch 009; Train loss: 0.2404; Time: 00:00:02
Fold 3 Epoch 010; Train loss: 0.2323; Time: 00:00:02
Fold 3: Evaluation at Epoch 10
HR@5       NDCG@5     HR@10      NDCG@10   
0.645695   0.496286   0.738411   0.526288  
Loss: 0.3117
Fold 3 Epoch 011; Train loss: 0.2241; Time: 00:00:02
Fold 3 Epoch 012; Train loss: 0.2085; Time: 00:00:02
Fold 3 Epoch 013; Train loss: 0.2044; Time: 00:00:02
Fold 3 Epoch 014; Train loss: 0.1969; Time: 00:00:02
Fold 3 Epoch 015; Train loss: 0.1898; Time: 00:00:02
Fold 3 Epoch 016; Train loss: 0.1791; Time: 00:00:02
Fold 3 Epoch 017; Train loss: 0.1710; Time: 00:00:02
Fold 3 Epoch 018; Train loss: 0.1663; Time: 00:00:02
Fold 3 Epoch 019; Train loss: 0.1567; Time: 00:00:02
Fold 3 Epoch 020; Train loss: 0.1493; Time: 00:00:02
Fold 3: Evaluation at Epoch 20
HR@5       NDCG@5     HR@10      NDCG@10   
0.635762   0.528158   0.738411   0.561352  
Loss: 0.2184
Fold 3 Epoch 021; Train loss: 0.1416; Time: 00:00:02
Fold 3 Epoch 022; Train loss: 0.1357; Time: 00:00:02
Fold 3 Epoch 023; Train loss: 0.1303; Time: 00:00:02
Fold 3 Epoch 024; Train loss: 0.1216; Time: 00:00:02
Fold 3 Epoch 025; Train loss: 0.1173; Time: 00:00:02
Fold 3 Epoch 026; Train loss: 0.1111; Time: 00:00:02
Fold 3 Epoch 027; Train loss: 0.1048; Time: 00:00:02
Fold 3 Epoch 028; Train loss: 0.0986; Time: 00:00:02
Fold 3 Epoch 029; Train loss: 0.0931; Time: 00:00:02
Fold 3 Epoch 030; Train loss: 0.0874; Time: 00:00:02
Fold 3: Evaluation at Epoch 30
HR@5       NDCG@5     HR@10      NDCG@10   
0.655629   0.531171   0.796358   0.576703  
Loss: 0.1408
Fold 3 Epoch 031; Train loss: 0.0834; Time: 00:00:02
Fold 3 Epoch 032; Train loss: 0.0778; Time: 00:00:02
Fold 3 Epoch 033; Train loss: 0.0728; Time: 00:00:02
Fold 3 Epoch 034; Train loss: 0.0689; Time: 00:00:02
Fold 3 Epoch 035; Train loss: 0.0648; Time: 00:00:02
Fold 3 Epoch 036; Train loss: 0.0607; Time: 00:00:02
Fold 3 Epoch 037; Train loss: 0.0567; Time: 00:00:02
Fold 3 Epoch 038; Train loss: 0.0538; Time: 00:00:01
Fold 3 Epoch 039; Train loss: 0.0502; Time: 00:00:02
Fold 3 Epoch 040; Train loss: 0.0468; Time: 00:00:02
Fold 3: Evaluation at Epoch 40
HR@5       NDCG@5     HR@10      NDCG@10   
0.604305   0.498728   0.832781   0.572593  
Loss: 0.0923
Fold 3 Epoch 041; Train loss: 0.0435; Time: 00:00:02
Fold 3 Epoch 042; Train loss: 0.0410; Time: 00:00:02
Fold 3 Epoch 043; Train loss: 0.0378; Time: 00:00:02
Fold 3 Epoch 044; Train loss: 0.0355; Time: 00:00:02
Fold 3 Epoch 045; Train loss: 0.0327; Time: 00:00:02
Fold 3 Epoch 046; Train loss: 0.0308; Time: 00:00:02
Fold 3 Epoch 047; Train loss: 0.0289; Time: 00:00:02
Fold 3 Epoch 048; Train loss: 0.0268; Time: 00:00:02
Fold 3 Epoch 049; Train loss: 0.0252; Time: 00:00:02
Fold 3 Epoch 050; Train loss: 0.0236; Time: 00:00:02
Fold 3: Evaluation at Epoch 50
HR@5       NDCG@5     HR@10      NDCG@10   
0.548013   0.428063   0.875828   0.538764  
Loss: 0.0583
Fold 3 Epoch 051; Train loss: 0.0222; Time: 00:00:02
Fold 3 Epoch 052; Train loss: 0.0207; Time: 00:00:02
Fold 3 Epoch 053; Train loss: 0.0194; Time: 00:00:02
Fold 3 Epoch 054; Train loss: 0.0184; Time: 00:00:02
Fold 3 Epoch 055; Train loss: 0.0171; Time: 00:00:02
Fold 3 Epoch 056; Train loss: 0.0163; Time: 00:00:02
Fold 3 Epoch 057; Train loss: 0.0157; Time: 00:00:02
Fold 3 Epoch 058; Train loss: 0.0143; Time: 00:00:02
Fold 3 Epoch 059; Train loss: 0.0135; Time: 00:00:02
Fold 3 Epoch 060; Train loss: 0.0127; Time: 00:00:02
Fold 3: Evaluation at Epoch 60
HR@5       NDCG@5     HR@10      NDCG@10   
0.495033   0.412821   0.879139   0.539769  
Loss: 0.0379
Fold 3 Epoch 061; Train loss: 0.0122; Time: 00:00:01
Fold 3 Epoch 062; Train loss: 0.0115; Time: 00:00:02
Fold 3 Epoch 063; Train loss: 0.0108; Time: 00:00:02
Fold 3 Epoch 064; Train loss: 0.0101; Time: 00:00:02
Fold 3 Epoch 065; Train loss: 0.0096; Time: 00:00:02
Fold 3 Epoch 066; Train loss: 0.0090; Time: 00:00:02
Fold 3 Epoch 067; Train loss: 0.0087; Time: 00:00:02
Fold 3 Epoch 068; Train loss: 0.0082; Time: 00:00:02
Fold 3 Epoch 069; Train loss: 0.0077; Time: 00:00:01
Fold 3 Epoch 070; Train loss: 0.0073; Time: 00:00:02
Fold 3: Evaluation at Epoch 70
HR@5       NDCG@5     HR@10      NDCG@10   
0.430464   0.372104   0.862583   0.507569  
Loss: 0.0256
Fold 3 Epoch 071; Train loss: 0.0069; Time: 00:00:02
Fold 3 Epoch 072; Train loss: 0.0066; Time: 00:00:02
Fold 3 Epoch 073; Train loss: 0.0062; Time: 00:00:02
Fold 3 Epoch 074; Train loss: 0.0059; Time: 00:00:02
Fold 3 Epoch 075; Train loss: 0.0055; Time: 00:00:02
Fold 3 Epoch 076; Train loss: 0.0053; Time: 00:00:02
Fold 3 Epoch 077; Train loss: 0.0051; Time: 00:00:02
Fold 3 Epoch 078; Train loss: 0.0048; Time: 00:00:02
Fold 3 Epoch 079; Train loss: 0.0046; Time: 00:00:02
Fold 3 Epoch 080; Train loss: 0.0043; Time: 00:00:01
Fold 3: Evaluation at Epoch 80
HR@5       NDCG@5     HR@10      NDCG@10   
0.417219   0.275584   0.857616   0.413798  
Loss: 0.0171
Fold 3 Epoch 081; Train loss: 0.0041; Time: 00:00:02
Fold 3 Epoch 082; Train loss: 0.0039; Time: 00:00:03
Fold 3 Epoch 083; Train loss: 0.0037; Time: 00:00:02
Fold 3 Epoch 084; Train loss: 0.0035; Time: 00:00:02
Fold 3 Epoch 085; Train loss: 0.0033; Time: 00:00:02
Fold 3 Epoch 086; Train loss: 0.0032; Time: 00:00:02
Fold 3 Epoch 087; Train loss: 0.0030; Time: 00:00:02
Fold 3 Epoch 088; Train loss: 0.0029; Time: 00:00:02
Fold 3 Epoch 089; Train loss: 0.0027; Time: 00:00:02
Fold 3 Epoch 090; Train loss: 0.0026; Time: 00:00:02
Fold 3: Evaluation at Epoch 90
HR@5       NDCG@5     HR@10      NDCG@10   
0.493377   0.281558   0.850993   0.387781  
Loss: 0.0118
Fold 3 Epoch 091; Train loss: 0.0024; Time: 00:00:02
Fold 3 Epoch 092; Train loss: 0.0023; Time: 00:00:02
Fold 3 Epoch 093; Train loss: 0.0022; Time: 00:00:02
Fold 3 Epoch 094; Train loss: 0.0021; Time: 00:00:02
Fold 3 Epoch 095; Train loss: 0.0020; Time: 00:00:02
Fold 3 Epoch 096; Train loss: 0.0019; Time: 00:00:02
Fold 3 Epoch 097; Train loss: 0.0018; Time: 00:00:02
Fold 3 Epoch 098; Train loss: 0.0018; Time: 00:00:02
Fold 3 Epoch 099; Train loss: 0.0016; Time: 00:00:02
Fold 3 Epoch 100; Train loss: 0.0016; Time: 00:00:02
Fold 3: Evaluation at Epoch 100
HR@5       NDCG@5     HR@10      NDCG@10   
0.491722   0.238755   0.620861   0.278607  
Loss: 0.0083
[timesteps candidate 600] Fold 3: HR@10 = [np.float64(0.7384105960264901), np.float64(0.7384105960264901), np.float64(0.7963576158940397), np.float64(0.8327814569536424), np.float64(0.8758278145695364), np.float64(0.8791390728476821), np.float64(0.8625827814569537), np.float64(0.8576158940397351), np.float64(0.8509933774834437), np.float64(0.6208609271523179)]
Tuning timesteps:  80%|████████  | 4/5 [18:13<04:38, 278.86s/it]
========== Fold 3 ==========
INFO:root:Using statics: seq_size = 10, genre_vocab_size = 18
Fold 3 Epoch 001; Train loss: 0.7999; Time: 00:00:01
Fold 3 Epoch 002; Train loss: 0.5263; Time: 00:00:02
Fold 3 Epoch 003; Train loss: 0.4590; Time: 00:00:02
Fold 3 Epoch 004; Train loss: 0.4270; Time: 00:00:02
Fold 3 Epoch 005; Train loss: 0.3934; Time: 00:00:02
Fold 3 Epoch 006; Train loss: 0.3719; Time: 00:00:02
Fold 3 Epoch 007; Train loss: 0.3600; Time: 00:00:02
Fold 3 Epoch 008; Train loss: 0.3359; Time: 00:00:02
Fold 3 Epoch 009; Train loss: 0.3255; Time: 00:00:02
Fold 3 Epoch 010; Train loss: 0.3087; Time: 00:00:02
Fold 3: Evaluation at Epoch 10
HR@5       NDCG@5     HR@10      NDCG@10   
0.629139   0.493689   0.753311   0.533356  
Loss: 0.3559
Fold 3 Epoch 011; Train loss: 0.2966; Time: 00:00:02
Fold 3 Epoch 012; Train loss: 0.2817; Time: 00:00:02
Fold 3 Epoch 013; Train loss: 0.2642; Time: 00:00:02
Fold 3 Epoch 014; Train loss: 0.2506; Time: 00:00:02
Fold 3 Epoch 015; Train loss: 0.2440; Time: 00:00:02
Fold 3 Epoch 016; Train loss: 0.2326; Time: 00:00:02
Fold 3 Epoch 017; Train loss: 0.2207; Time: 00:00:02
Fold 3 Epoch 018; Train loss: 0.2103; Time: 00:00:02
Fold 3 Epoch 019; Train loss: 0.1996; Time: 00:00:02
Fold 3 Epoch 020; Train loss: 0.1897; Time: 00:00:02
Fold 3: Evaluation at Epoch 20
HR@5       NDCG@5     HR@10      NDCG@10   
0.665563   0.542216   0.788079   0.582062  
Loss: 0.2344
Fold 3 Epoch 021; Train loss: 0.1814; Time: 00:00:02
Fold 3 Epoch 022; Train loss: 0.1716; Time: 00:00:02
Fold 3 Epoch 023; Train loss: 0.1624; Time: 00:00:02
Fold 3 Epoch 024; Train loss: 0.1556; Time: 00:00:02
Fold 3 Epoch 025; Train loss: 0.1465; Time: 00:00:02
Fold 3 Epoch 026; Train loss: 0.1394; Time: 00:00:02
Fold 3 Epoch 027; Train loss: 0.1334; Time: 00:00:02
Fold 3 Epoch 028; Train loss: 0.1249; Time: 00:00:02
Fold 3 Epoch 029; Train loss: 0.1192; Time: 00:00:02
Fold 3 Epoch 030; Train loss: 0.1121; Time: 00:00:02
Fold 3: Evaluation at Epoch 30
HR@5       NDCG@5     HR@10      NDCG@10   
0.637417   0.518109   0.796358   0.569108  
Loss: 0.1462
Fold 3 Epoch 031; Train loss: 0.1068; Time: 00:00:02
Fold 3 Epoch 032; Train loss: 0.0992; Time: 00:00:02
Fold 3 Epoch 033; Train loss: 0.0932; Time: 00:00:02
Fold 3 Epoch 034; Train loss: 0.0873; Time: 00:00:02
Fold 3 Epoch 035; Train loss: 0.0807; Time: 00:00:02
Fold 3 Epoch 036; Train loss: 0.0766; Time: 00:00:02
Fold 3 Epoch 037; Train loss: 0.0721; Time: 00:00:02
Fold 3 Epoch 038; Train loss: 0.0674; Time: 00:00:02
Fold 3 Epoch 039; Train loss: 0.0638; Time: 00:00:02
Fold 3 Epoch 040; Train loss: 0.0596; Time: 00:00:02
Fold 3: Evaluation at Epoch 40
HR@5       NDCG@5     HR@10      NDCG@10   
0.718543   0.565523   0.879139   0.617566  
Loss: 0.0831
Fold 3 Epoch 041; Train loss: 0.0553; Time: 00:00:02
Fold 3 Epoch 042; Train loss: 0.0514; Time: 00:00:02
Fold 3 Epoch 043; Train loss: 0.0478; Time: 00:00:02
Fold 3 Epoch 044; Train loss: 0.0442; Time: 00:00:02
Fold 3 Epoch 045; Train loss: 0.0417; Time: 00:00:02
Fold 3 Epoch 046; Train loss: 0.0384; Time: 00:00:02
Fold 3 Epoch 047; Train loss: 0.0354; Time: 00:00:02
Fold 3 Epoch 048; Train loss: 0.0335; Time: 00:00:02
Fold 3 Epoch 049; Train loss: 0.0310; Time: 00:00:02
Fold 3 Epoch 050; Train loss: 0.0287; Time: 00:00:02
Fold 3: Evaluation at Epoch 50
HR@5       NDCG@5     HR@10      NDCG@10   
0.713576   0.534211   0.894040   0.593348  
Loss: 0.0479
Fold 3 Epoch 051; Train loss: 0.0265; Time: 00:00:02
Fold 3 Epoch 052; Train loss: 0.0248; Time: 00:00:02
Fold 3 Epoch 053; Train loss: 0.0231; Time: 00:00:02
Fold 3 Epoch 054; Train loss: 0.0214; Time: 00:00:02
Fold 3 Epoch 055; Train loss: 0.0198; Time: 00:00:02
Fold 3 Epoch 056; Train loss: 0.0185; Time: 00:00:02
Fold 3 Epoch 057; Train loss: 0.0171; Time: 00:00:02
Fold 3 Epoch 058; Train loss: 0.0160; Time: 00:00:02
Fold 3 Epoch 059; Train loss: 0.0150; Time: 00:00:02
Fold 3 Epoch 060; Train loss: 0.0140; Time: 00:00:02
Fold 3: Evaluation at Epoch 60
HR@5       NDCG@5     HR@10      NDCG@10   
0.670530   0.460044   0.897351   0.535361  
Loss: 0.0289
Fold 3 Epoch 061; Train loss: 0.0131; Time: 00:00:01
Fold 3 Epoch 062; Train loss: 0.0121; Time: 00:00:01
Fold 3 Epoch 063; Train loss: 0.0115; Time: 00:00:02
Fold 3 Epoch 064; Train loss: 0.0106; Time: 00:00:02
Fold 3 Epoch 065; Train loss: 0.0099; Time: 00:00:02
Fold 3 Epoch 066; Train loss: 0.0093; Time: 00:00:02
Fold 3 Epoch 067; Train loss: 0.0087; Time: 00:00:02
Fold 3 Epoch 068; Train loss: 0.0083; Time: 00:00:02
Fold 3 Epoch 069; Train loss: 0.0078; Time: 00:00:02
Fold 3 Epoch 070; Train loss: 0.0074; Time: 00:00:02
Fold 3: Evaluation at Epoch 70
HR@5       NDCG@5     HR@10      NDCG@10   
0.657285   0.441729   0.892384   0.520726  
Loss: 0.0189
Fold 3 Epoch 071; Train loss: 0.0069; Time: 00:00:02
Fold 3 Epoch 072; Train loss: 0.0067; Time: 00:00:02
Fold 3 Epoch 073; Train loss: 0.0061; Time: 00:00:02
Fold 3 Epoch 074; Train loss: 0.0059; Time: 00:00:02
Fold 3 Epoch 075; Train loss: 0.0055; Time: 00:00:02
Fold 3 Epoch 076; Train loss: 0.0052; Time: 00:00:02
Fold 3 Epoch 077; Train loss: 0.0049; Time: 00:00:02
Fold 3 Epoch 078; Train loss: 0.0047; Time: 00:00:02
Fold 3 Epoch 079; Train loss: 0.0044; Time: 00:00:02
Fold 3 Epoch 080; Train loss: 0.0042; Time: 00:00:02
Fold 3: Evaluation at Epoch 80
HR@5       NDCG@5     HR@10      NDCG@10   
0.562914   0.431871   0.864238   0.535028  
Loss: 0.0131
Fold 3 Epoch 081; Train loss: 0.0040; Time: 00:00:02
Fold 3 Epoch 082; Train loss: 0.0038; Time: 00:00:02
Fold 3 Epoch 083; Train loss: 0.0036; Time: 00:00:01
Fold 3 Epoch 084; Train loss: 0.0034; Time: 00:00:02
Fold 3 Epoch 085; Train loss: 0.0033; Time: 00:00:02
Fold 3 Epoch 086; Train loss: 0.0031; Time: 00:00:02
Fold 3 Epoch 087; Train loss: 0.0030; Time: 00:00:02
Fold 3 Epoch 088; Train loss: 0.0028; Time: 00:00:02
Fold 3 Epoch 089; Train loss: 0.0027; Time: 00:00:02
Fold 3 Epoch 090; Train loss: 0.0027; Time: 00:00:02
Fold 3: Evaluation at Epoch 90
HR@5       NDCG@5     HR@10      NDCG@10   
0.647351   0.335066   0.862583   0.406701  
Loss: 0.0097
Fold 3 Epoch 091; Train loss: 0.0025; Time: 00:00:02
Fold 3 Epoch 092; Train loss: 0.0023; Time: 00:00:02
Fold 3 Epoch 093; Train loss: 0.0023; Time: 00:00:02
Fold 3 Epoch 094; Train loss: 0.0021; Time: 00:00:02
Fold 3 Epoch 095; Train loss: 0.0020; Time: 00:00:02
Fold 3 Epoch 096; Train loss: 0.0019; Time: 00:00:02
Fold 3 Epoch 097; Train loss: 0.0018; Time: 00:00:02
Fold 3 Epoch 098; Train loss: 0.0018; Time: 00:00:02
Fold 3 Epoch 099; Train loss: 0.0017; Time: 00:00:02
Fold 3 Epoch 100; Train loss: 0.0016; Time: 00:00:02
Fold 3: Evaluation at Epoch 100
HR@5       NDCG@5     HR@10      NDCG@10   
0.766556   0.365124   0.854305   0.394533  
Loss: 0.0074
[timesteps candidate 800] Fold 3: HR@10 = [np.float64(0.7533112582781457), np.float64(0.7880794701986755), np.float64(0.7963576158940397), np.float64(0.8791390728476821), np.float64(0.8940397350993378), np.float64(0.8973509933774835), np.float64(0.8923841059602649), np.float64(0.8642384105960265), np.float64(0.8625827814569537), np.float64(0.8543046357615894)]
Tuning timesteps: 100%|██████████| 5/5 [23:19<00:00, 288.69s/it]Tuning timesteps: 100%|██████████| 5/5 [23:19<00:00, 279.98s/it]

Best timesteps found: 200
Tuning dropout_rate:   0%|          | 0/7 [00:00<?, ?it/s]
========== Fold 3 ==========
INFO:root:Using statics: seq_size = 10, genre_vocab_size = 18
Fold 3 Epoch 001; Train loss: 0.5521; Time: 00:00:02
Fold 3 Epoch 002; Train loss: 0.2545; Time: 00:00:02
Fold 3 Epoch 003; Train loss: 0.1971; Time: 00:00:02
Fold 3 Epoch 004; Train loss: 0.1700; Time: 00:00:02
Fold 3 Epoch 005; Train loss: 0.1581; Time: 00:00:02
Fold 3 Epoch 006; Train loss: 0.1497; Time: 00:00:02
Fold 3 Epoch 007; Train loss: 0.1417; Time: 00:00:02
Fold 3 Epoch 008; Train loss: 0.1397; Time: 00:00:02
Fold 3 Epoch 009; Train loss: 0.1356; Time: 00:00:02
Fold 3 Epoch 010; Train loss: 0.1326; Time: 00:00:02
Fold 3: Evaluation at Epoch 10
HR@5       NDCG@5     HR@10      NDCG@10   
0.584437   0.481963   0.720199   0.525327  
Loss: 0.1655
Fold 3 Epoch 011; Train loss: 0.1287; Time: 00:00:02
Fold 3 Epoch 012; Train loss: 0.1270; Time: 00:00:02
Fold 3 Epoch 013; Train loss: 0.1263; Time: 00:00:02
Fold 3 Epoch 014; Train loss: 0.1212; Time: 00:00:02
Fold 3 Epoch 015; Train loss: 0.1188; Time: 00:00:02
Fold 3 Epoch 016; Train loss: 0.1149; Time: 00:00:02
Fold 3 Epoch 017; Train loss: 0.1137; Time: 00:00:02
Fold 3 Epoch 018; Train loss: 0.1142; Time: 00:00:02
Fold 3 Epoch 019; Train loss: 0.1112; Time: 00:00:02
Fold 3 Epoch 020; Train loss: 0.1074; Time: 00:00:02
Fold 3: Evaluation at Epoch 20
HR@5       NDCG@5     HR@10      NDCG@10   
0.602649   0.490311   0.733444   0.532427  
Loss: 0.1384
Fold 3 Epoch 021; Train loss: 0.1064; Time: 00:00:02
Fold 3 Epoch 022; Train loss: 0.1045; Time: 00:00:02
Fold 3 Epoch 023; Train loss: 0.1002; Time: 00:00:02
Fold 3 Epoch 024; Train loss: 0.1009; Time: 00:00:02
Fold 3 Epoch 025; Train loss: 0.0977; Time: 00:00:02
Fold 3 Epoch 026; Train loss: 0.0979; Time: 00:00:02
Fold 3 Epoch 027; Train loss: 0.0934; Time: 00:00:02
Fold 3 Epoch 028; Train loss: 0.0918; Time: 00:00:02
Fold 3 Epoch 029; Train loss: 0.0884; Time: 00:00:02
Fold 3 Epoch 030; Train loss: 0.0888; Time: 00:00:02
Fold 3: Evaluation at Epoch 30
HR@5       NDCG@5     HR@10      NDCG@10   
0.569536   0.461756   0.706954   0.505939  
Loss: 0.1171
Fold 3 Epoch 031; Train loss: 0.0870; Time: 00:00:02
Fold 3 Epoch 032; Train loss: 0.0825; Time: 00:00:02
Fold 3 Epoch 033; Train loss: 0.0817; Time: 00:00:02
Fold 3 Epoch 034; Train loss: 0.0791; Time: 00:00:02
Fold 3 Epoch 035; Train loss: 0.0785; Time: 00:00:02
Fold 3 Epoch 036; Train loss: 0.0758; Time: 00:00:02
Fold 3 Epoch 037; Train loss: 0.0736; Time: 00:00:02
Fold 3 Epoch 038; Train loss: 0.0723; Time: 00:00:02
Fold 3 Epoch 039; Train loss: 0.0692; Time: 00:00:02
Fold 3 Epoch 040; Train loss: 0.0691; Time: 00:00:02
Fold 3: Evaluation at Epoch 40
HR@5       NDCG@5     HR@10      NDCG@10   
0.586093   0.466518   0.746689   0.518141  
Loss: 0.0930
Fold 3 Epoch 041; Train loss: 0.0662; Time: 00:00:02
Fold 3 Epoch 042; Train loss: 0.0645; Time: 00:00:02
Fold 3 Epoch 043; Train loss: 0.0627; Time: 00:00:02
Fold 3 Epoch 044; Train loss: 0.0618; Time: 00:00:01
Fold 3 Epoch 045; Train loss: 0.0598; Time: 00:00:02
Fold 3 Epoch 046; Train loss: 0.0569; Time: 00:00:02
Fold 3 Epoch 047; Train loss: 0.0547; Time: 00:00:02
Fold 3 Epoch 048; Train loss: 0.0557; Time: 00:00:02
Fold 3 Epoch 049; Train loss: 0.0528; Time: 00:00:02
Fold 3 Epoch 050; Train loss: 0.0506; Time: 00:00:02
Fold 3: Evaluation at Epoch 50
HR@5       NDCG@5     HR@10      NDCG@10   
0.557947   0.372522   0.720199   0.425009  
Loss: 0.0719
Fold 3 Epoch 051; Train loss: 0.0473; Time: 00:00:02
Fold 3 Epoch 052; Train loss: 0.0456; Time: 00:00:02
Fold 3 Epoch 053; Train loss: 0.0446; Time: 00:00:01
Fold 3 Epoch 054; Train loss: 0.0422; Time: 00:00:01
Fold 3 Epoch 055; Train loss: 0.0420; Time: 00:00:02
Fold 3 Epoch 056; Train loss: 0.0393; Time: 00:00:02
Fold 3 Epoch 057; Train loss: 0.0374; Time: 00:00:02
Fold 3 Epoch 058; Train loss: 0.0353; Time: 00:00:01
Fold 3 Epoch 059; Train loss: 0.0334; Time: 00:00:02
Fold 3 Epoch 060; Train loss: 0.0320; Time: 00:00:02
Fold 3: Evaluation at Epoch 60
HR@5       NDCG@5     HR@10      NDCG@10   
0.498344   0.289273   0.663907   0.342894  
Loss: 0.0473
Fold 3 Epoch 061; Train loss: 0.0307; Time: 00:00:02
Fold 3 Epoch 062; Train loss: 0.0292; Time: 00:00:02
Fold 3 Epoch 063; Train loss: 0.0270; Time: 00:00:02
Fold 3 Epoch 064; Train loss: 0.0259; Time: 00:00:02
Fold 3 Epoch 065; Train loss: 0.0245; Time: 00:00:02
Fold 3 Epoch 066; Train loss: 0.0228; Time: 00:00:02
Fold 3 Epoch 067; Train loss: 0.0213; Time: 00:00:02
Fold 3 Epoch 068; Train loss: 0.0198; Time: 00:00:01
Fold 3 Epoch 069; Train loss: 0.0188; Time: 00:00:02
Fold 3 Epoch 070; Train loss: 0.0173; Time: 00:00:02
Fold 3: Evaluation at Epoch 70
HR@5       NDCG@5     HR@10      NDCG@10   
0.513245   0.280124   0.735099   0.350528  
Loss: 0.0313
Fold 3 Epoch 071; Train loss: 0.0161; Time: 00:00:01
Fold 3 Epoch 072; Train loss: 0.0148; Time: 00:00:02
Fold 3 Epoch 073; Train loss: 0.0136; Time: 00:00:02
Fold 3 Epoch 074; Train loss: 0.0123; Time: 00:00:01
Fold 3 Epoch 075; Train loss: 0.0115; Time: 00:00:02
Fold 3 Epoch 076; Train loss: 0.0102; Time: 00:00:02
Fold 3 Epoch 077; Train loss: 0.0095; Time: 00:00:02
Fold 3 Epoch 078; Train loss: 0.0084; Time: 00:00:02
Fold 3 Epoch 079; Train loss: 0.0078; Time: 00:00:02
Fold 3 Epoch 080; Train loss: 0.0071; Time: 00:00:02
Fold 3: Evaluation at Epoch 80
HR@5       NDCG@5     HR@10      NDCG@10   
0.137417   0.072210   0.781457   0.279954  
Loss: 0.0179
Fold 3 Epoch 081; Train loss: 0.0065; Time: 00:00:02
Fold 3 Epoch 082; Train loss: 0.0061; Time: 00:00:02
Fold 3 Epoch 083; Train loss: 0.0055; Time: 00:00:02
Fold 3 Epoch 084; Train loss: 0.0052; Time: 00:00:02
Fold 3 Epoch 085; Train loss: 0.0048; Time: 00:00:02
Fold 3 Epoch 086; Train loss: 0.0045; Time: 00:00:01
Fold 3 Epoch 087; Train loss: 0.0042; Time: 00:00:02
Fold 3 Epoch 088; Train loss: 0.0039; Time: 00:00:02
Fold 3 Epoch 089; Train loss: 0.0037; Time: 00:00:02
Fold 3 Epoch 090; Train loss: 0.0034; Time: 00:00:02
Fold 3: Evaluation at Epoch 90
HR@5       NDCG@5     HR@10      NDCG@10   
0.201987   0.095583   0.859272   0.309396  
Loss: 0.0125
Fold 3 Epoch 091; Train loss: 0.0033; Time: 00:00:02
Fold 3 Epoch 092; Train loss: 0.0031; Time: 00:00:02
Fold 3 Epoch 093; Train loss: 0.0029; Time: 00:00:02
Fold 3 Epoch 094; Train loss: 0.0027; Time: 00:00:02
Fold 3 Epoch 095; Train loss: 0.0026; Time: 00:00:02
Fold 3 Epoch 096; Train loss: 0.0025; Time: 00:00:02
Fold 3 Epoch 097; Train loss: 0.0023; Time: 00:00:02
Fold 3 Epoch 098; Train loss: 0.0022; Time: 00:00:02
Fold 3 Epoch 099; Train loss: 0.0021; Time: 00:00:01
Fold 3 Epoch 100; Train loss: 0.0020; Time: 00:00:02
Fold 3: Evaluation at Epoch 100
HR@5       NDCG@5     HR@10      NDCG@10   
0.299669   0.136373   0.658940   0.250522  
Loss: 0.0098
[dropout_rate candidate 0.05] Fold 3: HR@10 = [np.float64(0.7201986754966887), np.float64(0.7334437086092715), np.float64(0.706953642384106), np.float64(0.7466887417218543), np.float64(0.7201986754966887), np.float64(0.6639072847682119), np.float64(0.7350993377483444), np.float64(0.7814569536423841), np.float64(0.859271523178808), np.float64(0.6589403973509934)]
Tuning dropout_rate:  14%|█▍        | 1/7 [04:20<26:05, 260.91s/it]
========== Fold 3 ==========
INFO:root:Using statics: seq_size = 10, genre_vocab_size = 18
Fold 3 Epoch 001; Train loss: 0.6030; Time: 00:00:02
Fold 3 Epoch 002; Train loss: 0.2822; Time: 00:00:02
Fold 3 Epoch 003; Train loss: 0.2106; Time: 00:00:02
Fold 3 Epoch 004; Train loss: 0.1818; Time: 00:00:02
Fold 3 Epoch 005; Train loss: 0.1642; Time: 00:00:02
Fold 3 Epoch 006; Train loss: 0.1524; Time: 00:00:02
Fold 3 Epoch 007; Train loss: 0.1482; Time: 00:00:02
Fold 3 Epoch 008; Train loss: 0.1416; Time: 00:00:02
Fold 3 Epoch 009; Train loss: 0.1359; Time: 00:00:02
Fold 3 Epoch 010; Train loss: 0.1353; Time: 00:00:02
Fold 3: Evaluation at Epoch 10
HR@5       NDCG@5     HR@10      NDCG@10   
0.745033   0.548135   0.839404   0.579274  
Loss: 0.1723
Fold 3 Epoch 011; Train loss: 0.1324; Time: 00:00:02
Fold 3 Epoch 012; Train loss: 0.1274; Time: 00:00:02
Fold 3 Epoch 013; Train loss: 0.1264; Time: 00:00:02
Fold 3 Epoch 014; Train loss: 0.1242; Time: 00:00:02
Fold 3 Epoch 015; Train loss: 0.1248; Time: 00:00:02
Fold 3 Epoch 016; Train loss: 0.1217; Time: 00:00:02
Fold 3 Epoch 017; Train loss: 0.1148; Time: 00:00:02
Fold 3 Epoch 018; Train loss: 0.1162; Time: 00:00:02
Fold 3 Epoch 019; Train loss: 0.1103; Time: 00:00:02
Fold 3 Epoch 020; Train loss: 0.1121; Time: 00:00:02
Fold 3: Evaluation at Epoch 20
HR@5       NDCG@5     HR@10      NDCG@10   
0.710265   0.547918   0.849338   0.593894  
Loss: 0.1405
Fold 3 Epoch 021; Train loss: 0.1121; Time: 00:00:02
Fold 3 Epoch 022; Train loss: 0.1073; Time: 00:00:02
Fold 3 Epoch 023; Train loss: 0.1050; Time: 00:00:02
Fold 3 Epoch 024; Train loss: 0.1019; Time: 00:00:02
Fold 3 Epoch 025; Train loss: 0.0993; Time: 00:00:02
Fold 3 Epoch 026; Train loss: 0.0999; Time: 00:00:02
Fold 3 Epoch 027; Train loss: 0.0981; Time: 00:00:02
Fold 3 Epoch 028; Train loss: 0.0945; Time: 00:00:02
Fold 3 Epoch 029; Train loss: 0.0925; Time: 00:00:03
Fold 3 Epoch 030; Train loss: 0.0919; Time: 00:00:02
Fold 3: Evaluation at Epoch 30
HR@5       NDCG@5     HR@10      NDCG@10   
0.586093   0.454650   0.736755   0.502890  
Loss: 0.1238
Fold 3 Epoch 031; Train loss: 0.0904; Time: 00:00:02
Fold 3 Epoch 032; Train loss: 0.0869; Time: 00:00:02
Fold 3 Epoch 033; Train loss: 0.0844; Time: 00:00:02
Fold 3 Epoch 034; Train loss: 0.0824; Time: 00:00:02
Fold 3 Epoch 035; Train loss: 0.0814; Time: 00:00:02
Fold 3 Epoch 036; Train loss: 0.0790; Time: 00:00:02
Fold 3 Epoch 037; Train loss: 0.0770; Time: 00:00:02
Fold 3 Epoch 038; Train loss: 0.0731; Time: 00:00:02
Fold 3 Epoch 039; Train loss: 0.0740; Time: 00:00:02
Fold 3 Epoch 040; Train loss: 0.0707; Time: 00:00:02
Fold 3: Evaluation at Epoch 40
HR@5       NDCG@5     HR@10      NDCG@10   
0.630795   0.470751   0.761589   0.512896  
Loss: 0.0928
Fold 3 Epoch 041; Train loss: 0.0698; Time: 00:00:02
Fold 3 Epoch 042; Train loss: 0.0670; Time: 00:00:02
Fold 3 Epoch 043; Train loss: 0.0642; Time: 00:00:02
Fold 3 Epoch 044; Train loss: 0.0635; Time: 00:00:02
Fold 3 Epoch 045; Train loss: 0.0609; Time: 00:00:02
Fold 3 Epoch 046; Train loss: 0.0600; Time: 00:00:02
Fold 3 Epoch 047; Train loss: 0.0588; Time: 00:00:02
Fold 3 Epoch 048; Train loss: 0.0560; Time: 00:00:02
Fold 3 Epoch 049; Train loss: 0.0543; Time: 00:00:02
Fold 3 Epoch 050; Train loss: 0.0520; Time: 00:00:02
Fold 3: Evaluation at Epoch 50
HR@5       NDCG@5     HR@10      NDCG@10   
0.591060   0.401715   0.736755   0.448332  
Loss: 0.0683
Fold 3 Epoch 051; Train loss: 0.0496; Time: 00:00:02
Fold 3 Epoch 052; Train loss: 0.0485; Time: 00:00:02
Fold 3 Epoch 053; Train loss: 0.0456; Time: 00:00:02
Fold 3 Epoch 054; Train loss: 0.0437; Time: 00:00:02
Fold 3 Epoch 055; Train loss: 0.0419; Time: 00:00:02
Fold 3 Epoch 056; Train loss: 0.0408; Time: 00:00:02
Fold 3 Epoch 057; Train loss: 0.0384; Time: 00:00:02
Fold 3 Epoch 058; Train loss: 0.0369; Time: 00:00:02
Fold 3 Epoch 059; Train loss: 0.0350; Time: 00:00:02
Fold 3 Epoch 060; Train loss: 0.0330; Time: 00:00:02
Fold 3: Evaluation at Epoch 60
HR@5       NDCG@5     HR@10      NDCG@10   
0.602649   0.367720   0.781457   0.425321  
Loss: 0.0470
Fold 3 Epoch 061; Train loss: 0.0322; Time: 00:00:01
Fold 3 Epoch 062; Train loss: 0.0308; Time: 00:00:02
Fold 3 Epoch 063; Train loss: 0.0293; Time: 00:00:02
Fold 3 Epoch 064; Train loss: 0.0276; Time: 00:00:02
Fold 3 Epoch 065; Train loss: 0.0262; Time: 00:00:02
Fold 3 Epoch 066; Train loss: 0.0242; Time: 00:00:01
Fold 3 Epoch 067; Train loss: 0.0226; Time: 00:00:01
Fold 3 Epoch 068; Train loss: 0.0215; Time: 00:00:02
Fold 3 Epoch 069; Train loss: 0.0203; Time: 00:00:02
Fold 3 Epoch 070; Train loss: 0.0191; Time: 00:00:02
Fold 3: Evaluation at Epoch 70
HR@5       NDCG@5     HR@10      NDCG@10   
0.539735   0.285079   0.811258   0.374925  
Loss: 0.0310
Fold 3 Epoch 071; Train loss: 0.0182; Time: 00:00:01
Fold 3 Epoch 072; Train loss: 0.0173; Time: 00:00:02
Fold 3 Epoch 073; Train loss: 0.0156; Time: 00:00:02
Fold 3 Epoch 074; Train loss: 0.0144; Time: 00:00:02
Fold 3 Epoch 075; Train loss: 0.0140; Time: 00:00:02
Fold 3 Epoch 076; Train loss: 0.0126; Time: 00:00:02
Fold 3 Epoch 077; Train loss: 0.0119; Time: 00:00:02
Fold 3 Epoch 078; Train loss: 0.0112; Time: 00:00:02
Fold 3 Epoch 079; Train loss: 0.0102; Time: 00:00:02
Fold 3 Epoch 080; Train loss: 0.0098; Time: 00:00:02
Fold 3: Evaluation at Epoch 80
HR@5       NDCG@5     HR@10      NDCG@10   
0.438742   0.224004   0.899007   0.380149  
Loss: 0.0188
Fold 3 Epoch 081; Train loss: 0.0088; Time: 00:00:02
Fold 3 Epoch 082; Train loss: 0.0080; Time: 00:00:02
Fold 3 Epoch 083; Train loss: 0.0074; Time: 00:00:02
Fold 3 Epoch 084; Train loss: 0.0069; Time: 00:00:02
Fold 3 Epoch 085; Train loss: 0.0062; Time: 00:00:02
Fold 3 Epoch 086; Train loss: 0.0057; Time: 00:00:02
Fold 3 Epoch 087; Train loss: 0.0053; Time: 00:00:02
Fold 3 Epoch 088; Train loss: 0.0048; Time: 00:00:02
Fold 3 Epoch 089; Train loss: 0.0043; Time: 00:00:02
Fold 3 Epoch 090; Train loss: 0.0040; Time: 00:00:02
Fold 3: Evaluation at Epoch 90
HR@5       NDCG@5     HR@10      NDCG@10   
0.374172   0.175905   0.885762   0.350385  
Loss: 0.0115
Fold 3 Epoch 091; Train loss: 0.0036; Time: 00:00:02
Fold 3 Epoch 092; Train loss: 0.0033; Time: 00:00:02
Fold 3 Epoch 093; Train loss: 0.0031; Time: 00:00:02
Fold 3 Epoch 094; Train loss: 0.0028; Time: 00:00:02
Fold 3 Epoch 095; Train loss: 0.0027; Time: 00:00:02
Fold 3 Epoch 096; Train loss: 0.0025; Time: 00:00:02
Fold 3 Epoch 097; Train loss: 0.0023; Time: 00:00:02
Fold 3 Epoch 098; Train loss: 0.0022; Time: 00:00:02
Fold 3 Epoch 099; Train loss: 0.0021; Time: 00:00:02
Fold 3 Epoch 100; Train loss: 0.0020; Time: 00:00:02
Fold 3: Evaluation at Epoch 100
HR@5       NDCG@5     HR@10      NDCG@10   
0.455298   0.211515   0.885762   0.357024  
Loss: 0.0078
[dropout_rate candidate 0.07] Fold 3: HR@10 = [np.float64(0.8394039735099338), np.float64(0.8493377483443708), np.float64(0.7367549668874173), np.float64(0.7615894039735099), np.float64(0.7367549668874173), np.float64(0.7814569536423841), np.float64(0.8112582781456954), np.float64(0.8990066225165563), np.float64(0.8857615894039735), np.float64(0.8857615894039735)]
Tuning dropout_rate:  29%|██▊       | 2/7 [08:38<21:35, 259.05s/it]
========== Fold 3 ==========
INFO:root:Using statics: seq_size = 10, genre_vocab_size = 18
Fold 3 Epoch 001; Train loss: 0.5746; Time: 00:00:02
Fold 3 Epoch 002; Train loss: 0.2711; Time: 00:00:02
Fold 3 Epoch 003; Train loss: 0.2009; Time: 00:00:02
Fold 3 Epoch 004; Train loss: 0.1699; Time: 00:00:02
Fold 3 Epoch 005; Train loss: 0.1579; Time: 00:00:02
Fold 3 Epoch 006; Train loss: 0.1498; Time: 00:00:02
Fold 3 Epoch 007; Train loss: 0.1437; Time: 00:00:02
Fold 3 Epoch 008; Train loss: 0.1388; Time: 00:00:02
Fold 3 Epoch 009; Train loss: 0.1328; Time: 00:00:02
Fold 3 Epoch 010; Train loss: 0.1321; Time: 00:00:02
Fold 3: Evaluation at Epoch 10
HR@5       NDCG@5     HR@10      NDCG@10   
0.647351   0.518977   0.786424   0.564154  
Loss: 0.1727
Fold 3 Epoch 011; Train loss: 0.1274; Time: 00:00:02
Fold 3 Epoch 012; Train loss: 0.1232; Time: 00:00:02
Fold 3 Epoch 013; Train loss: 0.1215; Time: 00:00:02
Fold 3 Epoch 014; Train loss: 0.1176; Time: 00:00:02
Fold 3 Epoch 015; Train loss: 0.1171; Time: 00:00:02
Fold 3 Epoch 016; Train loss: 0.1151; Time: 00:00:02
Fold 3 Epoch 017; Train loss: 0.1137; Time: 00:00:02
Fold 3 Epoch 018; Train loss: 0.1091; Time: 00:00:02
Fold 3 Epoch 019; Train loss: 0.1075; Time: 00:00:02
Fold 3 Epoch 020; Train loss: 0.1097; Time: 00:00:02
Fold 3: Evaluation at Epoch 20
HR@5       NDCG@5     HR@10      NDCG@10   
0.609272   0.488339   0.718543   0.523898  
Loss: 0.1430
Fold 3 Epoch 021; Train loss: 0.1048; Time: 00:00:01
Fold 3 Epoch 022; Train loss: 0.0989; Time: 00:00:01
Fold 3 Epoch 023; Train loss: 0.1004; Time: 00:00:02
Fold 3 Epoch 024; Train loss: 0.0964; Time: 00:00:01
Fold 3 Epoch 025; Train loss: 0.0952; Time: 00:00:02
Fold 3 Epoch 026; Train loss: 0.0938; Time: 00:00:02
Fold 3 Epoch 027; Train loss: 0.0893; Time: 00:00:02
Fold 3 Epoch 028; Train loss: 0.0893; Time: 00:00:02
Fold 3 Epoch 029; Train loss: 0.0871; Time: 00:00:02
Fold 3 Epoch 030; Train loss: 0.0863; Time: 00:00:02
Fold 3: Evaluation at Epoch 30
HR@5       NDCG@5     HR@10      NDCG@10   
0.607616   0.463374   0.743377   0.506966  
Loss: 0.1124
Fold 3 Epoch 031; Train loss: 0.0845; Time: 00:00:02
Fold 3 Epoch 032; Train loss: 0.0818; Time: 00:00:02
Fold 3 Epoch 033; Train loss: 0.0806; Time: 00:00:02
Fold 3 Epoch 034; Train loss: 0.0795; Time: 00:00:02
Fold 3 Epoch 035; Train loss: 0.0762; Time: 00:00:02
Fold 3 Epoch 036; Train loss: 0.0761; Time: 00:00:02
Fold 3 Epoch 037; Train loss: 0.0734; Time: 00:00:02
Fold 3 Epoch 038; Train loss: 0.0700; Time: 00:00:02
Fold 3 Epoch 039; Train loss: 0.0673; Time: 00:00:02
Fold 3 Epoch 040; Train loss: 0.0655; Time: 00:00:02
Fold 3: Evaluation at Epoch 40
HR@5       NDCG@5     HR@10      NDCG@10   
0.529801   0.419489   0.665563   0.463463  
Loss: 0.0900
Fold 3 Epoch 041; Train loss: 0.0625; Time: 00:00:02
Fold 3 Epoch 042; Train loss: 0.0628; Time: 00:00:01
Fold 3 Epoch 043; Train loss: 0.0594; Time: 00:00:01
Fold 3 Epoch 044; Train loss: 0.0594; Time: 00:00:02
Fold 3 Epoch 045; Train loss: 0.0561; Time: 00:00:02
Fold 3 Epoch 046; Train loss: 0.0556; Time: 00:00:01
Fold 3 Epoch 047; Train loss: 0.0529; Time: 00:00:02
Fold 3 Epoch 048; Train loss: 0.0507; Time: 00:00:02
Fold 3 Epoch 049; Train loss: 0.0489; Time: 00:00:02
Fold 3 Epoch 050; Train loss: 0.0474; Time: 00:00:02
Fold 3: Evaluation at Epoch 50
HR@5       NDCG@5     HR@10      NDCG@10   
0.539735   0.380671   0.710265   0.434695  
Loss: 0.0666
Fold 3 Epoch 051; Train loss: 0.0460; Time: 00:00:02
Fold 3 Epoch 052; Train loss: 0.0439; Time: 00:00:02
Fold 3 Epoch 053; Train loss: 0.0425; Time: 00:00:02
Fold 3 Epoch 054; Train loss: 0.0410; Time: 00:00:02
Fold 3 Epoch 055; Train loss: 0.0380; Time: 00:00:02
Fold 3 Epoch 056; Train loss: 0.0369; Time: 00:00:02
Fold 3 Epoch 057; Train loss: 0.0361; Time: 00:00:02
Fold 3 Epoch 058; Train loss: 0.0341; Time: 00:00:02
Fold 3 Epoch 059; Train loss: 0.0332; Time: 00:00:02
Fold 3 Epoch 060; Train loss: 0.0316; Time: 00:00:02
Fold 3: Evaluation at Epoch 60
HR@5       NDCG@5     HR@10      NDCG@10   
0.461921   0.265403   0.605960   0.311409  
Loss: 0.0477
Fold 3 Epoch 061; Train loss: 0.0299; Time: 00:00:02
Fold 3 Epoch 062; Train loss: 0.0284; Time: 00:00:02
Fold 3 Epoch 063; Train loss: 0.0273; Time: 00:00:02
Fold 3 Epoch 064; Train loss: 0.0264; Time: 00:00:02
Fold 3 Epoch 065; Train loss: 0.0243; Time: 00:00:02
Fold 3 Epoch 066; Train loss: 0.0232; Time: 00:00:02
Fold 3 Epoch 067; Train loss: 0.0222; Time: 00:00:02
Fold 3 Epoch 068; Train loss: 0.0210; Time: 00:00:02
Fold 3 Epoch 069; Train loss: 0.0197; Time: 00:00:02
Fold 3 Epoch 070; Train loss: 0.0187; Time: 00:00:02
Fold 3: Evaluation at Epoch 70
HR@5       NDCG@5     HR@10      NDCG@10   
0.415563   0.239764   0.663907   0.321395  
Loss: 0.0302
Fold 3 Epoch 071; Train loss: 0.0174; Time: 00:00:02
Fold 3 Epoch 072; Train loss: 0.0165; Time: 00:00:02
Fold 3 Epoch 073; Train loss: 0.0151; Time: 00:00:02
Fold 3 Epoch 074; Train loss: 0.0143; Time: 00:00:02
Fold 3 Epoch 075; Train loss: 0.0133; Time: 00:00:02
Fold 3 Epoch 076; Train loss: 0.0126; Time: 00:00:02
Fold 3 Epoch 077; Train loss: 0.0117; Time: 00:00:02
Fold 3 Epoch 078; Train loss: 0.0109; Time: 00:00:02
Fold 3 Epoch 079; Train loss: 0.0102; Time: 00:00:02
Fold 3 Epoch 080; Train loss: 0.0095; Time: 00:00:02
Fold 3: Evaluation at Epoch 80
HR@5       NDCG@5     HR@10      NDCG@10   
0.579470   0.268017   0.846026   0.357168  
Loss: 0.0205
Fold 3 Epoch 081; Train loss: 0.0088; Time: 00:00:02
Fold 3 Epoch 082; Train loss: 0.0084; Time: 00:00:02
Fold 3 Epoch 083; Train loss: 0.0075; Time: 00:00:01
Fold 3 Epoch 084; Train loss: 0.0069; Time: 00:00:02
Fold 3 Epoch 085; Train loss: 0.0063; Time: 00:00:02
Fold 3 Epoch 086; Train loss: 0.0059; Time: 00:00:02
Fold 3 Epoch 087; Train loss: 0.0054; Time: 00:00:02
Fold 3 Epoch 088; Train loss: 0.0050; Time: 00:00:02
Fold 3 Epoch 089; Train loss: 0.0046; Time: 00:00:02
Fold 3 Epoch 090; Train loss: 0.0043; Time: 00:00:02
Fold 3: Evaluation at Epoch 90
HR@5       NDCG@5     HR@10      NDCG@10   
0.602649   0.307784   0.880795   0.403820  
Loss: 0.0137
Fold 3 Epoch 091; Train loss: 0.0038; Time: 00:00:02
Fold 3 Epoch 092; Train loss: 0.0036; Time: 00:00:02
Fold 3 Epoch 093; Train loss: 0.0033; Time: 00:00:02
Fold 3 Epoch 094; Train loss: 0.0032; Time: 00:00:02
Fold 3 Epoch 095; Train loss: 0.0028; Time: 00:00:02
Fold 3 Epoch 096; Train loss: 0.0026; Time: 00:00:02
Fold 3 Epoch 097; Train loss: 0.0025; Time: 00:00:02
Fold 3 Epoch 098; Train loss: 0.0023; Time: 00:00:02
Fold 3 Epoch 099; Train loss: 0.0021; Time: 00:00:02
Fold 3 Epoch 100; Train loss: 0.0020; Time: 00:00:02
Fold 3: Evaluation at Epoch 100
HR@5       NDCG@5     HR@10      NDCG@10   
0.514901   0.263812   0.895695   0.384040  
Loss: 0.0097
[dropout_rate candidate 0.1] Fold 3: HR@10 = [np.float64(0.7864238410596026), np.float64(0.7185430463576159), np.float64(0.7433774834437086), np.float64(0.6655629139072847), np.float64(0.7102649006622517), np.float64(0.6059602649006622), np.float64(0.6639072847682119), np.float64(0.8460264900662252), np.float64(0.8807947019867549), np.float64(0.8956953642384106)]
Tuning dropout_rate:  43%|████▎     | 3/7 [12:55<17:12, 258.10s/it]
========== Fold 3 ==========
INFO:root:Using statics: seq_size = 10, genre_vocab_size = 18
Fold 3 Epoch 001; Train loss: 0.6100; Time: 00:00:02
Fold 3 Epoch 002; Train loss: 0.2819; Time: 00:00:02
Fold 3 Epoch 003; Train loss: 0.2139; Time: 00:00:02
Fold 3 Epoch 004; Train loss: 0.1818; Time: 00:00:01
Fold 3 Epoch 005; Train loss: 0.1658; Time: 00:00:02
Fold 3 Epoch 006; Train loss: 0.1550; Time: 00:00:02
Fold 3 Epoch 007; Train loss: 0.1515; Time: 00:00:02
Fold 3 Epoch 008; Train loss: 0.1424; Time: 00:00:02
Fold 3 Epoch 009; Train loss: 0.1399; Time: 00:00:02
Fold 3 Epoch 010; Train loss: 0.1367; Time: 00:00:02
Fold 3: Evaluation at Epoch 10
HR@5       NDCG@5     HR@10      NDCG@10   
0.509934   0.410222   0.668874   0.460655  
Loss: 0.1878
Fold 3 Epoch 011; Train loss: 0.1330; Time: 00:00:02
Fold 3 Epoch 012; Train loss: 0.1321; Time: 00:00:02
Fold 3 Epoch 013; Train loss: 0.1286; Time: 00:00:02
Fold 3 Epoch 014; Train loss: 0.1259; Time: 00:00:02
Fold 3 Epoch 015; Train loss: 0.1251; Time: 00:00:02
Fold 3 Epoch 016; Train loss: 0.1205; Time: 00:00:02
Fold 3 Epoch 017; Train loss: 0.1199; Time: 00:00:02
Fold 3 Epoch 018; Train loss: 0.1157; Time: 00:00:02
Fold 3 Epoch 019; Train loss: 0.1138; Time: 00:00:02
Fold 3 Epoch 020; Train loss: 0.1135; Time: 00:00:02
Fold 3: Evaluation at Epoch 20
HR@5       NDCG@5     HR@10      NDCG@10   
0.632450   0.466550   0.814570   0.525207  
Loss: 0.1379
Fold 3 Epoch 021; Train loss: 0.1088; Time: 00:00:02
Fold 3 Epoch 022; Train loss: 0.1066; Time: 00:00:02
Fold 3 Epoch 023; Train loss: 0.1060; Time: 00:00:02
Fold 3 Epoch 024; Train loss: 0.1071; Time: 00:00:02
Fold 3 Epoch 025; Train loss: 0.1025; Time: 00:00:02
Fold 3 Epoch 026; Train loss: 0.1002; Time: 00:00:02
Fold 3 Epoch 027; Train loss: 0.0969; Time: 00:00:02
Fold 3 Epoch 028; Train loss: 0.0940; Time: 00:00:02
Fold 3 Epoch 029; Train loss: 0.0923; Time: 00:00:02
Fold 3 Epoch 030; Train loss: 0.0889; Time: 00:00:02
Fold 3: Evaluation at Epoch 30
HR@5       NDCG@5     HR@10      NDCG@10   
0.528146   0.403615   0.675497   0.450384  
Loss: 0.1161
Fold 3 Epoch 031; Train loss: 0.0855; Time: 00:00:02
Fold 3 Epoch 032; Train loss: 0.0852; Time: 00:00:02
Fold 3 Epoch 033; Train loss: 0.0834; Time: 00:00:02
Fold 3 Epoch 034; Train loss: 0.0808; Time: 00:00:02
Fold 3 Epoch 035; Train loss: 0.0789; Time: 00:00:02
Fold 3 Epoch 036; Train loss: 0.0764; Time: 00:00:02
Fold 3 Epoch 037; Train loss: 0.0740; Time: 00:00:02
Fold 3 Epoch 038; Train loss: 0.0739; Time: 00:00:02
Fold 3 Epoch 039; Train loss: 0.0709; Time: 00:00:02
Fold 3 Epoch 040; Train loss: 0.0689; Time: 00:00:02
Fold 3: Evaluation at Epoch 40
HR@5       NDCG@5     HR@10      NDCG@10   
0.615894   0.476541   0.791391   0.533530  
Loss: 0.0901
Fold 3 Epoch 041; Train loss: 0.0661; Time: 00:00:02
Fold 3 Epoch 042; Train loss: 0.0653; Time: 00:00:02
Fold 3 Epoch 043; Train loss: 0.0631; Time: 00:00:02
Fold 3 Epoch 044; Train loss: 0.0597; Time: 00:00:02
Fold 3 Epoch 045; Train loss: 0.0589; Time: 00:00:02
Fold 3 Epoch 046; Train loss: 0.0564; Time: 00:00:02
Fold 3 Epoch 047; Train loss: 0.0554; Time: 00:00:02
Fold 3 Epoch 048; Train loss: 0.0543; Time: 00:00:02
Fold 3 Epoch 049; Train loss: 0.0503; Time: 00:00:01
Fold 3 Epoch 050; Train loss: 0.0495; Time: 00:00:02
Fold 3: Evaluation at Epoch 50
HR@5       NDCG@5     HR@10      NDCG@10   
0.470199   0.341831   0.673841   0.407871  
Loss: 0.0673
Fold 3 Epoch 051; Train loss: 0.0478; Time: 00:00:02
Fold 3 Epoch 052; Train loss: 0.0461; Time: 00:00:02
Fold 3 Epoch 053; Train loss: 0.0437; Time: 00:00:02
Fold 3 Epoch 054; Train loss: 0.0422; Time: 00:00:02
Fold 3 Epoch 055; Train loss: 0.0413; Time: 00:00:02
Fold 3 Epoch 056; Train loss: 0.0387; Time: 00:00:02
Fold 3 Epoch 057; Train loss: 0.0374; Time: 00:00:02
Fold 3 Epoch 058; Train loss: 0.0359; Time: 00:00:02
Fold 3 Epoch 059; Train loss: 0.0344; Time: 00:00:02
Fold 3 Epoch 060; Train loss: 0.0328; Time: 00:00:02
Fold 3: Evaluation at Epoch 60
HR@5       NDCG@5     HR@10      NDCG@10   
0.579470   0.406947   0.812914   0.484020  
Loss: 0.0464
Fold 3 Epoch 061; Train loss: 0.0314; Time: 00:00:02
Fold 3 Epoch 062; Train loss: 0.0301; Time: 00:00:02
Fold 3 Epoch 063; Train loss: 0.0287; Time: 00:00:02
Fold 3 Epoch 064; Train loss: 0.0270; Time: 00:00:02
Fold 3 Epoch 065; Train loss: 0.0261; Time: 00:00:02
Fold 3 Epoch 066; Train loss: 0.0245; Time: 00:00:02
Fold 3 Epoch 067; Train loss: 0.0236; Time: 00:00:02
Fold 3 Epoch 068; Train loss: 0.0224; Time: 00:00:01
Fold 3 Epoch 069; Train loss: 0.0211; Time: 00:00:02
Fold 3 Epoch 070; Train loss: 0.0203; Time: 00:00:02
Fold 3: Evaluation at Epoch 70
HR@5       NDCG@5     HR@10      NDCG@10   
0.566225   0.369701   0.836093   0.457792  
Loss: 0.0321
Fold 3 Epoch 071; Train loss: 0.0189; Time: 00:00:02
Fold 3 Epoch 072; Train loss: 0.0180; Time: 00:00:02
Fold 3 Epoch 073; Train loss: 0.0170; Time: 00:00:02
Fold 3 Epoch 074; Train loss: 0.0160; Time: 00:00:02
Fold 3 Epoch 075; Train loss: 0.0151; Time: 00:00:02
Fold 3 Epoch 076; Train loss: 0.0139; Time: 00:00:02
Fold 3 Epoch 077; Train loss: 0.0134; Time: 00:00:02
Fold 3 Epoch 078; Train loss: 0.0126; Time: 00:00:02
Fold 3 Epoch 079; Train loss: 0.0119; Time: 00:00:02
Fold 3 Epoch 080; Train loss: 0.0111; Time: 00:00:02
Fold 3: Evaluation at Epoch 80
HR@5       NDCG@5     HR@10      NDCG@10   
0.461921   0.350881   0.912252   0.498560  
Loss: 0.0215
Fold 3 Epoch 081; Train loss: 0.0102; Time: 00:00:02
Fold 3 Epoch 082; Train loss: 0.0098; Time: 00:00:02
Fold 3 Epoch 083; Train loss: 0.0094; Time: 00:00:02
Fold 3 Epoch 084; Train loss: 0.0085; Time: 00:00:02
Fold 3 Epoch 085; Train loss: 0.0082; Time: 00:00:02
Fold 3 Epoch 086; Train loss: 0.0075; Time: 00:00:02
Fold 3 Epoch 087; Train loss: 0.0072; Time: 00:00:02
Fold 3 Epoch 088; Train loss: 0.0065; Time: 00:00:02
Fold 3 Epoch 089; Train loss: 0.0059; Time: 00:00:02
Fold 3 Epoch 090; Train loss: 0.0055; Time: 00:00:02
Fold 3: Evaluation at Epoch 90
HR@5       NDCG@5     HR@10      NDCG@10   
0.390728   0.202634   0.903974   0.364598  
Loss: 0.0133
Fold 3 Epoch 091; Train loss: 0.0050; Time: 00:00:02
Fold 3 Epoch 092; Train loss: 0.0047; Time: 00:00:02
Fold 3 Epoch 093; Train loss: 0.0043; Time: 00:00:02
Fold 3 Epoch 094; Train loss: 0.0039; Time: 00:00:02
Fold 3 Epoch 095; Train loss: 0.0037; Time: 00:00:02
Fold 3 Epoch 096; Train loss: 0.0033; Time: 00:00:02
Fold 3 Epoch 097; Train loss: 0.0032; Time: 00:00:02
Fold 3 Epoch 098; Train loss: 0.0029; Time: 00:00:02
Fold 3 Epoch 099; Train loss: 0.0027; Time: 00:00:02
Fold 3 Epoch 100; Train loss: 0.0025; Time: 00:00:02
Fold 3: Evaluation at Epoch 100
HR@5       NDCG@5     HR@10      NDCG@10   
0.403974   0.185682   0.899007   0.341406  
Loss: 0.0085
[dropout_rate candidate 0.2] Fold 3: HR@10 = [np.float64(0.6688741721854304), np.float64(0.8145695364238411), np.float64(0.6754966887417219), np.float64(0.7913907284768212), np.float64(0.6738410596026491), np.float64(0.8129139072847682), np.float64(0.8360927152317881), np.float64(0.9122516556291391), np.float64(0.9039735099337748), np.float64(0.8990066225165563)]
Tuning dropout_rate:  57%|█████▋    | 4/7 [17:14<12:55, 258.43s/it]
========== Fold 3 ==========
INFO:root:Using statics: seq_size = 10, genre_vocab_size = 18
Fold 3 Epoch 001; Train loss: 0.5838; Time: 00:00:02
Fold 3 Epoch 002; Train loss: 0.2782; Time: 00:00:02
Fold 3 Epoch 003; Train loss: 0.2025; Time: 00:00:02
Fold 3 Epoch 004; Train loss: 0.1752; Time: 00:00:02
Fold 3 Epoch 005; Train loss: 0.1635; Time: 00:00:02
Fold 3 Epoch 006; Train loss: 0.1520; Time: 00:00:02
Fold 3 Epoch 007; Train loss: 0.1468; Time: 00:00:02
Fold 3 Epoch 008; Train loss: 0.1403; Time: 00:00:02
Fold 3 Epoch 009; Train loss: 0.1326; Time: 00:00:02
Fold 3 Epoch 010; Train loss: 0.1328; Time: 00:00:02
Fold 3: Evaluation at Epoch 10
HR@5       NDCG@5     HR@10      NDCG@10   
0.509934   0.427831   0.640728   0.469831  
Loss: 0.1816
Fold 3 Epoch 011; Train loss: 0.1298; Time: 00:00:02
Fold 3 Epoch 012; Train loss: 0.1288; Time: 00:00:02
Fold 3 Epoch 013; Train loss: 0.1236; Time: 00:00:02
Fold 3 Epoch 014; Train loss: 0.1224; Time: 00:00:02
Fold 3 Epoch 015; Train loss: 0.1204; Time: 00:00:02
Fold 3 Epoch 016; Train loss: 0.1171; Time: 00:00:02
Fold 3 Epoch 017; Train loss: 0.1162; Time: 00:00:02
Fold 3 Epoch 018; Train loss: 0.1135; Time: 00:00:02
Fold 3 Epoch 019; Train loss: 0.1129; Time: 00:00:02
Fold 3 Epoch 020; Train loss: 0.1090; Time: 00:00:02
Fold 3: Evaluation at Epoch 20
HR@5       NDCG@5     HR@10      NDCG@10   
0.521523   0.449097   0.678808   0.499299  
Loss: 0.1472
Fold 3 Epoch 021; Train loss: 0.1080; Time: 00:00:02
Fold 3 Epoch 022; Train loss: 0.1066; Time: 00:00:02
Fold 3 Epoch 023; Train loss: 0.1034; Time: 00:00:02
Fold 3 Epoch 024; Train loss: 0.0995; Time: 00:00:02
Fold 3 Epoch 025; Train loss: 0.1001; Time: 00:00:02
Fold 3 Epoch 026; Train loss: 0.0967; Time: 00:00:02
Fold 3 Epoch 027; Train loss: 0.0956; Time: 00:00:02
Fold 3 Epoch 028; Train loss: 0.0935; Time: 00:00:02
Fold 3 Epoch 029; Train loss: 0.0917; Time: 00:00:02
Fold 3 Epoch 030; Train loss: 0.0904; Time: 00:00:02
Fold 3: Evaluation at Epoch 30
HR@5       NDCG@5     HR@10      NDCG@10   
0.597682   0.499755   0.741722   0.545781  
Loss: 0.1242
Fold 3 Epoch 031; Train loss: 0.0881; Time: 00:00:02
Fold 3 Epoch 032; Train loss: 0.0852; Time: 00:00:02
Fold 3 Epoch 033; Train loss: 0.0812; Time: 00:00:02
Fold 3 Epoch 034; Train loss: 0.0806; Time: 00:00:02
Fold 3 Epoch 035; Train loss: 0.0783; Time: 00:00:02
Fold 3 Epoch 036; Train loss: 0.0769; Time: 00:00:02
Fold 3 Epoch 037; Train loss: 0.0747; Time: 00:00:02
Fold 3 Epoch 038; Train loss: 0.0717; Time: 00:00:02
Fold 3 Epoch 039; Train loss: 0.0717; Time: 00:00:02
Fold 3 Epoch 040; Train loss: 0.0687; Time: 00:00:01
Fold 3: Evaluation at Epoch 40
HR@5       NDCG@5     HR@10      NDCG@10   
0.589404   0.487397   0.725166   0.530815  
Loss: 0.0962
Fold 3 Epoch 041; Train loss: 0.0678; Time: 00:00:02
Fold 3 Epoch 042; Train loss: 0.0639; Time: 00:00:02
Fold 3 Epoch 043; Train loss: 0.0625; Time: 00:00:02
Fold 3 Epoch 044; Train loss: 0.0624; Time: 00:00:02
Fold 3 Epoch 045; Train loss: 0.0599; Time: 00:00:02
Fold 3 Epoch 046; Train loss: 0.0557; Time: 00:00:01
Fold 3 Epoch 047; Train loss: 0.0555; Time: 00:00:01
Fold 3 Epoch 048; Train loss: 0.0525; Time: 00:00:02
Fold 3 Epoch 049; Train loss: 0.0513; Time: 00:00:02
Fold 3 Epoch 050; Train loss: 0.0493; Time: 00:00:02
Fold 3: Evaluation at Epoch 50
HR@5       NDCG@5     HR@10      NDCG@10   
0.629139   0.509050   0.751656   0.548389  
Loss: 0.0729
Fold 3 Epoch 051; Train loss: 0.0469; Time: 00:00:02
Fold 3 Epoch 052; Train loss: 0.0452; Time: 00:00:02
Fold 3 Epoch 053; Train loss: 0.0439; Time: 00:00:02
Fold 3 Epoch 054; Train loss: 0.0415; Time: 00:00:02
Fold 3 Epoch 055; Train loss: 0.0398; Time: 00:00:02
Fold 3 Epoch 056; Train loss: 0.0384; Time: 00:00:02
Fold 3 Epoch 057; Train loss: 0.0374; Time: 00:00:02
Fold 3 Epoch 058; Train loss: 0.0352; Time: 00:00:02
Fold 3 Epoch 059; Train loss: 0.0343; Time: 00:00:02
Fold 3 Epoch 060; Train loss: 0.0322; Time: 00:00:02
Fold 3: Evaluation at Epoch 60
HR@5       NDCG@5     HR@10      NDCG@10   
0.652318   0.476093   0.798013   0.523116  
Loss: 0.0511
Fold 3 Epoch 061; Train loss: 0.0297; Time: 00:00:03
Fold 3 Epoch 062; Train loss: 0.0294; Time: 00:00:02
Fold 3 Epoch 063; Train loss: 0.0273; Time: 00:00:02
Fold 3 Epoch 064; Train loss: 0.0260; Time: 00:00:02
Fold 3 Epoch 065; Train loss: 0.0245; Time: 00:00:02
Fold 3 Epoch 066; Train loss: 0.0226; Time: 00:00:02
Fold 3 Epoch 067; Train loss: 0.0222; Time: 00:00:02
Fold 3 Epoch 068; Train loss: 0.0200; Time: 00:00:02
Fold 3 Epoch 069; Train loss: 0.0188; Time: 00:00:02
Fold 3 Epoch 070; Train loss: 0.0176; Time: 00:00:02
Fold 3: Evaluation at Epoch 70
HR@5       NDCG@5     HR@10      NDCG@10   
0.710265   0.481413   0.895695   0.542096  
Loss: 0.0324
Fold 3 Epoch 071; Train loss: 0.0161; Time: 00:00:02
Fold 3 Epoch 072; Train loss: 0.0151; Time: 00:00:02
Fold 3 Epoch 073; Train loss: 0.0139; Time: 00:00:02
Fold 3 Epoch 074; Train loss: 0.0129; Time: 00:00:02
Fold 3 Epoch 075; Train loss: 0.0116; Time: 00:00:02
Fold 3 Epoch 076; Train loss: 0.0108; Time: 00:00:02
Fold 3 Epoch 077; Train loss: 0.0096; Time: 00:00:02
Fold 3 Epoch 078; Train loss: 0.0087; Time: 00:00:02
Fold 3 Epoch 079; Train loss: 0.0078; Time: 00:00:02
Fold 3 Epoch 080; Train loss: 0.0071; Time: 00:00:02
Fold 3: Evaluation at Epoch 80
HR@5       NDCG@5     HR@10      NDCG@10   
0.566225   0.370394   0.880795   0.478399  
Loss: 0.0179
Fold 3 Epoch 081; Train loss: 0.0064; Time: 00:00:01
Fold 3 Epoch 082; Train loss: 0.0058; Time: 00:00:01
Fold 3 Epoch 083; Train loss: 0.0053; Time: 00:00:02
Fold 3 Epoch 084; Train loss: 0.0048; Time: 00:00:02
Fold 3 Epoch 085; Train loss: 0.0044; Time: 00:00:02
Fold 3 Epoch 086; Train loss: 0.0040; Time: 00:00:02
Fold 3 Epoch 087; Train loss: 0.0037; Time: 00:00:02
Fold 3 Epoch 088; Train loss: 0.0035; Time: 00:00:02
Fold 3 Epoch 089; Train loss: 0.0032; Time: 00:00:02
Fold 3 Epoch 090; Train loss: 0.0030; Time: 00:00:03
Fold 3: Evaluation at Epoch 90
HR@5       NDCG@5     HR@10      NDCG@10   
0.761589   0.442114   0.875828   0.480008  
Loss: 0.0105
Fold 3 Epoch 091; Train loss: 0.0028; Time: 00:00:01
Fold 3 Epoch 092; Train loss: 0.0027; Time: 00:00:01
Fold 3 Epoch 093; Train loss: 0.0025; Time: 00:00:02
Fold 3 Epoch 094; Train loss: 0.0024; Time: 00:00:02
Fold 3 Epoch 095; Train loss: 0.0022; Time: 00:00:02
Fold 3 Epoch 096; Train loss: 0.0021; Time: 00:00:02
Fold 3 Epoch 097; Train loss: 0.0020; Time: 00:00:02
Fold 3 Epoch 098; Train loss: 0.0019; Time: 00:00:02
Fold 3 Epoch 099; Train loss: 0.0018; Time: 00:00:02
Fold 3 Epoch 100; Train loss: 0.0018; Time: 00:00:02
Fold 3: Evaluation at Epoch 100
HR@5       NDCG@5     HR@10      NDCG@10   
0.317881   0.155696   0.870861   0.334137  
Loss: 0.0074
[dropout_rate candidate 0.3] Fold 3: HR@10 = [np.float64(0.640728476821192), np.float64(0.6788079470198676), np.float64(0.7417218543046358), np.float64(0.7251655629139073), np.float64(0.7516556291390728), np.float64(0.7980132450331126), np.float64(0.8956953642384106), np.float64(0.8807947019867549), np.float64(0.8758278145695364), np.float64(0.8708609271523179)]
Tuning dropout_rate:  71%|███████▏  | 5/7 [21:35<08:38, 259.49s/it]
========== Fold 3 ==========
INFO:root:Using statics: seq_size = 10, genre_vocab_size = 18
Fold 3 Epoch 001; Train loss: 0.5944; Time: 00:00:02
Fold 3 Epoch 002; Train loss: 0.2803; Time: 00:00:02
Fold 3 Epoch 003; Train loss: 0.2101; Time: 00:00:02
Fold 3 Epoch 004; Train loss: 0.1814; Time: 00:00:02
Fold 3 Epoch 005; Train loss: 0.1596; Time: 00:00:02
Fold 3 Epoch 006; Train loss: 0.1554; Time: 00:00:02
Fold 3 Epoch 007; Train loss: 0.1445; Time: 00:00:02
Fold 3 Epoch 008; Train loss: 0.1410; Time: 00:00:02
Fold 3 Epoch 009; Train loss: 0.1357; Time: 00:00:02
Fold 3 Epoch 010; Train loss: 0.1350; Time: 00:00:02
Fold 3: Evaluation at Epoch 10
HR@5       NDCG@5     HR@10      NDCG@10   
0.614238   0.458723   0.783113   0.512543  
Loss: 0.1694
Fold 3 Epoch 011; Train loss: 0.1274; Time: 00:00:02
Fold 3 Epoch 012; Train loss: 0.1279; Time: 00:00:02
Fold 3 Epoch 013; Train loss: 0.1278; Time: 00:00:02
Fold 3 Epoch 014; Train loss: 0.1241; Time: 00:00:01
Fold 3 Epoch 015; Train loss: 0.1205; Time: 00:00:02
Fold 3 Epoch 016; Train loss: 0.1167; Time: 00:00:02
Fold 3 Epoch 017; Train loss: 0.1150; Time: 00:00:02
Fold 3 Epoch 018; Train loss: 0.1128; Time: 00:00:02
Fold 3 Epoch 019; Train loss: 0.1120; Time: 00:00:02
Fold 3 Epoch 020; Train loss: 0.1129; Time: 00:00:02
Fold 3: Evaluation at Epoch 20
HR@5       NDCG@5     HR@10      NDCG@10   
0.624172   0.476187   0.849338   0.548357  
Loss: 0.1423
Fold 3 Epoch 021; Train loss: 0.1036; Time: 00:00:01
Fold 3 Epoch 022; Train loss: 0.1040; Time: 00:00:02
Fold 3 Epoch 023; Train loss: 0.1034; Time: 00:00:02
Fold 3 Epoch 024; Train loss: 0.1010; Time: 00:00:01
Fold 3 Epoch 025; Train loss: 0.1009; Time: 00:00:02
Fold 3 Epoch 026; Train loss: 0.0992; Time: 00:00:02
Fold 3 Epoch 027; Train loss: 0.0948; Time: 00:00:02
Fold 3 Epoch 028; Train loss: 0.0910; Time: 00:00:02
Fold 3 Epoch 029; Train loss: 0.0894; Time: 00:00:01
Fold 3 Epoch 030; Train loss: 0.0887; Time: 00:00:02
Fold 3: Evaluation at Epoch 30
HR@5       NDCG@5     HR@10      NDCG@10   
0.698675   0.533597   0.870861   0.589567  
Loss: 0.1250
Fold 3 Epoch 031; Train loss: 0.0868; Time: 00:00:02
Fold 3 Epoch 032; Train loss: 0.0842; Time: 00:00:02
Fold 3 Epoch 033; Train loss: 0.0825; Time: 00:00:02
Fold 3 Epoch 034; Train loss: 0.0798; Time: 00:00:02
Fold 3 Epoch 035; Train loss: 0.0775; Time: 00:00:02
Fold 3 Epoch 036; Train loss: 0.0758; Time: 00:00:02
Fold 3 Epoch 037; Train loss: 0.0746; Time: 00:00:02
Fold 3 Epoch 038; Train loss: 0.0722; Time: 00:00:02
Fold 3 Epoch 039; Train loss: 0.0696; Time: 00:00:02
Fold 3 Epoch 040; Train loss: 0.0680; Time: 00:00:01
Fold 3: Evaluation at Epoch 40
HR@5       NDCG@5     HR@10      NDCG@10   
0.602649   0.447106   0.798013   0.509189  
Loss: 0.0939
Fold 3 Epoch 041; Train loss: 0.0649; Time: 00:00:02
Fold 3 Epoch 042; Train loss: 0.0643; Time: 00:00:02
Fold 3 Epoch 043; Train loss: 0.0617; Time: 00:00:02
Fold 3 Epoch 044; Train loss: 0.0590; Time: 00:00:03
Fold 3 Epoch 045; Train loss: 0.0585; Time: 00:00:02
Fold 3 Epoch 046; Train loss: 0.0553; Time: 00:00:02
Fold 3 Epoch 047; Train loss: 0.0536; Time: 00:00:02
Fold 3 Epoch 048; Train loss: 0.0517; Time: 00:00:02
Fold 3 Epoch 049; Train loss: 0.0488; Time: 00:00:02
Fold 3 Epoch 050; Train loss: 0.0469; Time: 00:00:02
Fold 3: Evaluation at Epoch 50
HR@5       NDCG@5     HR@10      NDCG@10   
0.682119   0.516595   0.894040   0.584759  
Loss: 0.0715
Fold 3 Epoch 051; Train loss: 0.0450; Time: 00:00:01
Fold 3 Epoch 052; Train loss: 0.0430; Time: 00:00:02
Fold 3 Epoch 053; Train loss: 0.0411; Time: 00:00:02
Fold 3 Epoch 054; Train loss: 0.0386; Time: 00:00:02
Fold 3 Epoch 055; Train loss: 0.0372; Time: 00:00:02
Fold 3 Epoch 056; Train loss: 0.0354; Time: 00:00:02
Fold 3 Epoch 057; Train loss: 0.0336; Time: 00:00:02
Fold 3 Epoch 058; Train loss: 0.0318; Time: 00:00:02
Fold 3 Epoch 059; Train loss: 0.0305; Time: 00:00:02
Fold 3 Epoch 060; Train loss: 0.0290; Time: 00:00:02
Fold 3: Evaluation at Epoch 60
HR@5       NDCG@5     HR@10      NDCG@10   
0.627483   0.430864   0.836093   0.498193  
Loss: 0.0493
Fold 3 Epoch 061; Train loss: 0.0274; Time: 00:00:02
Fold 3 Epoch 062; Train loss: 0.0255; Time: 00:00:02
Fold 3 Epoch 063; Train loss: 0.0240; Time: 00:00:02
Fold 3 Epoch 064; Train loss: 0.0220; Time: 00:00:02
Fold 3 Epoch 065; Train loss: 0.0209; Time: 00:00:02
Fold 3 Epoch 066; Train loss: 0.0187; Time: 00:00:02
Fold 3 Epoch 067; Train loss: 0.0179; Time: 00:00:02
Fold 3 Epoch 068; Train loss: 0.0168; Time: 00:00:02
Fold 3 Epoch 069; Train loss: 0.0158; Time: 00:00:02
Fold 3 Epoch 070; Train loss: 0.0146; Time: 00:00:02
Fold 3: Evaluation at Epoch 70
HR@5       NDCG@5     HR@10      NDCG@10   
0.567881   0.396793   0.890728   0.499828  
Loss: 0.0312
Fold 3 Epoch 071; Train loss: 0.0135; Time: 00:00:02
Fold 3 Epoch 072; Train loss: 0.0130; Time: 00:00:02
Fold 3 Epoch 073; Train loss: 0.0122; Time: 00:00:02
Fold 3 Epoch 074; Train loss: 0.0109; Time: 00:00:02
Fold 3 Epoch 075; Train loss: 0.0109; Time: 00:00:02
Fold 3 Epoch 076; Train loss: 0.0098; Time: 00:00:02
Fold 3 Epoch 077; Train loss: 0.0092; Time: 00:00:02
Fold 3 Epoch 078; Train loss: 0.0086; Time: 00:00:02
Fold 3 Epoch 079; Train loss: 0.0080; Time: 00:00:02
Fold 3 Epoch 080; Train loss: 0.0076; Time: 00:00:02
Fold 3: Evaluation at Epoch 80
HR@5       NDCG@5     HR@10      NDCG@10   
0.400662   0.333506   0.877483   0.490429  
Loss: 0.0207
Fold 3 Epoch 081; Train loss: 0.0071; Time: 00:00:02
Fold 3 Epoch 082; Train loss: 0.0067; Time: 00:00:02
Fold 3 Epoch 083; Train loss: 0.0062; Time: 00:00:02
Fold 3 Epoch 084; Train loss: 0.0058; Time: 00:00:02
Fold 3 Epoch 085; Train loss: 0.0054; Time: 00:00:02
Fold 3 Epoch 086; Train loss: 0.0051; Time: 00:00:02
Fold 3 Epoch 087; Train loss: 0.0048; Time: 00:00:02
Fold 3 Epoch 088; Train loss: 0.0045; Time: 00:00:02
Fold 3 Epoch 089; Train loss: 0.0042; Time: 00:00:02
Fold 3 Epoch 090; Train loss: 0.0040; Time: 00:00:02
Fold 3: Evaluation at Epoch 90
HR@5       NDCG@5     HR@10      NDCG@10   
0.514901   0.233969   0.864238   0.355455  
Loss: 0.0141
Fold 3 Epoch 091; Train loss: 0.0037; Time: 00:00:01
Fold 3 Epoch 092; Train loss: 0.0035; Time: 00:00:02
Fold 3 Epoch 093; Train loss: 0.0033; Time: 00:00:02
Fold 3 Epoch 094; Train loss: 0.0031; Time: 00:00:02
Fold 3 Epoch 095; Train loss: 0.0030; Time: 00:00:02
Fold 3 Epoch 096; Train loss: 0.0028; Time: 00:00:02
Fold 3 Epoch 097; Train loss: 0.0026; Time: 00:00:02
Fold 3 Epoch 098; Train loss: 0.0025; Time: 00:00:02
Fold 3 Epoch 099; Train loss: 0.0025; Time: 00:00:02
Fold 3 Epoch 100; Train loss: 0.0024; Time: 00:00:02
Fold 3: Evaluation at Epoch 100
HR@5       NDCG@5     HR@10      NDCG@10   
0.509934   0.265865   0.900662   0.392864  
Loss: 0.0111
[dropout_rate candidate 0.4] Fold 3: HR@10 = [np.float64(0.7831125827814569), np.float64(0.8493377483443708), np.float64(0.8708609271523179), np.float64(0.7980132450331126), np.float64(0.8940397350993378), np.float64(0.8360927152317881), np.float64(0.890728476821192), np.float64(0.8774834437086093), np.float64(0.8642384105960265), np.float64(0.9006622516556292)]
Tuning dropout_rate:  86%|████████▌ | 6/7 [25:54<04:19, 259.31s/it]
========== Fold 3 ==========
INFO:root:Using statics: seq_size = 10, genre_vocab_size = 18
Fold 3 Epoch 001; Train loss: 0.6305; Time: 00:00:02
Fold 3 Epoch 002; Train loss: 0.2944; Time: 00:00:02
Fold 3 Epoch 003; Train loss: 0.2209; Time: 00:00:02
Fold 3 Epoch 004; Train loss: 0.1906; Time: 00:00:02
Fold 3 Epoch 005; Train loss: 0.1723; Time: 00:00:02
Fold 3 Epoch 006; Train loss: 0.1628; Time: 00:00:02
Fold 3 Epoch 007; Train loss: 0.1542; Time: 00:00:02
Fold 3 Epoch 008; Train loss: 0.1473; Time: 00:00:02
Fold 3 Epoch 009; Train loss: 0.1439; Time: 00:00:02
Fold 3 Epoch 010; Train loss: 0.1403; Time: 00:00:02
Fold 3: Evaluation at Epoch 10
HR@5       NDCG@5     HR@10      NDCG@10   
0.670530   0.491879   0.796358   0.532585  
Loss: 0.1882
Fold 3 Epoch 011; Train loss: 0.1355; Time: 00:00:02
Fold 3 Epoch 012; Train loss: 0.1371; Time: 00:00:02
Fold 3 Epoch 013; Train loss: 0.1306; Time: 00:00:02
Fold 3 Epoch 014; Train loss: 0.1309; Time: 00:00:02
Fold 3 Epoch 015; Train loss: 0.1284; Time: 00:00:02
Fold 3 Epoch 016; Train loss: 0.1252; Time: 00:00:02
Fold 3 Epoch 017; Train loss: 0.1225; Time: 00:00:02
Fold 3 Epoch 018; Train loss: 0.1218; Time: 00:00:02
Fold 3 Epoch 019; Train loss: 0.1190; Time: 00:00:02
Fold 3 Epoch 020; Train loss: 0.1196; Time: 00:00:02
Fold 3: Evaluation at Epoch 20
HR@5       NDCG@5     HR@10      NDCG@10   
0.629139   0.462675   0.750000   0.501937  
Loss: 0.1574
Fold 3 Epoch 021; Train loss: 0.1149; Time: 00:00:02
Fold 3 Epoch 022; Train loss: 0.1120; Time: 00:00:02
Fold 3 Epoch 023; Train loss: 0.1109; Time: 00:00:02
Fold 3 Epoch 024; Train loss: 0.1073; Time: 00:00:02
Fold 3 Epoch 025; Train loss: 0.1079; Time: 00:00:02
Fold 3 Epoch 026; Train loss: 0.1050; Time: 00:00:02
Fold 3 Epoch 027; Train loss: 0.1053; Time: 00:00:02
Fold 3 Epoch 028; Train loss: 0.0999; Time: 00:00:02
Fold 3 Epoch 029; Train loss: 0.0977; Time: 00:00:02
Fold 3 Epoch 030; Train loss: 0.0953; Time: 00:00:02
Fold 3: Evaluation at Epoch 30
HR@5       NDCG@5     HR@10      NDCG@10   
0.670530   0.478878   0.831126   0.531556  
Loss: 0.1222
Fold 3 Epoch 031; Train loss: 0.0918; Time: 00:00:02
Fold 3 Epoch 032; Train loss: 0.0922; Time: 00:00:02
Fold 3 Epoch 033; Train loss: 0.0908; Time: 00:00:02
Fold 3 Epoch 034; Train loss: 0.0866; Time: 00:00:02
Fold 3 Epoch 035; Train loss: 0.0852; Time: 00:00:02
Fold 3 Epoch 036; Train loss: 0.0835; Time: 00:00:02
Fold 3 Epoch 037; Train loss: 0.0819; Time: 00:00:02
Fold 3 Epoch 038; Train loss: 0.0789; Time: 00:00:02
Fold 3 Epoch 039; Train loss: 0.0766; Time: 00:00:02
Fold 3 Epoch 040; Train loss: 0.0762; Time: 00:00:02
Fold 3: Evaluation at Epoch 40
HR@5       NDCG@5     HR@10      NDCG@10   
0.685430   0.508459   0.827815   0.554487  
Loss: 0.1042
Fold 3 Epoch 041; Train loss: 0.0739; Time: 00:00:02
Fold 3 Epoch 042; Train loss: 0.0713; Time: 00:00:02
Fold 3 Epoch 043; Train loss: 0.0706; Time: 00:00:02
Fold 3 Epoch 044; Train loss: 0.0671; Time: 00:00:02
Fold 3 Epoch 045; Train loss: 0.0655; Time: 00:00:02
Fold 3 Epoch 046; Train loss: 0.0633; Time: 00:00:02
Fold 3 Epoch 047; Train loss: 0.0608; Time: 00:00:02
Fold 3 Epoch 048; Train loss: 0.0610; Time: 00:00:02
Fold 3 Epoch 049; Train loss: 0.0592; Time: 00:00:02
Fold 3 Epoch 050; Train loss: 0.0570; Time: 00:00:02
Fold 3: Evaluation at Epoch 50
HR@5       NDCG@5     HR@10      NDCG@10   
0.551325   0.350406   0.801325   0.430015  
Loss: 0.0762
Fold 3 Epoch 051; Train loss: 0.0538; Time: 00:00:02
Fold 3 Epoch 052; Train loss: 0.0511; Time: 00:00:02
Fold 3 Epoch 053; Train loss: 0.0507; Time: 00:00:02
Fold 3 Epoch 054; Train loss: 0.0479; Time: 00:00:02
Fold 3 Epoch 055; Train loss: 0.0460; Time: 00:00:02
Fold 3 Epoch 056; Train loss: 0.0442; Time: 00:00:02
Fold 3 Epoch 057; Train loss: 0.0431; Time: 00:00:02
Fold 3 Epoch 058; Train loss: 0.0412; Time: 00:00:02
Fold 3 Epoch 059; Train loss: 0.0384; Time: 00:00:02
Fold 3 Epoch 060; Train loss: 0.0369; Time: 00:00:02
Fold 3: Evaluation at Epoch 60
HR@5       NDCG@5     HR@10      NDCG@10   
0.645695   0.380547   0.882450   0.457654  
Loss: 0.0539
Fold 3 Epoch 061; Train loss: 0.0353; Time: 00:00:02
Fold 3 Epoch 062; Train loss: 0.0337; Time: 00:00:02
Fold 3 Epoch 063; Train loss: 0.0322; Time: 00:00:02
Fold 3 Epoch 064; Train loss: 0.0302; Time: 00:00:02
Fold 3 Epoch 065; Train loss: 0.0286; Time: 00:00:02
Fold 3 Epoch 066; Train loss: 0.0273; Time: 00:00:02
Fold 3 Epoch 067; Train loss: 0.0262; Time: 00:00:02
Fold 3 Epoch 068; Train loss: 0.0254; Time: 00:00:02
Fold 3 Epoch 069; Train loss: 0.0233; Time: 00:00:02
Fold 3 Epoch 070; Train loss: 0.0221; Time: 00:00:02
Fold 3: Evaluation at Epoch 70
HR@5       NDCG@5     HR@10      NDCG@10   
0.640728   0.307314   0.895695   0.394448  
Loss: 0.0374
Fold 3 Epoch 071; Train loss: 0.0209; Time: 00:00:02
Fold 3 Epoch 072; Train loss: 0.0200; Time: 00:00:02
Fold 3 Epoch 073; Train loss: 0.0188; Time: 00:00:02
Fold 3 Epoch 074; Train loss: 0.0177; Time: 00:00:02
Fold 3 Epoch 075; Train loss: 0.0171; Time: 00:00:02
Fold 3 Epoch 076; Train loss: 0.0159; Time: 00:00:02
Fold 3 Epoch 077; Train loss: 0.0149; Time: 00:00:02
Fold 3 Epoch 078; Train loss: 0.0142; Time: 00:00:02
Fold 3 Epoch 079; Train loss: 0.0131; Time: 00:00:02
Fold 3 Epoch 080; Train loss: 0.0123; Time: 00:00:01
Fold 3: Evaluation at Epoch 80
HR@5       NDCG@5     HR@10      NDCG@10   
0.771523   0.378303   0.869205   0.411149  
Loss: 0.0240
Fold 3 Epoch 081; Train loss: 0.0115; Time: 00:00:02
Fold 3 Epoch 082; Train loss: 0.0106; Time: 00:00:02
Fold 3 Epoch 083; Train loss: 0.0103; Time: 00:00:02
Fold 3 Epoch 084; Train loss: 0.0093; Time: 00:00:02
Fold 3 Epoch 085; Train loss: 0.0086; Time: 00:00:02
Fold 3 Epoch 086; Train loss: 0.0081; Time: 00:00:02
Fold 3 Epoch 087; Train loss: 0.0075; Time: 00:00:02
Fold 3 Epoch 088; Train loss: 0.0071; Time: 00:00:01
Fold 3 Epoch 089; Train loss: 0.0066; Time: 00:00:02
Fold 3 Epoch 090; Train loss: 0.0063; Time: 00:00:01
Fold 3: Evaluation at Epoch 90
HR@5       NDCG@5     HR@10      NDCG@10   
0.819536   0.469591   0.870861   0.485941  
Loss: 0.0158
Fold 3 Epoch 091; Train loss: 0.0057; Time: 00:00:02
Fold 3 Epoch 092; Train loss: 0.0052; Time: 00:00:02
Fold 3 Epoch 093; Train loss: 0.0050; Time: 00:00:02
Fold 3 Epoch 094; Train loss: 0.0046; Time: 00:00:02
Fold 3 Epoch 095; Train loss: 0.0043; Time: 00:00:02
Fold 3 Epoch 096; Train loss: 0.0039; Time: 00:00:02
Fold 3 Epoch 097; Train loss: 0.0037; Time: 00:00:02
Fold 3 Epoch 098; Train loss: 0.0034; Time: 00:00:02
Fold 3 Epoch 099; Train loss: 0.0032; Time: 00:00:02
Fold 3 Epoch 100; Train loss: 0.0030; Time: 00:00:02
Fold 3: Evaluation at Epoch 100
HR@5       NDCG@5     HR@10      NDCG@10   
0.521523   0.274457   0.862583   0.382808  
Loss: 0.0104
[dropout_rate candidate 0.5] Fold 3: HR@10 = [np.float64(0.7963576158940397), np.float64(0.75), np.float64(0.8311258278145696), np.float64(0.8278145695364238), np.float64(0.8013245033112583), np.float64(0.8824503311258278), np.float64(0.8956953642384106), np.float64(0.8692052980132451), np.float64(0.8708609271523179), np.float64(0.8625827814569537)]
Tuning dropout_rate: 100%|██████████| 7/7 [30:17<00:00, 260.26s/it]Tuning dropout_rate: 100%|██████████| 7/7 [30:17<00:00, 259.59s/it]

Best dropout_rate found: 0.4
Tuning l2_decay:   0%|          | 0/5 [00:00<?, ?it/s]
========== Fold 3 ==========
INFO:root:Using statics: seq_size = 10, genre_vocab_size = 18
Fold 3 Epoch 001; Train loss: 0.6528; Time: 00:00:02
Fold 3 Epoch 002; Train loss: 0.3003; Time: 00:00:02
Fold 3 Epoch 003; Train loss: 0.2211; Time: 00:00:02
Fold 3 Epoch 004; Train loss: 0.1890; Time: 00:00:02
Fold 3 Epoch 005; Train loss: 0.1675; Time: 00:00:02
Fold 3 Epoch 006; Train loss: 0.1593; Time: 00:00:02
Fold 3 Epoch 007; Train loss: 0.1488; Time: 00:00:02
Fold 3 Epoch 008; Train loss: 0.1453; Time: 00:00:02
Fold 3 Epoch 009; Train loss: 0.1421; Time: 00:00:02
Fold 3 Epoch 010; Train loss: 0.1387; Time: 00:00:02
Fold 3: Evaluation at Epoch 10
HR@5       NDCG@5     HR@10      NDCG@10   
0.534768   0.436621   0.695364   0.487473  
Loss: 0.1701
Fold 3 Epoch 011; Train loss: 0.1345; Time: 00:00:02
Fold 3 Epoch 012; Train loss: 0.1291; Time: 00:00:02
Fold 3 Epoch 013; Train loss: 0.1255; Time: 00:00:02
Fold 3 Epoch 014; Train loss: 0.1236; Time: 00:00:02
Fold 3 Epoch 015; Train loss: 0.1192; Time: 00:00:02
Fold 3 Epoch 016; Train loss: 0.1175; Time: 00:00:02
Fold 3 Epoch 017; Train loss: 0.1166; Time: 00:00:02
Fold 3 Epoch 018; Train loss: 0.1144; Time: 00:00:02
Fold 3 Epoch 019; Train loss: 0.1136; Time: 00:00:02
Fold 3 Epoch 020; Train loss: 0.1079; Time: 00:00:02
Fold 3: Evaluation at Epoch 20
HR@5       NDCG@5     HR@10      NDCG@10   
0.637417   0.520376   0.786424   0.568280  
Loss: 0.1494
Fold 3 Epoch 021; Train loss: 0.1052; Time: 00:00:02
Fold 3 Epoch 022; Train loss: 0.1013; Time: 00:00:02
Fold 3 Epoch 023; Train loss: 0.0986; Time: 00:00:02
Fold 3 Epoch 024; Train loss: 0.0969; Time: 00:00:02
Fold 3 Epoch 025; Train loss: 0.0949; Time: 00:00:02
Fold 3 Epoch 026; Train loss: 0.0909; Time: 00:00:02
Fold 3 Epoch 027; Train loss: 0.0905; Time: 00:00:02
Fold 3 Epoch 028; Train loss: 0.0891; Time: 00:00:02
Fold 3 Epoch 029; Train loss: 0.0871; Time: 00:00:02
Fold 3 Epoch 030; Train loss: 0.0822; Time: 00:00:02
Fold 3: Evaluation at Epoch 30
HR@5       NDCG@5     HR@10      NDCG@10   
0.662252   0.541489   0.799669   0.585929  
Loss: 0.1142
Fold 3 Epoch 031; Train loss: 0.0810; Time: 00:00:02
Fold 3 Epoch 032; Train loss: 0.0786; Time: 00:00:02
Fold 3 Epoch 033; Train loss: 0.0749; Time: 00:00:02
Fold 3 Epoch 034; Train loss: 0.0731; Time: 00:00:02
Fold 3 Epoch 035; Train loss: 0.0702; Time: 00:00:02
Fold 3 Epoch 036; Train loss: 0.0682; Time: 00:00:02
Fold 3 Epoch 037; Train loss: 0.0669; Time: 00:00:02
Fold 3 Epoch 038; Train loss: 0.0638; Time: 00:00:01
Fold 3 Epoch 039; Train loss: 0.0602; Time: 00:00:02
Fold 3 Epoch 040; Train loss: 0.0591; Time: 00:00:02
Fold 3: Evaluation at Epoch 40
HR@5       NDCG@5     HR@10      NDCG@10   
0.678808   0.548843   0.837748   0.599453  
Loss: 0.0847
Fold 3 Epoch 041; Train loss: 0.0573; Time: 00:00:02
Fold 3 Epoch 042; Train loss: 0.0564; Time: 00:00:02
Fold 3 Epoch 043; Train loss: 0.0522; Time: 00:00:02
Fold 3 Epoch 044; Train loss: 0.0506; Time: 00:00:02
Fold 3 Epoch 045; Train loss: 0.0481; Time: 00:00:02
Fold 3 Epoch 046; Train loss: 0.0469; Time: 00:00:02
Fold 3 Epoch 047; Train loss: 0.0449; Time: 00:00:02
Fold 3 Epoch 048; Train loss: 0.0428; Time: 00:00:02
Fold 3 Epoch 049; Train loss: 0.0398; Time: 00:00:02
Fold 3 Epoch 050; Train loss: 0.0384; Time: 00:00:02
Fold 3: Evaluation at Epoch 50
HR@5       NDCG@5     HR@10      NDCG@10   
0.663907   0.509068   0.849338   0.568027  
Loss: 0.0525
Fold 3 Epoch 051; Train loss: 0.0363; Time: 00:00:02
Fold 3 Epoch 052; Train loss: 0.0340; Time: 00:00:02
Fold 3 Epoch 053; Train loss: 0.0313; Time: 00:00:02
Fold 3 Epoch 054; Train loss: 0.0296; Time: 00:00:02
Fold 3 Epoch 055; Train loss: 0.0278; Time: 00:00:02
Fold 3 Epoch 056; Train loss: 0.0262; Time: 00:00:02
Fold 3 Epoch 057; Train loss: 0.0243; Time: 00:00:02
Fold 3 Epoch 058; Train loss: 0.0220; Time: 00:00:02
Fold 3 Epoch 059; Train loss: 0.0205; Time: 00:00:02
Fold 3 Epoch 060; Train loss: 0.0189; Time: 00:00:02
Fold 3: Evaluation at Epoch 60
HR@5       NDCG@5     HR@10      NDCG@10   
0.718543   0.565659   0.903974   0.625843  
Loss: 0.0301
Fold 3 Epoch 061; Train loss: 0.0172; Time: 00:00:01
Fold 3 Epoch 062; Train loss: 0.0156; Time: 00:00:02
Fold 3 Epoch 063; Train loss: 0.0141; Time: 00:00:02
Fold 3 Epoch 064; Train loss: 0.0125; Time: 00:00:02
Fold 3 Epoch 065; Train loss: 0.0113; Time: 00:00:02
Fold 3 Epoch 066; Train loss: 0.0098; Time: 00:00:02
Fold 3 Epoch 067; Train loss: 0.0089; Time: 00:00:02
Fold 3 Epoch 068; Train loss: 0.0080; Time: 00:00:02
Fold 3 Epoch 069; Train loss: 0.0070; Time: 00:00:01
Fold 3 Epoch 070; Train loss: 0.0062; Time: 00:00:02
Fold 3: Evaluation at Epoch 70
HR@5       NDCG@5     HR@10      NDCG@10   
0.791391   0.539538   0.903974   0.576722  
Loss: 0.0130
Fold 3 Epoch 071; Train loss: 0.0056; Time: 00:00:02
Fold 3 Epoch 072; Train loss: 0.0051; Time: 00:00:02
Fold 3 Epoch 073; Train loss: 0.0044; Time: 00:00:01
Fold 3 Epoch 074; Train loss: 0.0041; Time: 00:00:02
Fold 3 Epoch 075; Train loss: 0.0036; Time: 00:00:02
Fold 3 Epoch 076; Train loss: 0.0033; Time: 00:00:02
Fold 3 Epoch 077; Train loss: 0.0030; Time: 00:00:02
Fold 3 Epoch 078; Train loss: 0.0027; Time: 00:00:02
Fold 3 Epoch 079; Train loss: 0.0025; Time: 00:00:03
Fold 3 Epoch 080; Train loss: 0.0022; Time: 00:00:02
Fold 3: Evaluation at Epoch 80
HR@5       NDCG@5     HR@10      NDCG@10   
0.789735   0.414597   0.899007   0.450159  
Loss: 0.0071
Fold 3 Epoch 081; Train loss: 0.0021; Time: 00:00:02
Fold 3 Epoch 082; Train loss: 0.0019; Time: 00:00:02
Fold 3 Epoch 083; Train loss: 0.0017; Time: 00:00:02
Fold 3 Epoch 084; Train loss: 0.0016; Time: 00:00:02
Fold 3 Epoch 085; Train loss: 0.0014; Time: 00:00:02
Fold 3 Epoch 086; Train loss: 0.0014; Time: 00:00:02
Fold 3 Epoch 087; Train loss: 0.0012; Time: 00:00:02
Fold 3 Epoch 088; Train loss: 0.0011; Time: 00:00:02
Fold 3 Epoch 089; Train loss: 0.0011; Time: 00:00:02
Fold 3 Epoch 090; Train loss: 0.0010; Time: 00:00:02
Fold 3: Evaluation at Epoch 90
HR@5       NDCG@5     HR@10      NDCG@10   
0.541391   0.370361   0.895695   0.487498  
Loss: 0.0044
Fold 3 Epoch 091; Train loss: 0.0010; Time: 00:00:02
Fold 3 Epoch 092; Train loss: 0.0009; Time: 00:00:02
Fold 3 Epoch 093; Train loss: 0.0008; Time: 00:00:02
Fold 3 Epoch 094; Train loss: 0.0008; Time: 00:00:02
Fold 3 Epoch 095; Train loss: 0.0007; Time: 00:00:02
Fold 3 Epoch 096; Train loss: 0.0007; Time: 00:00:02
Fold 3 Epoch 097; Train loss: 0.0006; Time: 00:00:02
Fold 3 Epoch 098; Train loss: 0.0006; Time: 00:00:02
Fold 3 Epoch 099; Train loss: 0.0006; Time: 00:00:02
Fold 3 Epoch 100; Train loss: 0.0005; Time: 00:00:02
Fold 3: Evaluation at Epoch 100
HR@5       NDCG@5     HR@10      NDCG@10   
0.341060   0.189270   0.899007   0.369479  
Loss: 0.0028
[l2_decay candidate 0.1] Fold 3: HR@10 = [np.float64(0.695364238410596), np.float64(0.7864238410596026), np.float64(0.7996688741721855), np.float64(0.8377483443708609), np.float64(0.8493377483443708), np.float64(0.9039735099337748), np.float64(0.9039735099337748), np.float64(0.8990066225165563), np.float64(0.8956953642384106), np.float64(0.8990066225165563)]
Tuning l2_decay:  20%|██        | 1/5 [04:24<17:36, 264.21s/it]
========== Fold 3 ==========
INFO:root:Using statics: seq_size = 10, genre_vocab_size = 18
Fold 3 Epoch 001; Train loss: 0.6035; Time: 00:00:02
Fold 3 Epoch 002; Train loss: 0.2808; Time: 00:00:02
Fold 3 Epoch 003; Train loss: 0.2081; Time: 00:00:02
Fold 3 Epoch 004; Train loss: 0.1746; Time: 00:00:02
Fold 3 Epoch 005; Train loss: 0.1609; Time: 00:00:02
Fold 3 Epoch 006; Train loss: 0.1511; Time: 00:00:02
Fold 3 Epoch 007; Train loss: 0.1425; Time: 00:00:02
Fold 3 Epoch 008; Train loss: 0.1400; Time: 00:00:02
Fold 3 Epoch 009; Train loss: 0.1333; Time: 00:00:02
Fold 3 Epoch 010; Train loss: 0.1316; Time: 00:00:02
Fold 3: Evaluation at Epoch 10
HR@5       NDCG@5     HR@10      NDCG@10   
0.592715   0.489260   0.716887   0.529264  
Loss: 0.1839
Fold 3 Epoch 011; Train loss: 0.1278; Time: 00:00:02
Fold 3 Epoch 012; Train loss: 0.1218; Time: 00:00:02
Fold 3 Epoch 013; Train loss: 0.1227; Time: 00:00:02
Fold 3 Epoch 014; Train loss: 0.1198; Time: 00:00:02
Fold 3 Epoch 015; Train loss: 0.1177; Time: 00:00:02
Fold 3 Epoch 016; Train loss: 0.1151; Time: 00:00:02
Fold 3 Epoch 017; Train loss: 0.1097; Time: 00:00:02
Fold 3 Epoch 018; Train loss: 0.1085; Time: 00:00:01
Fold 3 Epoch 019; Train loss: 0.1074; Time: 00:00:02
Fold 3 Epoch 020; Train loss: 0.1073; Time: 00:00:02
Fold 3: Evaluation at Epoch 20
HR@5       NDCG@5     HR@10      NDCG@10   
0.630795   0.516390   0.773179   0.562777  
Loss: 0.1492
Fold 3 Epoch 021; Train loss: 0.1042; Time: 00:00:01
Fold 3 Epoch 022; Train loss: 0.1027; Time: 00:00:02
Fold 3 Epoch 023; Train loss: 0.1013; Time: 00:00:02
Fold 3 Epoch 024; Train loss: 0.0947; Time: 00:00:02
Fold 3 Epoch 025; Train loss: 0.0969; Time: 00:00:02
Fold 3 Epoch 026; Train loss: 0.0967; Time: 00:00:02
Fold 3 Epoch 027; Train loss: 0.0922; Time: 00:00:02
Fold 3 Epoch 028; Train loss: 0.0922; Time: 00:00:02
Fold 3 Epoch 029; Train loss: 0.0875; Time: 00:00:02
Fold 3 Epoch 030; Train loss: 0.0861; Time: 00:00:02
Fold 3: Evaluation at Epoch 30
HR@5       NDCG@5     HR@10      NDCG@10   
0.662252   0.510571   0.783113   0.548694  
Loss: 0.1204
Fold 3 Epoch 031; Train loss: 0.0833; Time: 00:00:01
Fold 3 Epoch 032; Train loss: 0.0835; Time: 00:00:02
Fold 3 Epoch 033; Train loss: 0.0795; Time: 00:00:02
Fold 3 Epoch 034; Train loss: 0.0764; Time: 00:00:02
Fold 3 Epoch 035; Train loss: 0.0753; Time: 00:00:03
Fold 3 Epoch 036; Train loss: 0.0725; Time: 00:00:02
Fold 3 Epoch 037; Train loss: 0.0723; Time: 00:00:03
Fold 3 Epoch 038; Train loss: 0.0704; Time: 00:00:02
Fold 3 Epoch 039; Train loss: 0.0683; Time: 00:00:02
Fold 3 Epoch 040; Train loss: 0.0668; Time: 00:00:02
Fold 3: Evaluation at Epoch 40
HR@5       NDCG@5     HR@10      NDCG@10   
0.615894   0.493018   0.750000   0.536767  
Loss: 0.0996
Fold 3 Epoch 041; Train loss: 0.0635; Time: 00:00:02
Fold 3 Epoch 042; Train loss: 0.0612; Time: 00:00:02
Fold 3 Epoch 043; Train loss: 0.0594; Time: 00:00:02
Fold 3 Epoch 044; Train loss: 0.0576; Time: 00:00:02
Fold 3 Epoch 045; Train loss: 0.0569; Time: 00:00:02
Fold 3 Epoch 046; Train loss: 0.0541; Time: 00:00:02
Fold 3 Epoch 047; Train loss: 0.0521; Time: 00:00:02
Fold 3 Epoch 048; Train loss: 0.0498; Time: 00:00:02
Fold 3 Epoch 049; Train loss: 0.0474; Time: 00:00:02
Fold 3 Epoch 050; Train loss: 0.0464; Time: 00:00:02
Fold 3: Evaluation at Epoch 50
HR@5       NDCG@5     HR@10      NDCG@10   
0.639073   0.513054   0.786424   0.560834  
Loss: 0.0777
Fold 3 Epoch 051; Train loss: 0.0448; Time: 00:00:02
Fold 3 Epoch 052; Train loss: 0.0419; Time: 00:00:02
Fold 3 Epoch 053; Train loss: 0.0401; Time: 00:00:02
Fold 3 Epoch 054; Train loss: 0.0386; Time: 00:00:02
Fold 3 Epoch 055; Train loss: 0.0372; Time: 00:00:02
Fold 3 Epoch 056; Train loss: 0.0358; Time: 00:00:02
Fold 3 Epoch 057; Train loss: 0.0343; Time: 00:00:02
Fold 3 Epoch 058; Train loss: 0.0318; Time: 00:00:02
Fold 3 Epoch 059; Train loss: 0.0304; Time: 00:00:02
Fold 3 Epoch 060; Train loss: 0.0279; Time: 00:00:02
Fold 3: Evaluation at Epoch 60
HR@5       NDCG@5     HR@10      NDCG@10   
0.677152   0.488342   0.836093   0.539061  
Loss: 0.0524
Fold 3 Epoch 061; Train loss: 0.0264; Time: 00:00:02
Fold 3 Epoch 062; Train loss: 0.0250; Time: 00:00:02
Fold 3 Epoch 063; Train loss: 0.0232; Time: 00:00:02
Fold 3 Epoch 064; Train loss: 0.0214; Time: 00:00:02
Fold 3 Epoch 065; Train loss: 0.0204; Time: 00:00:02
Fold 3 Epoch 066; Train loss: 0.0186; Time: 00:00:02
Fold 3 Epoch 067; Train loss: 0.0175; Time: 00:00:02
Fold 3 Epoch 068; Train loss: 0.0161; Time: 00:00:02
Fold 3 Epoch 069; Train loss: 0.0151; Time: 00:00:02
Fold 3 Epoch 070; Train loss: 0.0141; Time: 00:00:02
Fold 3: Evaluation at Epoch 70
HR@5       NDCG@5     HR@10      NDCG@10   
0.571192   0.341165   0.870861   0.435928  
Loss: 0.0354
Fold 3 Epoch 071; Train loss: 0.0128; Time: 00:00:02
Fold 3 Epoch 072; Train loss: 0.0121; Time: 00:00:02
Fold 3 Epoch 073; Train loss: 0.0110; Time: 00:00:02
Fold 3 Epoch 074; Train loss: 0.0103; Time: 00:00:02
Fold 3 Epoch 075; Train loss: 0.0095; Time: 00:00:02
Fold 3 Epoch 076; Train loss: 0.0088; Time: 00:00:02
Fold 3 Epoch 077; Train loss: 0.0083; Time: 00:00:02
Fold 3 Epoch 078; Train loss: 0.0076; Time: 00:00:02
Fold 3 Epoch 079; Train loss: 0.0072; Time: 00:00:02
Fold 3 Epoch 080; Train loss: 0.0067; Time: 00:00:02
Fold 3: Evaluation at Epoch 80
HR@5       NDCG@5     HR@10      NDCG@10   
0.266556   0.208982   0.852649   0.392413  
Loss: 0.0216
Fold 3 Epoch 081; Train loss: 0.0063; Time: 00:00:02
Fold 3 Epoch 082; Train loss: 0.0059; Time: 00:00:02
Fold 3 Epoch 083; Train loss: 0.0055; Time: 00:00:02
Fold 3 Epoch 084; Train loss: 0.0051; Time: 00:00:02
Fold 3 Epoch 085; Train loss: 0.0048; Time: 00:00:02
Fold 3 Epoch 086; Train loss: 0.0044; Time: 00:00:02
Fold 3 Epoch 087; Train loss: 0.0041; Time: 00:00:02
Fold 3 Epoch 088; Train loss: 0.0039; Time: 00:00:02
Fold 3 Epoch 089; Train loss: 0.0037; Time: 00:00:02
Fold 3 Epoch 090; Train loss: 0.0035; Time: 00:00:02
Fold 3: Evaluation at Epoch 90
HR@5       NDCG@5     HR@10      NDCG@10   
0.263245   0.119647   0.774834   0.273962  
Loss: 0.0142
Fold 3 Epoch 091; Train loss: 0.0033; Time: 00:00:02
Fold 3 Epoch 092; Train loss: 0.0032; Time: 00:00:02
Fold 3 Epoch 093; Train loss: 0.0030; Time: 00:00:02
Fold 3 Epoch 094; Train loss: 0.0028; Time: 00:00:02
Fold 3 Epoch 095; Train loss: 0.0026; Time: 00:00:02
Fold 3 Epoch 096; Train loss: 0.0025; Time: 00:00:02
Fold 3 Epoch 097; Train loss: 0.0024; Time: 00:00:02
Fold 3 Epoch 098; Train loss: 0.0023; Time: 00:00:02
Fold 3 Epoch 099; Train loss: 0.0022; Time: 00:00:02
Fold 3 Epoch 100; Train loss: 0.0021; Time: 00:00:02
Fold 3: Evaluation at Epoch 100
HR@5       NDCG@5     HR@10      NDCG@10   
0.243377   0.101939   0.846026   0.286101  
Loss: 0.0106
[l2_decay candidate 0.01] Fold 3: HR@10 = [np.float64(0.7168874172185431), np.float64(0.7731788079470199), np.float64(0.7831125827814569), np.float64(0.75), np.float64(0.7864238410596026), np.float64(0.8360927152317881), np.float64(0.8708609271523179), np.float64(0.8526490066225165), np.float64(0.7748344370860927), np.float64(0.8460264900662252)]
Tuning l2_decay:  40%|████      | 2/5 [08:46<13:09, 263.17s/it]
========== Fold 3 ==========
INFO:root:Using statics: seq_size = 10, genre_vocab_size = 18
Fold 3 Epoch 001; Train loss: 0.6601; Time: 00:00:02
Fold 3 Epoch 002; Train loss: 0.3037; Time: 00:00:01
Fold 3 Epoch 003; Train loss: 0.2238; Time: 00:00:02
Fold 3 Epoch 004; Train loss: 0.1865; Time: 00:00:02
Fold 3 Epoch 005; Train loss: 0.1753; Time: 00:00:02
Fold 3 Epoch 006; Train loss: 0.1651; Time: 00:00:01
Fold 3 Epoch 007; Train loss: 0.1579; Time: 00:00:02
Fold 3 Epoch 008; Train loss: 0.1507; Time: 00:00:02
Fold 3 Epoch 009; Train loss: 0.1429; Time: 00:00:02
Fold 3 Epoch 010; Train loss: 0.1438; Time: 00:00:02
Fold 3: Evaluation at Epoch 10
HR@5       NDCG@5     HR@10      NDCG@10   
0.533113   0.387094   0.697020   0.439291  
Loss: 0.1859
Fold 3 Epoch 011; Train loss: 0.1369; Time: 00:00:02
Fold 3 Epoch 012; Train loss: 0.1353; Time: 00:00:02
Fold 3 Epoch 013; Train loss: 0.1330; Time: 00:00:02
Fold 3 Epoch 014; Train loss: 0.1313; Time: 00:00:02
Fold 3 Epoch 015; Train loss: 0.1275; Time: 00:00:02
Fold 3 Epoch 016; Train loss: 0.1266; Time: 00:00:02
Fold 3 Epoch 017; Train loss: 0.1224; Time: 00:00:02
Fold 3 Epoch 018; Train loss: 0.1235; Time: 00:00:02
Fold 3 Epoch 019; Train loss: 0.1180; Time: 00:00:02
Fold 3 Epoch 020; Train loss: 0.1165; Time: 00:00:02
Fold 3: Evaluation at Epoch 20
HR@5       NDCG@5     HR@10      NDCG@10   
0.543046   0.415768   0.672185   0.456947  
Loss: 0.1500
Fold 3 Epoch 021; Train loss: 0.1175; Time: 00:00:02
Fold 3 Epoch 022; Train loss: 0.1119; Time: 00:00:02
Fold 3 Epoch 023; Train loss: 0.1127; Time: 00:00:02
Fold 3 Epoch 024; Train loss: 0.1093; Time: 00:00:02
Fold 3 Epoch 025; Train loss: 0.1072; Time: 00:00:02
Fold 3 Epoch 026; Train loss: 0.1050; Time: 00:00:02
Fold 3 Epoch 027; Train loss: 0.1041; Time: 00:00:02
Fold 3 Epoch 028; Train loss: 0.1014; Time: 00:00:02
Fold 3 Epoch 029; Train loss: 0.0998; Time: 00:00:02
Fold 3 Epoch 030; Train loss: 0.0960; Time: 00:00:02
Fold 3: Evaluation at Epoch 30
HR@5       NDCG@5     HR@10      NDCG@10   
0.564570   0.449856   0.718543   0.498872  
Loss: 0.1187
Fold 3 Epoch 031; Train loss: 0.0945; Time: 00:00:02
Fold 3 Epoch 032; Train loss: 0.0911; Time: 00:00:02
Fold 3 Epoch 033; Train loss: 0.0926; Time: 00:00:02
Fold 3 Epoch 034; Train loss: 0.0876; Time: 00:00:02
Fold 3 Epoch 035; Train loss: 0.0876; Time: 00:00:02
Fold 3 Epoch 036; Train loss: 0.0854; Time: 00:00:02
Fold 3 Epoch 037; Train loss: 0.0805; Time: 00:00:02
Fold 3 Epoch 038; Train loss: 0.0803; Time: 00:00:02
Fold 3 Epoch 039; Train loss: 0.0770; Time: 00:00:02
Fold 3 Epoch 040; Train loss: 0.0776; Time: 00:00:02
Fold 3: Evaluation at Epoch 40
HR@5       NDCG@5     HR@10      NDCG@10   
0.541391   0.433339   0.725166   0.491230  
Loss: 0.1043
Fold 3 Epoch 041; Train loss: 0.0744; Time: 00:00:02
Fold 3 Epoch 042; Train loss: 0.0723; Time: 00:00:02
Fold 3 Epoch 043; Train loss: 0.0717; Time: 00:00:02
Fold 3 Epoch 044; Train loss: 0.0691; Time: 00:00:02
Fold 3 Epoch 045; Train loss: 0.0687; Time: 00:00:02
Fold 3 Epoch 046; Train loss: 0.0657; Time: 00:00:02
Fold 3 Epoch 047; Train loss: 0.0640; Time: 00:00:02
Fold 3 Epoch 048; Train loss: 0.0607; Time: 00:00:02
Fold 3 Epoch 049; Train loss: 0.0605; Time: 00:00:02
Fold 3 Epoch 050; Train loss: 0.0567; Time: 00:00:02
Fold 3: Evaluation at Epoch 50
HR@5       NDCG@5     HR@10      NDCG@10   
0.640728   0.495773   0.809603   0.549693  
Loss: 0.0789
Fold 3 Epoch 051; Train loss: 0.0561; Time: 00:00:02
Fold 3 Epoch 052; Train loss: 0.0535; Time: 00:00:02
Fold 3 Epoch 053; Train loss: 0.0521; Time: 00:00:02
Fold 3 Epoch 054; Train loss: 0.0504; Time: 00:00:02
Fold 3 Epoch 055; Train loss: 0.0493; Time: 00:00:02
Fold 3 Epoch 056; Train loss: 0.0467; Time: 00:00:02
Fold 3 Epoch 057; Train loss: 0.0458; Time: 00:00:02
Fold 3 Epoch 058; Train loss: 0.0435; Time: 00:00:02
Fold 3 Epoch 059; Train loss: 0.0426; Time: 00:00:02
Fold 3 Epoch 060; Train loss: 0.0409; Time: 00:00:02
Fold 3: Evaluation at Epoch 60
HR@5       NDCG@5     HR@10      NDCG@10   
0.538079   0.363711   0.736755   0.427414  
Loss: 0.0598
Fold 3 Epoch 061; Train loss: 0.0379; Time: 00:00:02
Fold 3 Epoch 062; Train loss: 0.0367; Time: 00:00:02
Fold 3 Epoch 063; Train loss: 0.0351; Time: 00:00:02
Fold 3 Epoch 064; Train loss: 0.0334; Time: 00:00:01
Fold 3 Epoch 065; Train loss: 0.0319; Time: 00:00:02
Fold 3 Epoch 066; Train loss: 0.0308; Time: 00:00:02
Fold 3 Epoch 067; Train loss: 0.0292; Time: 00:00:02
Fold 3 Epoch 068; Train loss: 0.0282; Time: 00:00:02
Fold 3 Epoch 069; Train loss: 0.0263; Time: 00:00:02
Fold 3 Epoch 070; Train loss: 0.0251; Time: 00:00:02
Fold 3: Evaluation at Epoch 70
HR@5       NDCG@5     HR@10      NDCG@10   
0.663907   0.360696   0.847682   0.421828  
Loss: 0.0428
Fold 3 Epoch 071; Train loss: 0.0242; Time: 00:00:02
Fold 3 Epoch 072; Train loss: 0.0222; Time: 00:00:02
Fold 3 Epoch 073; Train loss: 0.0211; Time: 00:00:01
Fold 3 Epoch 074; Train loss: 0.0199; Time: 00:00:02
Fold 3 Epoch 075; Train loss: 0.0181; Time: 00:00:02
Fold 3 Epoch 076; Train loss: 0.0173; Time: 00:00:02
Fold 3 Epoch 077; Train loss: 0.0159; Time: 00:00:02
Fold 3 Epoch 078; Train loss: 0.0150; Time: 00:00:02
Fold 3 Epoch 079; Train loss: 0.0140; Time: 00:00:01
Fold 3 Epoch 080; Train loss: 0.0128; Time: 00:00:02
Fold 3: Evaluation at Epoch 80
HR@5       NDCG@5     HR@10      NDCG@10   
0.706954   0.359880   0.865894   0.412577  
Loss: 0.0275
Fold 3 Epoch 081; Train loss: 0.0122; Time: 00:00:02
Fold 3 Epoch 082; Train loss: 0.0112; Time: 00:00:02
Fold 3 Epoch 083; Train loss: 0.0106; Time: 00:00:02
Fold 3 Epoch 084; Train loss: 0.0096; Time: 00:00:02
Fold 3 Epoch 085; Train loss: 0.0088; Time: 00:00:02
Fold 3 Epoch 086; Train loss: 0.0083; Time: 00:00:02
Fold 3 Epoch 087; Train loss: 0.0078; Time: 00:00:02
Fold 3 Epoch 088; Train loss: 0.0070; Time: 00:00:02
Fold 3 Epoch 089; Train loss: 0.0065; Time: 00:00:02
Fold 3 Epoch 090; Train loss: 0.0061; Time: 00:00:02
Fold 3: Evaluation at Epoch 90
HR@5       NDCG@5     HR@10      NDCG@10   
0.513245   0.300136   0.872517   0.419223  
Loss: 0.0175
Fold 3 Epoch 091; Train loss: 0.0055; Time: 00:00:02
Fold 3 Epoch 092; Train loss: 0.0050; Time: 00:00:02
Fold 3 Epoch 093; Train loss: 0.0046; Time: 00:00:02
Fold 3 Epoch 094; Train loss: 0.0044; Time: 00:00:02
Fold 3 Epoch 095; Train loss: 0.0041; Time: 00:00:02
Fold 3 Epoch 096; Train loss: 0.0038; Time: 00:00:02
Fold 3 Epoch 097; Train loss: 0.0035; Time: 00:00:02
Fold 3 Epoch 098; Train loss: 0.0032; Time: 00:00:02
Fold 3 Epoch 099; Train loss: 0.0030; Time: 00:00:02
Fold 3 Epoch 100; Train loss: 0.0029; Time: 00:00:02
Fold 3: Evaluation at Epoch 100
HR@5       NDCG@5     HR@10      NDCG@10   
0.500000   0.238275   0.860927   0.348484  
Loss: 0.0120
[l2_decay candidate 0.0001] Fold 3: HR@10 = [np.float64(0.6970198675496688), np.float64(0.6721854304635762), np.float64(0.7185430463576159), np.float64(0.7251655629139073), np.float64(0.8096026490066225), np.float64(0.7367549668874173), np.float64(0.847682119205298), np.float64(0.8658940397350994), np.float64(0.8725165562913907), np.float64(0.8609271523178808)]
Tuning l2_decay:  60%|██████    | 3/5 [13:12<08:48, 264.17s/it]
========== Fold 3 ==========
INFO:root:Using statics: seq_size = 10, genre_vocab_size = 18
Fold 3 Epoch 001; Train loss: 0.6544; Time: 00:00:02
Fold 3 Epoch 002; Train loss: 0.2984; Time: 00:00:02
Fold 3 Epoch 003; Train loss: 0.2219; Time: 00:00:02
Fold 3 Epoch 004; Train loss: 0.1891; Time: 00:00:02
Fold 3 Epoch 005; Train loss: 0.1723; Time: 00:00:02
Fold 3 Epoch 006; Train loss: 0.1638; Time: 00:00:02
Fold 3 Epoch 007; Train loss: 0.1563; Time: 00:00:02
Fold 3 Epoch 008; Train loss: 0.1508; Time: 00:00:02
Fold 3 Epoch 009; Train loss: 0.1461; Time: 00:00:02
Fold 3 Epoch 010; Train loss: 0.1405; Time: 00:00:02
Fold 3: Evaluation at Epoch 10
HR@5       NDCG@5     HR@10      NDCG@10   
0.579470   0.455934   0.741722   0.508198  
Loss: 0.1857
Fold 3 Epoch 011; Train loss: 0.1374; Time: 00:00:02
Fold 3 Epoch 012; Train loss: 0.1378; Time: 00:00:01
Fold 3 Epoch 013; Train loss: 0.1341; Time: 00:00:01
Fold 3 Epoch 014; Train loss: 0.1286; Time: 00:00:02
Fold 3 Epoch 015; Train loss: 0.1254; Time: 00:00:02
Fold 3 Epoch 016; Train loss: 0.1267; Time: 00:00:02
Fold 3 Epoch 017; Train loss: 0.1240; Time: 00:00:02
Fold 3 Epoch 018; Train loss: 0.1213; Time: 00:00:02
Fold 3 Epoch 019; Train loss: 0.1195; Time: 00:00:02
Fold 3 Epoch 020; Train loss: 0.1179; Time: 00:00:02
Fold 3: Evaluation at Epoch 20
HR@5       NDCG@5     HR@10      NDCG@10   
0.557947   0.424054   0.706954   0.473409  
Loss: 0.1514
Fold 3 Epoch 021; Train loss: 0.1123; Time: 00:00:02
Fold 3 Epoch 022; Train loss: 0.1118; Time: 00:00:02
Fold 3 Epoch 023; Train loss: 0.1082; Time: 00:00:02
Fold 3 Epoch 024; Train loss: 0.1072; Time: 00:00:02
Fold 3 Epoch 025; Train loss: 0.1050; Time: 00:00:02
Fold 3 Epoch 026; Train loss: 0.1046; Time: 00:00:02
Fold 3 Epoch 027; Train loss: 0.0996; Time: 00:00:02
Fold 3 Epoch 028; Train loss: 0.0980; Time: 00:00:02
Fold 3 Epoch 029; Train loss: 0.0932; Time: 00:00:02
Fold 3 Epoch 030; Train loss: 0.0939; Time: 00:00:02
Fold 3: Evaluation at Epoch 30
HR@5       NDCG@5     HR@10      NDCG@10   
0.481788   0.362446   0.668874   0.423049  
Loss: 0.1236
Fold 3 Epoch 031; Train loss: 0.0896; Time: 00:00:02
Fold 3 Epoch 032; Train loss: 0.0891; Time: 00:00:02
Fold 3 Epoch 033; Train loss: 0.0855; Time: 00:00:02
Fold 3 Epoch 034; Train loss: 0.0849; Time: 00:00:02
Fold 3 Epoch 035; Train loss: 0.0823; Time: 00:00:02
Fold 3 Epoch 036; Train loss: 0.0831; Time: 00:00:02
Fold 3 Epoch 037; Train loss: 0.0792; Time: 00:00:02
Fold 3 Epoch 038; Train loss: 0.0759; Time: 00:00:02
Fold 3 Epoch 039; Train loss: 0.0756; Time: 00:00:02
Fold 3 Epoch 040; Train loss: 0.0720; Time: 00:00:02
Fold 3: Evaluation at Epoch 40
HR@5       NDCG@5     HR@10      NDCG@10   
0.341060   0.211602   0.619205   0.302773  
Loss: 0.0970
Fold 3 Epoch 041; Train loss: 0.0709; Time: 00:00:02
Fold 3 Epoch 042; Train loss: 0.0695; Time: 00:00:02
Fold 3 Epoch 043; Train loss: 0.0679; Time: 00:00:02
Fold 3 Epoch 044; Train loss: 0.0669; Time: 00:00:02
Fold 3 Epoch 045; Train loss: 0.0630; Time: 00:00:02
Fold 3 Epoch 046; Train loss: 0.0610; Time: 00:00:02
Fold 3 Epoch 047; Train loss: 0.0594; Time: 00:00:02
Fold 3 Epoch 048; Train loss: 0.0568; Time: 00:00:02
Fold 3 Epoch 049; Train loss: 0.0566; Time: 00:00:02
Fold 3 Epoch 050; Train loss: 0.0537; Time: 00:00:02
Fold 3: Evaluation at Epoch 50
HR@5       NDCG@5     HR@10      NDCG@10   
0.387417   0.226576   0.687086   0.323108  
Loss: 0.0741
Fold 3 Epoch 051; Train loss: 0.0526; Time: 00:00:02
Fold 3 Epoch 052; Train loss: 0.0514; Time: 00:00:02
Fold 3 Epoch 053; Train loss: 0.0483; Time: 00:00:02
Fold 3 Epoch 054; Train loss: 0.0477; Time: 00:00:02
Fold 3 Epoch 055; Train loss: 0.0459; Time: 00:00:02
Fold 3 Epoch 056; Train loss: 0.0437; Time: 00:00:02
Fold 3 Epoch 057; Train loss: 0.0416; Time: 00:00:02
Fold 3 Epoch 058; Train loss: 0.0414; Time: 00:00:02
Fold 3 Epoch 059; Train loss: 0.0393; Time: 00:00:02
Fold 3 Epoch 060; Train loss: 0.0379; Time: 00:00:02
Fold 3: Evaluation at Epoch 60
HR@5       NDCG@5     HR@10      NDCG@10   
0.432119   0.248157   0.798013   0.368193  
Loss: 0.0516
Fold 3 Epoch 061; Train loss: 0.0367; Time: 00:00:02
Fold 3 Epoch 062; Train loss: 0.0349; Time: 00:00:02
Fold 3 Epoch 063; Train loss: 0.0337; Time: 00:00:02
Fold 3 Epoch 064; Train loss: 0.0323; Time: 00:00:02
Fold 3 Epoch 065; Train loss: 0.0305; Time: 00:00:02
Fold 3 Epoch 066; Train loss: 0.0291; Time: 00:00:02
Fold 3 Epoch 067; Train loss: 0.0291; Time: 00:00:02
Fold 3 Epoch 068; Train loss: 0.0270; Time: 00:00:02
Fold 3 Epoch 069; Train loss: 0.0260; Time: 00:00:02
Fold 3 Epoch 070; Train loss: 0.0243; Time: 00:00:02
Fold 3: Evaluation at Epoch 70
HR@5       NDCG@5     HR@10      NDCG@10   
0.481788   0.262749   0.880795   0.393181  
Loss: 0.0372
Fold 3 Epoch 071; Train loss: 0.0241; Time: 00:00:02
Fold 3 Epoch 072; Train loss: 0.0221; Time: 00:00:02
Fold 3 Epoch 073; Train loss: 0.0210; Time: 00:00:02
Fold 3 Epoch 074; Train loss: 0.0208; Time: 00:00:02
Fold 3 Epoch 075; Train loss: 0.0196; Time: 00:00:02
Fold 3 Epoch 076; Train loss: 0.0183; Time: 00:00:02
Fold 3 Epoch 077; Train loss: 0.0178; Time: 00:00:02
Fold 3 Epoch 078; Train loss: 0.0165; Time: 00:00:02
Fold 3 Epoch 079; Train loss: 0.0158; Time: 00:00:02
Fold 3 Epoch 080; Train loss: 0.0146; Time: 00:00:02
Fold 3: Evaluation at Epoch 80
HR@5       NDCG@5     HR@10      NDCG@10   
0.531457   0.264031   0.910596   0.388771  
Loss: 0.0241
Fold 3 Epoch 081; Train loss: 0.0132; Time: 00:00:02
Fold 3 Epoch 082; Train loss: 0.0125; Time: 00:00:01
Fold 3 Epoch 083; Train loss: 0.0119; Time: 00:00:02
Fold 3 Epoch 084; Train loss: 0.0107; Time: 00:00:02
Fold 3 Epoch 085; Train loss: 0.0098; Time: 00:00:02
Fold 3 Epoch 086; Train loss: 0.0089; Time: 00:00:02
Fold 3 Epoch 087; Train loss: 0.0082; Time: 00:00:02
Fold 3 Epoch 088; Train loss: 0.0075; Time: 00:00:02
Fold 3 Epoch 089; Train loss: 0.0067; Time: 00:00:02
Fold 3 Epoch 090; Train loss: 0.0062; Time: 00:00:02
Fold 3: Evaluation at Epoch 90
HR@5       NDCG@5     HR@10      NDCG@10   
0.551325   0.251641   0.902318   0.370312  
Loss: 0.0154
Fold 3 Epoch 091; Train loss: 0.0055; Time: 00:00:02
Fold 3 Epoch 092; Train loss: 0.0051; Time: 00:00:02
Fold 3 Epoch 093; Train loss: 0.0047; Time: 00:00:02
Fold 3 Epoch 094; Train loss: 0.0043; Time: 00:00:02
Fold 3 Epoch 095; Train loss: 0.0041; Time: 00:00:02
Fold 3 Epoch 096; Train loss: 0.0036; Time: 00:00:02
Fold 3 Epoch 097; Train loss: 0.0034; Time: 00:00:02
Fold 3 Epoch 098; Train loss: 0.0031; Time: 00:00:02
Fold 3 Epoch 099; Train loss: 0.0030; Time: 00:00:02
Fold 3 Epoch 100; Train loss: 0.0028; Time: 00:00:02
Fold 3: Evaluation at Epoch 100
HR@5       NDCG@5     HR@10      NDCG@10   
0.536424   0.247703   0.895695   0.362269  
Loss: 0.0109
[l2_decay candidate 1e-08] Fold 3: HR@10 = [np.float64(0.7417218543046358), np.float64(0.706953642384106), np.float64(0.6688741721854304), np.float64(0.6192052980132451), np.float64(0.6870860927152318), np.float64(0.7980132450331126), np.float64(0.8807947019867549), np.float64(0.9105960264900662), np.float64(0.902317880794702), np.float64(0.8956953642384106)]
Tuning l2_decay:  80%|████████  | 4/5 [17:33<04:22, 262.98s/it]
========== Fold 3 ==========
INFO:root:Using statics: seq_size = 10, genre_vocab_size = 18
Fold 3 Epoch 001; Train loss: 0.5967; Time: 00:00:02
Fold 3 Epoch 002; Train loss: 0.2776; Time: 00:00:02
Fold 3 Epoch 003; Train loss: 0.2074; Time: 00:00:02
Fold 3 Epoch 004; Train loss: 0.1744; Time: 00:00:02
Fold 3 Epoch 005; Train loss: 0.1638; Time: 00:00:01
Fold 3 Epoch 006; Train loss: 0.1500; Time: 00:00:02
Fold 3 Epoch 007; Train loss: 0.1488; Time: 00:00:02
Fold 3 Epoch 008; Train loss: 0.1422; Time: 00:00:02
Fold 3 Epoch 009; Train loss: 0.1356; Time: 00:00:02
Fold 3 Epoch 010; Train loss: 0.1322; Time: 00:00:02
Fold 3: Evaluation at Epoch 10
HR@5       NDCG@5     HR@10      NDCG@10   
0.708609   0.525545   0.834437   0.565522  
Loss: 0.1780
Fold 3 Epoch 011; Train loss: 0.1331; Time: 00:00:02
Fold 3 Epoch 012; Train loss: 0.1300; Time: 00:00:02
Fold 3 Epoch 013; Train loss: 0.1250; Time: 00:00:02
Fold 3 Epoch 014; Train loss: 0.1243; Time: 00:00:02
Fold 3 Epoch 015; Train loss: 0.1209; Time: 00:00:02
Fold 3 Epoch 016; Train loss: 0.1207; Time: 00:00:02
Fold 3 Epoch 017; Train loss: 0.1183; Time: 00:00:02
Fold 3 Epoch 018; Train loss: 0.1166; Time: 00:00:02
Fold 3 Epoch 019; Train loss: 0.1135; Time: 00:00:02
Fold 3 Epoch 020; Train loss: 0.1093; Time: 00:00:02
Fold 3: Evaluation at Epoch 20
HR@5       NDCG@5     HR@10      NDCG@10   
0.716887   0.540846   0.817881   0.573224  
Loss: 0.1504
Fold 3 Epoch 021; Train loss: 0.1080; Time: 00:00:02
Fold 3 Epoch 022; Train loss: 0.1064; Time: 00:00:02
Fold 3 Epoch 023; Train loss: 0.1074; Time: 00:00:02
Fold 3 Epoch 024; Train loss: 0.1019; Time: 00:00:02
Fold 3 Epoch 025; Train loss: 0.1002; Time: 00:00:02
Fold 3 Epoch 026; Train loss: 0.0974; Time: 00:00:02
Fold 3 Epoch 027; Train loss: 0.0956; Time: 00:00:02
Fold 3 Epoch 028; Train loss: 0.0943; Time: 00:00:02
Fold 3 Epoch 029; Train loss: 0.0922; Time: 00:00:02
Fold 3 Epoch 030; Train loss: 0.0899; Time: 00:00:02
Fold 3: Evaluation at Epoch 30
HR@5       NDCG@5     HR@10      NDCG@10   
0.649007   0.502086   0.826159   0.559747  
Loss: 0.1138
Fold 3 Epoch 031; Train loss: 0.0892; Time: 00:00:02
Fold 3 Epoch 032; Train loss: 0.0872; Time: 00:00:02
Fold 3 Epoch 033; Train loss: 0.0852; Time: 00:00:02
Fold 3 Epoch 034; Train loss: 0.0833; Time: 00:00:01
Fold 3 Epoch 035; Train loss: 0.0800; Time: 00:00:01
Fold 3 Epoch 036; Train loss: 0.0783; Time: 00:00:02
Fold 3 Epoch 037; Train loss: 0.0760; Time: 00:00:02
Fold 3 Epoch 038; Train loss: 0.0745; Time: 00:00:02
Fold 3 Epoch 039; Train loss: 0.0717; Time: 00:00:02
Fold 3 Epoch 040; Train loss: 0.0710; Time: 00:00:02
Fold 3: Evaluation at Epoch 40
HR@5       NDCG@5     HR@10      NDCG@10   
0.673841   0.482908   0.885762   0.552067  
Loss: 0.0958
Fold 3 Epoch 041; Train loss: 0.0688; Time: 00:00:02
Fold 3 Epoch 042; Train loss: 0.0663; Time: 00:00:02
Fold 3 Epoch 043; Train loss: 0.0654; Time: 00:00:02
Fold 3 Epoch 044; Train loss: 0.0628; Time: 00:00:02
Fold 3 Epoch 045; Train loss: 0.0616; Time: 00:00:02
Fold 3 Epoch 046; Train loss: 0.0593; Time: 00:00:02
Fold 3 Epoch 047; Train loss: 0.0579; Time: 00:00:02
Fold 3 Epoch 048; Train loss: 0.0570; Time: 00:00:02
Fold 3 Epoch 049; Train loss: 0.0545; Time: 00:00:02
Fold 3 Epoch 050; Train loss: 0.0523; Time: 00:00:02
Fold 3: Evaluation at Epoch 50
HR@5       NDCG@5     HR@10      NDCG@10   
0.726821   0.573696   0.875828   0.621857  
Loss: 0.0704
Fold 3 Epoch 051; Train loss: 0.0509; Time: 00:00:02
Fold 3 Epoch 052; Train loss: 0.0485; Time: 00:00:02
Fold 3 Epoch 053; Train loss: 0.0468; Time: 00:00:02
Fold 3 Epoch 054; Train loss: 0.0449; Time: 00:00:02
Fold 3 Epoch 055; Train loss: 0.0442; Time: 00:00:02
Fold 3 Epoch 056; Train loss: 0.0413; Time: 00:00:02
Fold 3 Epoch 057; Train loss: 0.0385; Time: 00:00:02
Fold 3 Epoch 058; Train loss: 0.0378; Time: 00:00:02
Fold 3 Epoch 059; Train loss: 0.0361; Time: 00:00:02
Fold 3 Epoch 060; Train loss: 0.0343; Time: 00:00:02
Fold 3: Evaluation at Epoch 60
HR@5       NDCG@5     HR@10      NDCG@10   
0.781457   0.572310   0.903974   0.612222  
Loss: 0.0507
Fold 3 Epoch 061; Train loss: 0.0340; Time: 00:00:02
Fold 3 Epoch 062; Train loss: 0.0324; Time: 00:00:02
Fold 3 Epoch 063; Train loss: 0.0302; Time: 00:00:02
Fold 3 Epoch 064; Train loss: 0.0287; Time: 00:00:02
Fold 3 Epoch 065; Train loss: 0.0275; Time: 00:00:02
Fold 3 Epoch 066; Train loss: 0.0266; Time: 00:00:02
Fold 3 Epoch 067; Train loss: 0.0251; Time: 00:00:02
Fold 3 Epoch 068; Train loss: 0.0236; Time: 00:00:02
Fold 3 Epoch 069; Train loss: 0.0226; Time: 00:00:02
Fold 3 Epoch 070; Train loss: 0.0213; Time: 00:00:02
Fold 3: Evaluation at Epoch 70
HR@5       NDCG@5     HR@10      NDCG@10   
0.812914   0.604562   0.915563   0.638737  
Loss: 0.0346
Fold 3 Epoch 071; Train loss: 0.0195; Time: 00:00:02
Fold 3 Epoch 072; Train loss: 0.0187; Time: 00:00:02
Fold 3 Epoch 073; Train loss: 0.0178; Time: 00:00:02
Fold 3 Epoch 074; Train loss: 0.0163; Time: 00:00:02
Fold 3 Epoch 075; Train loss: 0.0154; Time: 00:00:02
Fold 3 Epoch 076; Train loss: 0.0142; Time: 00:00:02
Fold 3 Epoch 077; Train loss: 0.0132; Time: 00:00:02
Fold 3 Epoch 078; Train loss: 0.0121; Time: 00:00:02
Fold 3 Epoch 079; Train loss: 0.0112; Time: 00:00:02
Fold 3 Epoch 080; Train loss: 0.0103; Time: 00:00:02
Fold 3: Evaluation at Epoch 80
HR@5       NDCG@5     HR@10      NDCG@10   
0.822848   0.540591   0.917219   0.572400  
Loss: 0.0181
Fold 3 Epoch 081; Train loss: 0.0092; Time: 00:00:02
Fold 3 Epoch 082; Train loss: 0.0084; Time: 00:00:02
Fold 3 Epoch 083; Train loss: 0.0077; Time: 00:00:02
Fold 3 Epoch 084; Train loss: 0.0071; Time: 00:00:02
Fold 3 Epoch 085; Train loss: 0.0064; Time: 00:00:02
Fold 3 Epoch 086; Train loss: 0.0059; Time: 00:00:02
Fold 3 Epoch 087; Train loss: 0.0055; Time: 00:00:02
Fold 3 Epoch 088; Train loss: 0.0050; Time: 00:00:02
Fold 3 Epoch 089; Train loss: 0.0046; Time: 00:00:02
Fold 3 Epoch 090; Train loss: 0.0043; Time: 00:00:02
Fold 3: Evaluation at Epoch 90
HR@5       NDCG@5     HR@10      NDCG@10   
0.841060   0.559467   0.922185   0.586286  
Loss: 0.0114
Fold 3 Epoch 091; Train loss: 0.0039; Time: 00:00:02
Fold 3 Epoch 092; Train loss: 0.0036; Time: 00:00:02
Fold 3 Epoch 093; Train loss: 0.0034; Time: 00:00:02
Fold 3 Epoch 094; Train loss: 0.0030; Time: 00:00:02
Fold 3 Epoch 095; Train loss: 0.0028; Time: 00:00:02
Fold 3 Epoch 096; Train loss: 0.0026; Time: 00:00:02
Fold 3 Epoch 097; Train loss: 0.0024; Time: 00:00:02
Fold 3 Epoch 098; Train loss: 0.0023; Time: 00:00:02
Fold 3 Epoch 099; Train loss: 0.0022; Time: 00:00:02
Fold 3 Epoch 100; Train loss: 0.0020; Time: 00:00:02
Fold 3: Evaluation at Epoch 100
HR@5       NDCG@5     HR@10      NDCG@10   
0.569536   0.408091   0.922185   0.523281  
Loss: 0.0074
[l2_decay candidate 5e-16] Fold 3: HR@10 = [np.float64(0.8344370860927153), np.float64(0.8178807947019867), np.float64(0.8261589403973509), np.float64(0.8857615894039735), np.float64(0.8758278145695364), np.float64(0.9039735099337748), np.float64(0.9155629139072847), np.float64(0.9172185430463576), np.float64(0.9221854304635762), np.float64(0.9221854304635762)]
Tuning l2_decay: 100%|██████████| 5/5 [21:54<00:00, 262.22s/it]Tuning l2_decay: 100%|██████████| 5/5 [21:54<00:00, 262.81s/it]

Best l2_decay found: 5e-16
Tuning eps:   0%|          | 0/5 [00:00<?, ?it/s]
========== Fold 3 ==========
INFO:root:Using statics: seq_size = 10, genre_vocab_size = 18
Fold 3 Epoch 001; Train loss: 1.0657; Time: 00:00:01
Fold 3 Epoch 002; Train loss: 0.8663; Time: 00:00:02
Fold 3 Epoch 003; Train loss: 0.7339; Time: 00:00:02
Fold 3 Epoch 004; Train loss: 0.6450; Time: 00:00:02
Fold 3 Epoch 005; Train loss: 0.5769; Time: 00:00:02
Fold 3 Epoch 006; Train loss: 0.5257; Time: 00:00:02
Fold 3 Epoch 007; Train loss: 0.4858; Time: 00:00:02
Fold 3 Epoch 008; Train loss: 0.4513; Time: 00:00:02
Fold 3 Epoch 009; Train loss: 0.4285; Time: 00:00:02
Fold 3 Epoch 010; Train loss: 0.4083; Time: 00:00:02
Fold 3: Evaluation at Epoch 10
HR@5       NDCG@5     HR@10      NDCG@10   
0.604305   0.420238   0.822848   0.491552  
Loss: 0.5881
Fold 3 Epoch 011; Train loss: 0.3874; Time: 00:00:02
Fold 3 Epoch 012; Train loss: 0.3705; Time: 00:00:02
Fold 3 Epoch 013; Train loss: 0.3563; Time: 00:00:02
Fold 3 Epoch 014; Train loss: 0.3443; Time: 00:00:02
Fold 3 Epoch 015; Train loss: 0.3314; Time: 00:00:02
Fold 3 Epoch 016; Train loss: 0.3280; Time: 00:00:02
Fold 3 Epoch 017; Train loss: 0.3118; Time: 00:00:02
Fold 3 Epoch 018; Train loss: 0.3097; Time: 00:00:02
Fold 3 Epoch 019; Train loss: 0.3023; Time: 00:00:02
Fold 3 Epoch 020; Train loss: 0.2969; Time: 00:00:02
Fold 3: Evaluation at Epoch 20
HR@5       NDCG@5     HR@10      NDCG@10   
0.485099   0.354126   0.804636   0.456389  
Loss: 0.4608
Fold 3 Epoch 021; Train loss: 0.2896; Time: 00:00:02
Fold 3 Epoch 022; Train loss: 0.2874; Time: 00:00:02
Fold 3 Epoch 023; Train loss: 0.2808; Time: 00:00:02
Fold 3 Epoch 024; Train loss: 0.2778; Time: 00:00:02
Fold 3 Epoch 025; Train loss: 0.2741; Time: 00:00:02
Fold 3 Epoch 026; Train loss: 0.2653; Time: 00:00:02
Fold 3 Epoch 027; Train loss: 0.2675; Time: 00:00:01
Fold 3 Epoch 028; Train loss: 0.2590; Time: 00:00:02
Fold 3 Epoch 029; Train loss: 0.2561; Time: 00:00:02
Fold 3 Epoch 030; Train loss: 0.2535; Time: 00:00:02
Fold 3: Evaluation at Epoch 30
HR@5       NDCG@5     HR@10      NDCG@10   
0.478477   0.351985   0.731788   0.431836  
Loss: 0.3792
Fold 3 Epoch 031; Train loss: 0.2477; Time: 00:00:02
Fold 3 Epoch 032; Train loss: 0.2492; Time: 00:00:02
Fold 3 Epoch 033; Train loss: 0.2422; Time: 00:00:02
Fold 3 Epoch 034; Train loss: 0.2424; Time: 00:00:02
Fold 3 Epoch 035; Train loss: 0.2376; Time: 00:00:02
Fold 3 Epoch 036; Train loss: 0.2386; Time: 00:00:02
Fold 3 Epoch 037; Train loss: 0.2351; Time: 00:00:02
Fold 3 Epoch 038; Train loss: 0.2284; Time: 00:00:02
Fold 3 Epoch 039; Train loss: 0.2299; Time: 00:00:02
Fold 3 Epoch 040; Train loss: 0.2283; Time: 00:00:02
Fold 3: Evaluation at Epoch 40
HR@5       NDCG@5     HR@10      NDCG@10   
0.495033   0.365770   0.720199   0.437935  
Loss: 0.3253
Fold 3 Epoch 041; Train loss: 0.2286; Time: 00:00:02
Fold 3 Epoch 042; Train loss: 0.2260; Time: 00:00:02
Fold 3 Epoch 043; Train loss: 0.2253; Time: 00:00:02
Fold 3 Epoch 044; Train loss: 0.2166; Time: 00:00:02
Fold 3 Epoch 045; Train loss: 0.2190; Time: 00:00:02
Fold 3 Epoch 046; Train loss: 0.2165; Time: 00:00:02
Fold 3 Epoch 047; Train loss: 0.2140; Time: 00:00:02
Fold 3 Epoch 048; Train loss: 0.2131; Time: 00:00:02
Fold 3 Epoch 049; Train loss: 0.2156; Time: 00:00:02
Fold 3 Epoch 050; Train loss: 0.2143; Time: 00:00:02
Fold 3: Evaluation at Epoch 50
HR@5       NDCG@5     HR@10      NDCG@10   
0.519868   0.386250   0.731788   0.454460  
Loss: 0.2997
Fold 3 Epoch 051; Train loss: 0.2096; Time: 00:00:02
Fold 3 Epoch 052; Train loss: 0.2079; Time: 00:00:02
Fold 3 Epoch 053; Train loss: 0.2092; Time: 00:00:02
Fold 3 Epoch 054; Train loss: 0.2034; Time: 00:00:02
Fold 3 Epoch 055; Train loss: 0.2057; Time: 00:00:02
Fold 3 Epoch 056; Train loss: 0.2006; Time: 00:00:02
Fold 3 Epoch 057; Train loss: 0.1997; Time: 00:00:01
Fold 3 Epoch 058; Train loss: 0.1983; Time: 00:00:02
Fold 3 Epoch 059; Train loss: 0.1980; Time: 00:00:02
Fold 3 Epoch 060; Train loss: 0.1983; Time: 00:00:02
Fold 3: Evaluation at Epoch 60
HR@5       NDCG@5     HR@10      NDCG@10   
0.524834   0.400747   0.740066   0.470158  
Loss: 0.2824
Fold 3 Epoch 061; Train loss: 0.1975; Time: 00:00:01
Fold 3 Epoch 062; Train loss: 0.1957; Time: 00:00:02
Fold 3 Epoch 063; Train loss: 0.1902; Time: 00:00:02
Fold 3 Epoch 064; Train loss: 0.1940; Time: 00:00:02
Fold 3 Epoch 065; Train loss: 0.1924; Time: 00:00:02
Fold 3 Epoch 066; Train loss: 0.1929; Time: 00:00:02
Fold 3 Epoch 067; Train loss: 0.1936; Time: 00:00:02
Fold 3 Epoch 068; Train loss: 0.1903; Time: 00:00:02
Fold 3 Epoch 069; Train loss: 0.1902; Time: 00:00:01
Fold 3 Epoch 070; Train loss: 0.1902; Time: 00:00:02
Fold 3: Evaluation at Epoch 70
HR@5       NDCG@5     HR@10      NDCG@10   
0.567881   0.422601   0.751656   0.482185  
Loss: 0.2546
Fold 3 Epoch 071; Train loss: 0.1871; Time: 00:00:02
Fold 3 Epoch 072; Train loss: 0.1883; Time: 00:00:01
Fold 3 Epoch 073; Train loss: 0.1854; Time: 00:00:02
Fold 3 Epoch 074; Train loss: 0.1870; Time: 00:00:02
Fold 3 Epoch 075; Train loss: 0.1881; Time: 00:00:02
Fold 3 Epoch 076; Train loss: 0.1850; Time: 00:00:02
Fold 3 Epoch 077; Train loss: 0.1831; Time: 00:00:02
Fold 3 Epoch 078; Train loss: 0.1844; Time: 00:00:02
Fold 3 Epoch 079; Train loss: 0.1788; Time: 00:00:02
Fold 3 Epoch 080; Train loss: 0.1811; Time: 00:00:02
Fold 3: Evaluation at Epoch 80
HR@5       NDCG@5     HR@10      NDCG@10   
0.572848   0.433301   0.746689   0.489992  
Loss: 0.2412
Fold 3 Epoch 081; Train loss: 0.1821; Time: 00:00:01
Fold 3 Epoch 082; Train loss: 0.1829; Time: 00:00:02
Fold 3 Epoch 083; Train loss: 0.1800; Time: 00:00:02
Fold 3 Epoch 084; Train loss: 0.1776; Time: 00:00:02
Fold 3 Epoch 085; Train loss: 0.1799; Time: 00:00:02
Fold 3 Epoch 086; Train loss: 0.1752; Time: 00:00:02
Fold 3 Epoch 087; Train loss: 0.1775; Time: 00:00:02
Fold 3 Epoch 088; Train loss: 0.1775; Time: 00:00:02
Fold 3 Epoch 089; Train loss: 0.1751; Time: 00:00:02
Fold 3 Epoch 090; Train loss: 0.1762; Time: 00:00:02
Fold 3: Evaluation at Epoch 90
HR@5       NDCG@5     HR@10      NDCG@10   
0.557947   0.425491   0.751656   0.487932  
Loss: 0.2493
Fold 3 Epoch 091; Train loss: 0.1748; Time: 00:00:02
Fold 3 Epoch 092; Train loss: 0.1759; Time: 00:00:02
Fold 3 Epoch 093; Train loss: 0.1744; Time: 00:00:02
Fold 3 Epoch 094; Train loss: 0.1728; Time: 00:00:02
Fold 3 Epoch 095; Train loss: 0.1748; Time: 00:00:02
Fold 3 Epoch 096; Train loss: 0.1752; Time: 00:00:02
Fold 3 Epoch 097; Train loss: 0.1710; Time: 00:00:02
Fold 3 Epoch 098; Train loss: 0.1727; Time: 00:00:02
Fold 3 Epoch 099; Train loss: 0.1732; Time: 00:00:02
Fold 3 Epoch 100; Train loss: 0.1758; Time: 00:00:02
Fold 3: Evaluation at Epoch 100
HR@5       NDCG@5     HR@10      NDCG@10   
0.591060   0.455132   0.764901   0.510342  
Loss: 0.2328
[eps candidate 0.1] Fold 3: HR@10 = [np.float64(0.8228476821192053), np.float64(0.804635761589404), np.float64(0.7317880794701986), np.float64(0.7201986754966887), np.float64(0.7317880794701986), np.float64(0.7400662251655629), np.float64(0.7516556291390728), np.float64(0.7466887417218543), np.float64(0.7516556291390728), np.float64(0.7649006622516556)]
Tuning eps:  20%|██        | 1/5 [04:18<17:15, 258.96s/it]
========== Fold 3 ==========
INFO:root:Using statics: seq_size = 10, genre_vocab_size = 18
Fold 3 Epoch 001; Train loss: 0.8448; Time: 00:00:01
Fold 3 Epoch 002; Train loss: 0.4727; Time: 00:00:02
Fold 3 Epoch 003; Train loss: 0.3574; Time: 00:00:02
Fold 3 Epoch 004; Train loss: 0.3020; Time: 00:00:02
Fold 3 Epoch 005; Train loss: 0.2672; Time: 00:00:02
Fold 3 Epoch 006; Train loss: 0.2496; Time: 00:00:02
Fold 3 Epoch 007; Train loss: 0.2324; Time: 00:00:02
Fold 3 Epoch 008; Train loss: 0.2155; Time: 00:00:02
Fold 3 Epoch 009; Train loss: 0.2144; Time: 00:00:02
Fold 3 Epoch 010; Train loss: 0.2021; Time: 00:00:02
Fold 3: Evaluation at Epoch 10
HR@5       NDCG@5     HR@10      NDCG@10   
0.596026   0.476914   0.745033   0.524065  
Loss: 0.2741
Fold 3 Epoch 011; Train loss: 0.1941; Time: 00:00:02
Fold 3 Epoch 012; Train loss: 0.1912; Time: 00:00:02
Fold 3 Epoch 013; Train loss: 0.1855; Time: 00:00:01
Fold 3 Epoch 014; Train loss: 0.1813; Time: 00:00:02
Fold 3 Epoch 015; Train loss: 0.1781; Time: 00:00:02
Fold 3 Epoch 016; Train loss: 0.1714; Time: 00:00:02
Fold 3 Epoch 017; Train loss: 0.1696; Time: 00:00:02
Fold 3 Epoch 018; Train loss: 0.1694; Time: 00:00:02
Fold 3 Epoch 019; Train loss: 0.1675; Time: 00:00:02
Fold 3 Epoch 020; Train loss: 0.1683; Time: 00:00:02
Fold 3: Evaluation at Epoch 20
HR@5       NDCG@5     HR@10      NDCG@10   
0.592715   0.468091   0.756623   0.520828  
Loss: 0.2153
Fold 3 Epoch 021; Train loss: 0.1645; Time: 00:00:02
Fold 3 Epoch 022; Train loss: 0.1596; Time: 00:00:02
Fold 3 Epoch 023; Train loss: 0.1623; Time: 00:00:02
Fold 3 Epoch 024; Train loss: 0.1610; Time: 00:00:02
Fold 3 Epoch 025; Train loss: 0.1578; Time: 00:00:02
Fold 3 Epoch 026; Train loss: 0.1596; Time: 00:00:02
Fold 3 Epoch 027; Train loss: 0.1556; Time: 00:00:02
Fold 3 Epoch 028; Train loss: 0.1552; Time: 00:00:01
Fold 3 Epoch 029; Train loss: 0.1548; Time: 00:00:02
Fold 3 Epoch 030; Train loss: 0.1525; Time: 00:00:02
Fold 3: Evaluation at Epoch 30
HR@5       NDCG@5     HR@10      NDCG@10   
0.620861   0.492400   0.781457   0.543662  
Loss: 0.2013
Fold 3 Epoch 031; Train loss: 0.1547; Time: 00:00:02
Fold 3 Epoch 032; Train loss: 0.1550; Time: 00:00:02
Fold 3 Epoch 033; Train loss: 0.1506; Time: 00:00:02
Fold 3 Epoch 034; Train loss: 0.1524; Time: 00:00:02
Fold 3 Epoch 035; Train loss: 0.1523; Time: 00:00:02
Fold 3 Epoch 036; Train loss: 0.1475; Time: 00:00:02
Fold 3 Epoch 037; Train loss: 0.1483; Time: 00:00:02
Fold 3 Epoch 038; Train loss: 0.1495; Time: 00:00:02
Fold 3 Epoch 039; Train loss: 0.1510; Time: 00:00:02
Fold 3 Epoch 040; Train loss: 0.1475; Time: 00:00:02
Fold 3: Evaluation at Epoch 40
HR@5       NDCG@5     HR@10      NDCG@10   
0.625828   0.498323   0.788079   0.550361  
Loss: 0.1959
Fold 3 Epoch 041; Train loss: 0.1465; Time: 00:00:02
Fold 3 Epoch 042; Train loss: 0.1466; Time: 00:00:02
Fold 3 Epoch 043; Train loss: 0.1463; Time: 00:00:02
Fold 3 Epoch 044; Train loss: 0.1466; Time: 00:00:02
Fold 3 Epoch 045; Train loss: 0.1483; Time: 00:00:02
Fold 3 Epoch 046; Train loss: 0.1454; Time: 00:00:02
Fold 3 Epoch 047; Train loss: 0.1456; Time: 00:00:02
Fold 3 Epoch 048; Train loss: 0.1442; Time: 00:00:02
Fold 3 Epoch 049; Train loss: 0.1462; Time: 00:00:02
Fold 3 Epoch 050; Train loss: 0.1457; Time: 00:00:02
Fold 3: Evaluation at Epoch 50
HR@5       NDCG@5     HR@10      NDCG@10   
0.602649   0.468741   0.758278   0.518415  
Loss: 0.1885
Fold 3 Epoch 051; Train loss: 0.1439; Time: 00:00:02
Fold 3 Epoch 052; Train loss: 0.1436; Time: 00:00:02
Fold 3 Epoch 053; Train loss: 0.1449; Time: 00:00:02
Fold 3 Epoch 054; Train loss: 0.1463; Time: 00:00:02
Fold 3 Epoch 055; Train loss: 0.1424; Time: 00:00:02
Fold 3 Epoch 056; Train loss: 0.1424; Time: 00:00:02
Fold 3 Epoch 057; Train loss: 0.1413; Time: 00:00:02
Fold 3 Epoch 058; Train loss: 0.1420; Time: 00:00:02
Fold 3 Epoch 059; Train loss: 0.1450; Time: 00:00:02
Fold 3 Epoch 060; Train loss: 0.1414; Time: 00:00:02
Fold 3: Evaluation at Epoch 60
HR@5       NDCG@5     HR@10      NDCG@10   
0.577815   0.456329   0.750000   0.511579  
Loss: 0.1836
Fold 3 Epoch 061; Train loss: 0.1437; Time: 00:00:02
Fold 3 Epoch 062; Train loss: 0.1409; Time: 00:00:02
Fold 3 Epoch 063; Train loss: 0.1432; Time: 00:00:02
Fold 3 Epoch 064; Train loss: 0.1434; Time: 00:00:02
Fold 3 Epoch 065; Train loss: 0.1399; Time: 00:00:02
Fold 3 Epoch 066; Train loss: 0.1432; Time: 00:00:02
Fold 3 Epoch 067; Train loss: 0.1415; Time: 00:00:02
Fold 3 Epoch 068; Train loss: 0.1408; Time: 00:00:02
Fold 3 Epoch 069; Train loss: 0.1424; Time: 00:00:02
Fold 3 Epoch 070; Train loss: 0.1414; Time: 00:00:02
Fold 3: Evaluation at Epoch 70
HR@5       NDCG@5     HR@10      NDCG@10   
0.599338   0.470218   0.756623   0.520956  
Loss: 0.1744
Fold 3 Epoch 071; Train loss: 0.1378; Time: 00:00:02
Fold 3 Epoch 072; Train loss: 0.1406; Time: 00:00:02
Fold 3 Epoch 073; Train loss: 0.1379; Time: 00:00:02
Fold 3 Epoch 074; Train loss: 0.1389; Time: 00:00:02
Fold 3 Epoch 075; Train loss: 0.1379; Time: 00:00:02
Fold 3 Epoch 076; Train loss: 0.1368; Time: 00:00:02
Fold 3 Epoch 077; Train loss: 0.1389; Time: 00:00:02
Fold 3 Epoch 078; Train loss: 0.1398; Time: 00:00:02
Fold 3 Epoch 079; Train loss: 0.1386; Time: 00:00:02
Fold 3 Epoch 080; Train loss: 0.1420; Time: 00:00:02
Fold 3: Evaluation at Epoch 80
HR@5       NDCG@5     HR@10      NDCG@10   
0.614238   0.502241   0.764901   0.550494  
Loss: 0.1884
Fold 3 Epoch 081; Train loss: 0.1361; Time: 00:00:02
Fold 3 Epoch 082; Train loss: 0.1380; Time: 00:00:02
Fold 3 Epoch 083; Train loss: 0.1391; Time: 00:00:02
Fold 3 Epoch 084; Train loss: 0.1367; Time: 00:00:02
Fold 3 Epoch 085; Train loss: 0.1363; Time: 00:00:02
Fold 3 Epoch 086; Train loss: 0.1361; Time: 00:00:02
Fold 3 Epoch 087; Train loss: 0.1361; Time: 00:00:02
Fold 3 Epoch 088; Train loss: 0.1352; Time: 00:00:02
Fold 3 Epoch 089; Train loss: 0.1379; Time: 00:00:02
Fold 3 Epoch 090; Train loss: 0.1327; Time: 00:00:02
Fold 3: Evaluation at Epoch 90
HR@5       NDCG@5     HR@10      NDCG@10   
0.604305   0.479218   0.751656   0.525993  
Loss: 0.1724
Fold 3 Epoch 091; Train loss: 0.1390; Time: 00:00:02
Fold 3 Epoch 092; Train loss: 0.1367; Time: 00:00:02
Fold 3 Epoch 093; Train loss: 0.1354; Time: 00:00:02
Fold 3 Epoch 094; Train loss: 0.1342; Time: 00:00:02
Fold 3 Epoch 095; Train loss: 0.1364; Time: 00:00:02
Fold 3 Epoch 096; Train loss: 0.1350; Time: 00:00:02
Fold 3 Epoch 097; Train loss: 0.1334; Time: 00:00:02
Fold 3 Epoch 098; Train loss: 0.1364; Time: 00:00:02
Fold 3 Epoch 099; Train loss: 0.1340; Time: 00:00:02
Fold 3 Epoch 100; Train loss: 0.1369; Time: 00:00:02
Fold 3: Evaluation at Epoch 100
HR@5       NDCG@5     HR@10      NDCG@10   
0.632450   0.504471   0.784768   0.553955  
Loss: 0.1700
[eps candidate 0.01] Fold 3: HR@10 = [np.float64(0.7450331125827815), np.float64(0.7566225165562914), np.float64(0.7814569536423841), np.float64(0.7880794701986755), np.float64(0.7582781456953642), np.float64(0.75), np.float64(0.7566225165562914), np.float64(0.7649006622516556), np.float64(0.7516556291390728), np.float64(0.7847682119205298)]
Tuning eps:  40%|████      | 2/5 [08:42<13:05, 261.69s/it]
========== Fold 3 ==========
INFO:root:Using statics: seq_size = 10, genre_vocab_size = 18
Fold 3 Epoch 001; Train loss: 0.6714; Time: 00:00:01
Fold 3 Epoch 002; Train loss: 0.3031; Time: 00:00:02
Fold 3 Epoch 003; Train loss: 0.2195; Time: 00:00:02
Fold 3 Epoch 004; Train loss: 0.1923; Time: 00:00:02
Fold 3 Epoch 005; Train loss: 0.1746; Time: 00:00:02
Fold 3 Epoch 006; Train loss: 0.1626; Time: 00:00:02
Fold 3 Epoch 007; Train loss: 0.1576; Time: 00:00:02
Fold 3 Epoch 008; Train loss: 0.1537; Time: 00:00:02
Fold 3 Epoch 009; Train loss: 0.1464; Time: 00:00:02
Fold 3 Epoch 010; Train loss: 0.1436; Time: 00:00:02
Fold 3: Evaluation at Epoch 10
HR@5       NDCG@5     HR@10      NDCG@10   
0.576159   0.457352   0.697020   0.496032  
Loss: 0.1799
Fold 3 Epoch 011; Train loss: 0.1405; Time: 00:00:03
Fold 3 Epoch 012; Train loss: 0.1372; Time: 00:00:02
Fold 3 Epoch 013; Train loss: 0.1350; Time: 00:00:02
Fold 3 Epoch 014; Train loss: 0.1342; Time: 00:00:02
Fold 3 Epoch 015; Train loss: 0.1316; Time: 00:00:02
Fold 3 Epoch 016; Train loss: 0.1324; Time: 00:00:02
Fold 3 Epoch 017; Train loss: 0.1285; Time: 00:00:02
Fold 3 Epoch 018; Train loss: 0.1240; Time: 00:00:02
Fold 3 Epoch 019; Train loss: 0.1238; Time: 00:00:02
Fold 3 Epoch 020; Train loss: 0.1227; Time: 00:00:02
Fold 3: Evaluation at Epoch 20
HR@5       NDCG@5     HR@10      NDCG@10   
0.574503   0.471354   0.718543   0.517883  
Loss: 0.1616
Fold 3 Epoch 021; Train loss: 0.1188; Time: 00:00:01
Fold 3 Epoch 022; Train loss: 0.1144; Time: 00:00:02
Fold 3 Epoch 023; Train loss: 0.1145; Time: 00:00:02
Fold 3 Epoch 024; Train loss: 0.1130; Time: 00:00:02
Fold 3 Epoch 025; Train loss: 0.1097; Time: 00:00:02
Fold 3 Epoch 026; Train loss: 0.1101; Time: 00:00:02
Fold 3 Epoch 027; Train loss: 0.1074; Time: 00:00:02
Fold 3 Epoch 028; Train loss: 0.1023; Time: 00:00:02
Fold 3 Epoch 029; Train loss: 0.1004; Time: 00:00:02
Fold 3 Epoch 030; Train loss: 0.0985; Time: 00:00:02
Fold 3: Evaluation at Epoch 30
HR@5       NDCG@5     HR@10      NDCG@10   
0.569536   0.461886   0.710265   0.506871  
Loss: 0.1293
Fold 3 Epoch 031; Train loss: 0.0973; Time: 00:00:02
Fold 3 Epoch 032; Train loss: 0.0974; Time: 00:00:02
Fold 3 Epoch 033; Train loss: 0.0940; Time: 00:00:02
Fold 3 Epoch 034; Train loss: 0.0907; Time: 00:00:02
Fold 3 Epoch 035; Train loss: 0.0903; Time: 00:00:02
Fold 3 Epoch 036; Train loss: 0.0879; Time: 00:00:02
Fold 3 Epoch 037; Train loss: 0.0864; Time: 00:00:02
Fold 3 Epoch 038; Train loss: 0.0852; Time: 00:00:02
Fold 3 Epoch 039; Train loss: 0.0812; Time: 00:00:02
Fold 3 Epoch 040; Train loss: 0.0787; Time: 00:00:02
Fold 3: Evaluation at Epoch 40
HR@5       NDCG@5     HR@10      NDCG@10   
0.561258   0.411016   0.731788   0.466196  
Loss: 0.1111
Fold 3 Epoch 041; Train loss: 0.0778; Time: 00:00:02
Fold 3 Epoch 042; Train loss: 0.0752; Time: 00:00:02
Fold 3 Epoch 043; Train loss: 0.0756; Time: 00:00:02
Fold 3 Epoch 044; Train loss: 0.0735; Time: 00:00:02
Fold 3 Epoch 045; Train loss: 0.0708; Time: 00:00:02
Fold 3 Epoch 046; Train loss: 0.0687; Time: 00:00:02
Fold 3 Epoch 047; Train loss: 0.0657; Time: 00:00:02
Fold 3 Epoch 048; Train loss: 0.0655; Time: 00:00:02
Fold 3 Epoch 049; Train loss: 0.0629; Time: 00:00:02
Fold 3 Epoch 050; Train loss: 0.0618; Time: 00:00:03
Fold 3: Evaluation at Epoch 50
HR@5       NDCG@5     HR@10      NDCG@10   
0.503311   0.404064   0.660596   0.453985  
Loss: 0.0858
Fold 3 Epoch 051; Train loss: 0.0584; Time: 00:00:02
Fold 3 Epoch 052; Train loss: 0.0571; Time: 00:00:02
Fold 3 Epoch 053; Train loss: 0.0553; Time: 00:00:02
Fold 3 Epoch 054; Train loss: 0.0541; Time: 00:00:02
Fold 3 Epoch 055; Train loss: 0.0526; Time: 00:00:02
Fold 3 Epoch 056; Train loss: 0.0496; Time: 00:00:02
Fold 3 Epoch 057; Train loss: 0.0486; Time: 00:00:02
Fold 3 Epoch 058; Train loss: 0.0463; Time: 00:00:02
Fold 3 Epoch 059; Train loss: 0.0449; Time: 00:00:02
Fold 3 Epoch 060; Train loss: 0.0417; Time: 00:00:02
Fold 3: Evaluation at Epoch 60
HR@5       NDCG@5     HR@10      NDCG@10   
0.458609   0.299958   0.649007   0.359988  
Loss: 0.0616
Fold 3 Epoch 061; Train loss: 0.0408; Time: 00:00:02
Fold 3 Epoch 062; Train loss: 0.0392; Time: 00:00:02
Fold 3 Epoch 063; Train loss: 0.0373; Time: 00:00:02
Fold 3 Epoch 064; Train loss: 0.0355; Time: 00:00:02
Fold 3 Epoch 065; Train loss: 0.0350; Time: 00:00:02
Fold 3 Epoch 066; Train loss: 0.0324; Time: 00:00:02
Fold 3 Epoch 067; Train loss: 0.0303; Time: 00:00:02
Fold 3 Epoch 068; Train loss: 0.0300; Time: 00:00:02
Fold 3 Epoch 069; Train loss: 0.0288; Time: 00:00:01
Fold 3 Epoch 070; Train loss: 0.0273; Time: 00:00:02
Fold 3: Evaluation at Epoch 70
HR@5       NDCG@5     HR@10      NDCG@10   
0.548013   0.297676   0.832781   0.387424  
Loss: 0.0395
Fold 3 Epoch 071; Train loss: 0.0263; Time: 00:00:01
Fold 3 Epoch 072; Train loss: 0.0249; Time: 00:00:02
Fold 3 Epoch 073; Train loss: 0.0236; Time: 00:00:02
Fold 3 Epoch 074; Train loss: 0.0225; Time: 00:00:01
Fold 3 Epoch 075; Train loss: 0.0213; Time: 00:00:02
Fold 3 Epoch 076; Train loss: 0.0195; Time: 00:00:02
Fold 3 Epoch 077; Train loss: 0.0193; Time: 00:00:02
Fold 3 Epoch 078; Train loss: 0.0181; Time: 00:00:02
Fold 3 Epoch 079; Train loss: 0.0169; Time: 00:00:02
Fold 3 Epoch 080; Train loss: 0.0158; Time: 00:00:02
Fold 3: Evaluation at Epoch 80
HR@5       NDCG@5     HR@10      NDCG@10   
0.453642   0.210359   0.917219   0.357719  
Loss: 0.0234
Fold 3 Epoch 081; Train loss: 0.0147; Time: 00:00:02
Fold 3 Epoch 082; Train loss: 0.0140; Time: 00:00:02
Fold 3 Epoch 083; Train loss: 0.0130; Time: 00:00:02
Fold 3 Epoch 084; Train loss: 0.0121; Time: 00:00:02
Fold 3 Epoch 085; Train loss: 0.0116; Time: 00:00:02
Fold 3 Epoch 086; Train loss: 0.0106; Time: 00:00:02
Fold 3 Epoch 087; Train loss: 0.0098; Time: 00:00:02
Fold 3 Epoch 088; Train loss: 0.0090; Time: 00:00:02
Fold 3 Epoch 089; Train loss: 0.0084; Time: 00:00:02
Fold 3 Epoch 090; Train loss: 0.0077; Time: 00:00:02
Fold 3: Evaluation at Epoch 90
HR@5       NDCG@5     HR@10      NDCG@10   
0.349338   0.168865   0.927152   0.353698  
Loss: 0.0140
Fold 3 Epoch 091; Train loss: 0.0071; Time: 00:00:03
Fold 3 Epoch 092; Train loss: 0.0064; Time: 00:00:02
Fold 3 Epoch 093; Train loss: 0.0060; Time: 00:00:02
Fold 3 Epoch 094; Train loss: 0.0055; Time: 00:00:02
Fold 3 Epoch 095; Train loss: 0.0050; Time: 00:00:02
Fold 3 Epoch 096; Train loss: 0.0046; Time: 00:00:02
Fold 3 Epoch 097; Train loss: 0.0042; Time: 00:00:02
Fold 3 Epoch 098; Train loss: 0.0038; Time: 00:00:02
Fold 3 Epoch 099; Train loss: 0.0035; Time: 00:00:02
Fold 3 Epoch 100; Train loss: 0.0033; Time: 00:00:02
Fold 3: Evaluation at Epoch 100
HR@5       NDCG@5     HR@10      NDCG@10   
0.322848   0.161967   0.922185   0.357303  
Loss: 0.0085
[eps candidate 0.0001] Fold 3: HR@10 = [np.float64(0.6970198675496688), np.float64(0.7185430463576159), np.float64(0.7102649006622517), np.float64(0.7317880794701986), np.float64(0.6605960264900662), np.float64(0.6490066225165563), np.float64(0.8327814569536424), np.float64(0.9172185430463576), np.float64(0.9271523178807947), np.float64(0.9221854304635762)]
Tuning eps:  60%|██████    | 3/5 [13:02<08:42, 261.05s/it]
========== Fold 3 ==========
INFO:root:Using statics: seq_size = 10, genre_vocab_size = 18
Fold 3 Epoch 001; Train loss: 0.6028; Time: 00:00:01
Fold 3 Epoch 002; Train loss: 0.2692; Time: 00:00:01
Fold 3 Epoch 003; Train loss: 0.1968; Time: 00:00:02
Fold 3 Epoch 004; Train loss: 0.1727; Time: 00:00:02
Fold 3 Epoch 005; Train loss: 0.1579; Time: 00:00:02
Fold 3 Epoch 006; Train loss: 0.1506; Time: 00:00:02
Fold 3 Epoch 007; Train loss: 0.1439; Time: 00:00:02
Fold 3 Epoch 008; Train loss: 0.1385; Time: 00:00:02
Fold 3 Epoch 009; Train loss: 0.1344; Time: 00:00:02
Fold 3 Epoch 010; Train loss: 0.1343; Time: 00:00:02
Fold 3: Evaluation at Epoch 10
HR@5       NDCG@5     HR@10      NDCG@10   
0.589404   0.489006   0.697020   0.523462  
Loss: 0.1711
Fold 3 Epoch 011; Train loss: 0.1272; Time: 00:00:02
Fold 3 Epoch 012; Train loss: 0.1250; Time: 00:00:02
Fold 3 Epoch 013; Train loss: 0.1243; Time: 00:00:02
Fold 3 Epoch 014; Train loss: 0.1184; Time: 00:00:02
Fold 3 Epoch 015; Train loss: 0.1160; Time: 00:00:02
Fold 3 Epoch 016; Train loss: 0.1104; Time: 00:00:02
Fold 3 Epoch 017; Train loss: 0.1108; Time: 00:00:02
Fold 3 Epoch 018; Train loss: 0.1071; Time: 00:00:02
Fold 3 Epoch 019; Train loss: 0.1038; Time: 00:00:02
Fold 3 Epoch 020; Train loss: 0.1021; Time: 00:00:02
Fold 3: Evaluation at Epoch 20
HR@5       NDCG@5     HR@10      NDCG@10   
0.529801   0.409816   0.673841   0.455935  
Loss: 0.1457
Fold 3 Epoch 021; Train loss: 0.0988; Time: 00:00:01
Fold 3 Epoch 022; Train loss: 0.0970; Time: 00:00:02
Fold 3 Epoch 023; Train loss: 0.0930; Time: 00:00:02
Fold 3 Epoch 024; Train loss: 0.0931; Time: 00:00:02
Fold 3 Epoch 025; Train loss: 0.0894; Time: 00:00:02
Fold 3 Epoch 026; Train loss: 0.0877; Time: 00:00:02
Fold 3 Epoch 027; Train loss: 0.0845; Time: 00:00:02
Fold 3 Epoch 028; Train loss: 0.0818; Time: 00:00:02
Fold 3 Epoch 029; Train loss: 0.0804; Time: 00:00:02
Fold 3 Epoch 030; Train loss: 0.0762; Time: 00:00:02
Fold 3: Evaluation at Epoch 30
HR@5       NDCG@5     HR@10      NDCG@10   
0.660596   0.548032   0.781457   0.586848  
Loss: 0.1059
Fold 3 Epoch 031; Train loss: 0.0763; Time: 00:00:02
Fold 3 Epoch 032; Train loss: 0.0694; Time: 00:00:02
Fold 3 Epoch 033; Train loss: 0.0680; Time: 00:00:02
Fold 3 Epoch 034; Train loss: 0.0655; Time: 00:00:02
Fold 3 Epoch 035; Train loss: 0.0664; Time: 00:00:02
Fold 3 Epoch 036; Train loss: 0.0621; Time: 00:00:02
Fold 3 Epoch 037; Train loss: 0.0605; Time: 00:00:02
Fold 3 Epoch 038; Train loss: 0.0601; Time: 00:00:02
Fold 3 Epoch 039; Train loss: 0.0561; Time: 00:00:02
Fold 3 Epoch 040; Train loss: 0.0552; Time: 00:00:02
Fold 3: Evaluation at Epoch 40
HR@5       NDCG@5     HR@10      NDCG@10   
0.612583   0.482643   0.764901   0.532397  
Loss: 0.0761
Fold 3 Epoch 041; Train loss: 0.0540; Time: 00:00:02
Fold 3 Epoch 042; Train loss: 0.0514; Time: 00:00:02
Fold 3 Epoch 043; Train loss: 0.0479; Time: 00:00:02
Fold 3 Epoch 044; Train loss: 0.0472; Time: 00:00:02
Fold 3 Epoch 045; Train loss: 0.0454; Time: 00:00:02
Fold 3 Epoch 046; Train loss: 0.0438; Time: 00:00:02
Fold 3 Epoch 047; Train loss: 0.0415; Time: 00:00:02
Fold 3 Epoch 048; Train loss: 0.0404; Time: 00:00:02
Fold 3 Epoch 049; Train loss: 0.0377; Time: 00:00:02
Fold 3 Epoch 050; Train loss: 0.0364; Time: 00:00:02
Fold 3: Evaluation at Epoch 50
HR@5       NDCG@5     HR@10      NDCG@10   
0.615894   0.453034   0.758278   0.499311  
Loss: 0.0532
Fold 3 Epoch 051; Train loss: 0.0342; Time: 00:00:02
Fold 3 Epoch 052; Train loss: 0.0331; Time: 00:00:02
Fold 3 Epoch 053; Train loss: 0.0321; Time: 00:00:02
Fold 3 Epoch 054; Train loss: 0.0299; Time: 00:00:02
Fold 3 Epoch 055; Train loss: 0.0288; Time: 00:00:02
Fold 3 Epoch 056; Train loss: 0.0277; Time: 00:00:02
Fold 3 Epoch 057; Train loss: 0.0258; Time: 00:00:02
Fold 3 Epoch 058; Train loss: 0.0243; Time: 00:00:02
Fold 3 Epoch 059; Train loss: 0.0237; Time: 00:00:02
Fold 3 Epoch 060; Train loss: 0.0217; Time: 00:00:02
Fold 3: Evaluation at Epoch 60
HR@5       NDCG@5     HR@10      NDCG@10   
0.605960   0.417758   0.849338   0.497470  
Loss: 0.0339
Fold 3 Epoch 061; Train loss: 0.0208; Time: 00:00:02
Fold 3 Epoch 062; Train loss: 0.0193; Time: 00:00:02
Fold 3 Epoch 063; Train loss: 0.0176; Time: 00:00:02
Fold 3 Epoch 064; Train loss: 0.0164; Time: 00:00:02
Fold 3 Epoch 065; Train loss: 0.0154; Time: 00:00:02
Fold 3 Epoch 066; Train loss: 0.0134; Time: 00:00:02
Fold 3 Epoch 067; Train loss: 0.0127; Time: 00:00:02
Fold 3 Epoch 068; Train loss: 0.0111; Time: 00:00:02
Fold 3 Epoch 069; Train loss: 0.0098; Time: 00:00:02
Fold 3 Epoch 070; Train loss: 0.0088; Time: 00:00:02
Fold 3: Evaluation at Epoch 70
HR@5       NDCG@5     HR@10      NDCG@10   
0.591060   0.301840   0.880795   0.401226  
Loss: 0.0139
Fold 3 Epoch 071; Train loss: 0.0076; Time: 00:00:02
Fold 3 Epoch 072; Train loss: 0.0064; Time: 00:00:01
Fold 3 Epoch 073; Train loss: 0.0056; Time: 00:00:01
Fold 3 Epoch 074; Train loss: 0.0048; Time: 00:00:02
Fold 3 Epoch 075; Train loss: 0.0042; Time: 00:00:02
Fold 3 Epoch 076; Train loss: 0.0035; Time: 00:00:02
Fold 3 Epoch 077; Train loss: 0.0030; Time: 00:00:02
Fold 3 Epoch 078; Train loss: 0.0025; Time: 00:00:02
Fold 3 Epoch 079; Train loss: 0.0022; Time: 00:00:02
Fold 3 Epoch 080; Train loss: 0.0019; Time: 00:00:02
Fold 3: Evaluation at Epoch 80
HR@5       NDCG@5     HR@10      NDCG@10   
0.789735   0.361158   0.865894   0.385708  
Loss: 0.0055
Fold 3 Epoch 081; Train loss: 0.0016; Time: 00:00:02
Fold 3 Epoch 082; Train loss: 0.0014; Time: 00:00:02
Fold 3 Epoch 083; Train loss: 0.0012; Time: 00:00:02
Fold 3 Epoch 084; Train loss: 0.0011; Time: 00:00:02
Fold 3 Epoch 085; Train loss: 0.0010; Time: 00:00:01
Fold 3 Epoch 086; Train loss: 0.0008; Time: 00:00:02
Fold 3 Epoch 087; Train loss: 0.0008; Time: 00:00:02
Fold 3 Epoch 088; Train loss: 0.0007; Time: 00:00:02
Fold 3 Epoch 089; Train loss: 0.0006; Time: 00:00:02
Fold 3 Epoch 090; Train loss: 0.0005; Time: 00:00:02
Fold 3: Evaluation at Epoch 90
HR@5       NDCG@5     HR@10      NDCG@10   
0.534768   0.290565   0.889073   0.412751  
Loss: 0.0029
Fold 3 Epoch 091; Train loss: 0.0005; Time: 00:00:01
Fold 3 Epoch 092; Train loss: 0.0005; Time: 00:00:02
Fold 3 Epoch 093; Train loss: 0.0004; Time: 00:00:02
Fold 3 Epoch 094; Train loss: 0.0004; Time: 00:00:02
Fold 3 Epoch 095; Train loss: 0.0004; Time: 00:00:02
Fold 3 Epoch 096; Train loss: 0.0003; Time: 00:00:02
Fold 3 Epoch 097; Train loss: 0.0003; Time: 00:00:02
Fold 3 Epoch 098; Train loss: 0.0003; Time: 00:00:02
Fold 3 Epoch 099; Train loss: 0.0003; Time: 00:00:02
Fold 3 Epoch 100; Train loss: 0.0002; Time: 00:00:02
Fold 3: Evaluation at Epoch 100
HR@5       NDCG@5     HR@10      NDCG@10   
0.344371   0.182839   0.870861   0.342070  
Loss: 0.0017
[eps candidate 1e-08] Fold 3: HR@10 = [np.float64(0.6970198675496688), np.float64(0.6738410596026491), np.float64(0.7814569536423841), np.float64(0.7649006622516556), np.float64(0.7582781456953642), np.float64(0.8493377483443708), np.float64(0.8807947019867549), np.float64(0.8658940397350994), np.float64(0.8890728476821192), np.float64(0.8708609271523179)]
Tuning eps:  80%|████████  | 4/5 [17:22<04:20, 260.53s/it]
========== Fold 3 ==========
INFO:root:Using statics: seq_size = 10, genre_vocab_size = 18
Fold 3 Epoch 001; Train loss: 0.5589; Time: 00:00:02
Fold 3 Epoch 002; Train loss: 0.2645; Time: 00:00:02
Fold 3 Epoch 003; Train loss: 0.1917; Time: 00:00:02
Fold 3 Epoch 004; Train loss: 0.1680; Time: 00:00:02
Fold 3 Epoch 005; Train loss: 0.1490; Time: 00:00:02
Fold 3 Epoch 006; Train loss: 0.1426; Time: 00:00:02
Fold 3 Epoch 007; Train loss: 0.1363; Time: 00:00:02
Fold 3 Epoch 008; Train loss: 0.1307; Time: 00:00:02
Fold 3 Epoch 009; Train loss: 0.1266; Time: 00:00:02
Fold 3 Epoch 010; Train loss: 0.1254; Time: 00:00:02
Fold 3: Evaluation at Epoch 10
HR@5       NDCG@5     HR@10      NDCG@10   
0.544702   0.417738   0.698675   0.467875  
Loss: 0.1742
Fold 3 Epoch 011; Train loss: 0.1224; Time: 00:00:02
Fold 3 Epoch 012; Train loss: 0.1165; Time: 00:00:02
Fold 3 Epoch 013; Train loss: 0.1175; Time: 00:00:01
Fold 3 Epoch 014; Train loss: 0.1119; Time: 00:00:02
Fold 3 Epoch 015; Train loss: 0.1096; Time: 00:00:01
Fold 3 Epoch 016; Train loss: 0.1079; Time: 00:00:02
Fold 3 Epoch 017; Train loss: 0.1052; Time: 00:00:02
Fold 3 Epoch 018; Train loss: 0.1030; Time: 00:00:02
Fold 3 Epoch 019; Train loss: 0.0986; Time: 00:00:02
Fold 3 Epoch 020; Train loss: 0.0969; Time: 00:00:02
Fold 3: Evaluation at Epoch 20
HR@5       NDCG@5     HR@10      NDCG@10   
0.597682   0.411243   0.774834   0.468521  
Loss: 0.1285
Fold 3 Epoch 021; Train loss: 0.0967; Time: 00:00:02
Fold 3 Epoch 022; Train loss: 0.0938; Time: 00:00:02
Fold 3 Epoch 023; Train loss: 0.0880; Time: 00:00:02
Fold 3 Epoch 024; Train loss: 0.0869; Time: 00:00:02
Fold 3 Epoch 025; Train loss: 0.0851; Time: 00:00:02
Fold 3 Epoch 026; Train loss: 0.0823; Time: 00:00:02
Fold 3 Epoch 027; Train loss: 0.0796; Time: 00:00:02
Fold 3 Epoch 028; Train loss: 0.0783; Time: 00:00:02
Fold 3 Epoch 029; Train loss: 0.0742; Time: 00:00:02
Fold 3 Epoch 030; Train loss: 0.0726; Time: 00:00:02
Fold 3: Evaluation at Epoch 30
HR@5       NDCG@5     HR@10      NDCG@10   
0.548013   0.393107   0.723510   0.449386  
Loss: 0.0949
Fold 3 Epoch 031; Train loss: 0.0704; Time: 00:00:02
Fold 3 Epoch 032; Train loss: 0.0673; Time: 00:00:02
Fold 3 Epoch 033; Train loss: 0.0666; Time: 00:00:02
Fold 3 Epoch 034; Train loss: 0.0629; Time: 00:00:01
Fold 3 Epoch 035; Train loss: 0.0618; Time: 00:00:02
Fold 3 Epoch 036; Train loss: 0.0595; Time: 00:00:02
Fold 3 Epoch 037; Train loss: 0.0573; Time: 00:00:02
Fold 3 Epoch 038; Train loss: 0.0556; Time: 00:00:02
Fold 3 Epoch 039; Train loss: 0.0536; Time: 00:00:02
Fold 3 Epoch 040; Train loss: 0.0517; Time: 00:00:02
Fold 3: Evaluation at Epoch 40
HR@5       NDCG@5     HR@10      NDCG@10   
0.591060   0.400399   0.771523   0.457897  
Loss: 0.0678
Fold 3 Epoch 041; Train loss: 0.0481; Time: 00:00:02
Fold 3 Epoch 042; Train loss: 0.0455; Time: 00:00:02
Fold 3 Epoch 043; Train loss: 0.0448; Time: 00:00:02
Fold 3 Epoch 044; Train loss: 0.0424; Time: 00:00:02
Fold 3 Epoch 045; Train loss: 0.0409; Time: 00:00:02
Fold 3 Epoch 046; Train loss: 0.0383; Time: 00:00:02
Fold 3 Epoch 047; Train loss: 0.0360; Time: 00:00:02
Fold 3 Epoch 048; Train loss: 0.0343; Time: 00:00:02
Fold 3 Epoch 049; Train loss: 0.0325; Time: 00:00:02
Fold 3 Epoch 050; Train loss: 0.0312; Time: 00:00:02
Fold 3: Evaluation at Epoch 50
HR@5       NDCG@5     HR@10      NDCG@10   
0.629139   0.438620   0.761589   0.480721  
Loss: 0.0430
Fold 3 Epoch 051; Train loss: 0.0292; Time: 00:00:01
Fold 3 Epoch 052; Train loss: 0.0278; Time: 00:00:02
Fold 3 Epoch 053; Train loss: 0.0258; Time: 00:00:02
Fold 3 Epoch 054; Train loss: 0.0253; Time: 00:00:02
Fold 3 Epoch 055; Train loss: 0.0228; Time: 00:00:02
Fold 3 Epoch 056; Train loss: 0.0220; Time: 00:00:02
Fold 3 Epoch 057; Train loss: 0.0197; Time: 00:00:02
Fold 3 Epoch 058; Train loss: 0.0184; Time: 00:00:02
Fold 3 Epoch 059; Train loss: 0.0173; Time: 00:00:02
Fold 3 Epoch 060; Train loss: 0.0157; Time: 00:00:02
Fold 3: Evaluation at Epoch 60
HR@5       NDCG@5     HR@10      NDCG@10   
0.622517   0.345183   0.761589   0.388637  
Loss: 0.0248
Fold 3 Epoch 061; Train loss: 0.0140; Time: 00:00:02
Fold 3 Epoch 062; Train loss: 0.0127; Time: 00:00:02
Fold 3 Epoch 063; Train loss: 0.0115; Time: 00:00:02
Fold 3 Epoch 064; Train loss: 0.0104; Time: 00:00:02
Fold 3 Epoch 065; Train loss: 0.0096; Time: 00:00:02
Fold 3 Epoch 066; Train loss: 0.0084; Time: 00:00:02
Fold 3 Epoch 067; Train loss: 0.0077; Time: 00:00:02
Fold 3 Epoch 068; Train loss: 0.0069; Time: 00:00:02
Fold 3 Epoch 069; Train loss: 0.0064; Time: 00:00:02
Fold 3 Epoch 070; Train loss: 0.0057; Time: 00:00:02
Fold 3: Evaluation at Epoch 70
HR@5       NDCG@5     HR@10      NDCG@10   
0.541391   0.357206   0.880795   0.463453  
Loss: 0.0098
Fold 3 Epoch 071; Train loss: 0.0050; Time: 00:00:02
Fold 3 Epoch 072; Train loss: 0.0045; Time: 00:00:02
Fold 3 Epoch 073; Train loss: 0.0039; Time: 00:00:02
Fold 3 Epoch 074; Train loss: 0.0034; Time: 00:00:01
Fold 3 Epoch 075; Train loss: 0.0030; Time: 00:00:02
Fold 3 Epoch 076; Train loss: 0.0027; Time: 00:00:02
Fold 3 Epoch 077; Train loss: 0.0023; Time: 00:00:02
Fold 3 Epoch 078; Train loss: 0.0020; Time: 00:00:02
Fold 3 Epoch 079; Train loss: 0.0017; Time: 00:00:02
Fold 3 Epoch 080; Train loss: 0.0015; Time: 00:00:02
Fold 3: Evaluation at Epoch 80
HR@5       NDCG@5     HR@10      NDCG@10   
0.256623   0.130341   0.877483   0.337692  
Loss: 0.0046
Fold 3 Epoch 081; Train loss: 0.0014; Time: 00:00:02
Fold 3 Epoch 082; Train loss: 0.0012; Time: 00:00:02
Fold 3 Epoch 083; Train loss: 0.0011; Time: 00:00:01
Fold 3 Epoch 084; Train loss: 0.0010; Time: 00:00:02
Fold 3 Epoch 085; Train loss: 0.0009; Time: 00:00:02
Fold 3 Epoch 086; Train loss: 0.0008; Time: 00:00:02
Fold 3 Epoch 087; Train loss: 0.0007; Time: 00:00:02
Fold 3 Epoch 088; Train loss: 0.0006; Time: 00:00:02
Fold 3 Epoch 089; Train loss: 0.0006; Time: 00:00:02
Fold 3 Epoch 090; Train loss: 0.0006; Time: 00:00:02
Fold 3: Evaluation at Epoch 90
HR@5       NDCG@5     HR@10      NDCG@10   
0.059603   0.037801   0.879139   0.308807  
Loss: 0.0027
Fold 3 Epoch 091; Train loss: 0.0005; Time: 00:00:02
Fold 3 Epoch 092; Train loss: 0.0005; Time: 00:00:02
Fold 3 Epoch 093; Train loss: 0.0004; Time: 00:00:02
Fold 3 Epoch 094; Train loss: 0.0004; Time: 00:00:02
Fold 3 Epoch 095; Train loss: 0.0004; Time: 00:00:02
Fold 3 Epoch 096; Train loss: 0.0003; Time: 00:00:02
Fold 3 Epoch 097; Train loss: 0.0003; Time: 00:00:02
Fold 3 Epoch 098; Train loss: 0.0003; Time: 00:00:03
Fold 3 Epoch 099; Train loss: 0.0003; Time: 00:00:02
Fold 3 Epoch 100; Train loss: 0.0003; Time: 00:00:02
Fold 3: Evaluation at Epoch 100
HR@5       NDCG@5     HR@10      NDCG@10   
0.261589   0.121688   0.879139   0.310878  
Loss: 0.0017
[eps candidate 5e-16] Fold 3: HR@10 = [np.float64(0.6986754966887417), np.float64(0.7748344370860927), np.float64(0.7235099337748344), np.float64(0.7715231788079471), np.float64(0.7615894039735099), np.float64(0.7615894039735099), np.float64(0.8807947019867549), np.float64(0.8774834437086093), np.float64(0.8791390728476821), np.float64(0.8791390728476821)]
Tuning eps: 100%|██████████| 5/5 [21:39<00:00, 259.36s/it]Tuning eps: 100%|██████████| 5/5 [21:39<00:00, 259.97s/it]

Best eps found: 0.0001
Tuning scheduler:   0%|          | 0/3 [00:00<?, ?it/s]
========== Fold 3 ==========
INFO:root:Using statics: seq_size = 10, genre_vocab_size = 18
Fold 3 Epoch 001; Train loss: 0.6236; Time: 00:00:02
Fold 3 Epoch 002; Train loss: 0.2943; Time: 00:00:01
Fold 3 Epoch 003; Train loss: 0.2224; Time: 00:00:01
Fold 3 Epoch 004; Train loss: 0.1847; Time: 00:00:02
Fold 3 Epoch 005; Train loss: 0.1715; Time: 00:00:02
Fold 3 Epoch 006; Train loss: 0.1585; Time: 00:00:02
Fold 3 Epoch 007; Train loss: 0.1508; Time: 00:00:02
Fold 3 Epoch 008; Train loss: 0.1429; Time: 00:00:02
Fold 3 Epoch 009; Train loss: 0.1455; Time: 00:00:01
Fold 3 Epoch 010; Train loss: 0.1383; Time: 00:00:02
Fold 3: Evaluation at Epoch 10
HR@5       NDCG@5     HR@10      NDCG@10   
0.680464   0.512418   0.799669   0.551040  
Loss: 0.1973
Fold 3 Epoch 011; Train loss: 0.1348; Time: 00:00:02
Fold 3 Epoch 012; Train loss: 0.1299; Time: 00:00:02
Fold 3 Epoch 013; Train loss: 0.1320; Time: 00:00:02
Fold 3 Epoch 014; Train loss: 0.1278; Time: 00:00:03
Fold 3 Epoch 015; Train loss: 0.1237; Time: 00:00:02
Fold 3 Epoch 016; Train loss: 0.1225; Time: 00:00:02
Fold 3 Epoch 017; Train loss: 0.1200; Time: 00:00:02
Fold 3 Epoch 018; Train loss: 0.1183; Time: 00:00:02
Fold 3 Epoch 019; Train loss: 0.1172; Time: 00:00:02
Fold 3 Epoch 020; Train loss: 0.1177; Time: 00:00:02
Fold 3: Evaluation at Epoch 20
HR@5       NDCG@5     HR@10      NDCG@10   
0.731788   0.555096   0.826159   0.585768  
Loss: 0.1583
Fold 3 Epoch 021; Train loss: 0.1148; Time: 00:00:02
Fold 3 Epoch 022; Train loss: 0.1107; Time: 00:00:02
Fold 3 Epoch 023; Train loss: 0.1071; Time: 00:00:02
Fold 3 Epoch 024; Train loss: 0.1061; Time: 00:00:02
Fold 3 Epoch 025; Train loss: 0.1037; Time: 00:00:02
Fold 3 Epoch 026; Train loss: 0.1028; Time: 00:00:03
Fold 3 Epoch 027; Train loss: 0.1009; Time: 00:00:02
Fold 3 Epoch 028; Train loss: 0.0973; Time: 00:00:02
Fold 3 Epoch 029; Train loss: 0.0948; Time: 00:00:02
Fold 3 Epoch 030; Train loss: 0.0969; Time: 00:00:02
Fold 3: Evaluation at Epoch 30
HR@5       NDCG@5     HR@10      NDCG@10   
0.653974   0.468956   0.834437   0.528251  
Loss: 0.1300
Fold 3 Epoch 031; Train loss: 0.0924; Time: 00:00:01
Fold 3 Epoch 032; Train loss: 0.0891; Time: 00:00:01
Fold 3 Epoch 033; Train loss: 0.0898; Time: 00:00:02
Fold 3 Epoch 034; Train loss: 0.0851; Time: 00:00:01
Fold 3 Epoch 035; Train loss: 0.0831; Time: 00:00:02
Fold 3 Epoch 036; Train loss: 0.0826; Time: 00:00:02
Fold 3 Epoch 037; Train loss: 0.0813; Time: 00:00:02
Fold 3 Epoch 038; Train loss: 0.0773; Time: 00:00:02
Fold 3 Epoch 039; Train loss: 0.0770; Time: 00:00:02
Fold 3 Epoch 040; Train loss: 0.0732; Time: 00:00:02
Fold 3: Evaluation at Epoch 40
HR@5       NDCG@5     HR@10      NDCG@10   
0.635762   0.455769   0.814570   0.513521  
Loss: 0.1133
Fold 3 Epoch 041; Train loss: 0.0718; Time: 00:00:02
Fold 3 Epoch 042; Train loss: 0.0704; Time: 00:00:02
Fold 3 Epoch 043; Train loss: 0.0673; Time: 00:00:02
Fold 3 Epoch 044; Train loss: 0.0663; Time: 00:00:03
Fold 3 Epoch 045; Train loss: 0.0642; Time: 00:00:02
Fold 3 Epoch 046; Train loss: 0.0625; Time: 00:00:02
Fold 3 Epoch 047; Train loss: 0.0609; Time: 00:00:02
Fold 3 Epoch 048; Train loss: 0.0573; Time: 00:00:02
Fold 3 Epoch 049; Train loss: 0.0564; Time: 00:00:02
Fold 3 Epoch 050; Train loss: 0.0552; Time: 00:00:02
Fold 3: Evaluation at Epoch 50
HR@5       NDCG@5     HR@10      NDCG@10   
0.587748   0.415907   0.836093   0.498290  
Loss: 0.0830
Fold 3 Epoch 051; Train loss: 0.0528; Time: 00:00:02
Fold 3 Epoch 052; Train loss: 0.0505; Time: 00:00:02
Fold 3 Epoch 053; Train loss: 0.0493; Time: 00:00:02
Fold 3 Epoch 054; Train loss: 0.0490; Time: 00:00:02
Fold 3 Epoch 055; Train loss: 0.0466; Time: 00:00:02
Fold 3 Epoch 056; Train loss: 0.0443; Time: 00:00:02
Fold 3 Epoch 057; Train loss: 0.0428; Time: 00:00:02
Fold 3 Epoch 058; Train loss: 0.0416; Time: 00:00:02
Fold 3 Epoch 059; Train loss: 0.0398; Time: 00:00:02
Fold 3 Epoch 060; Train loss: 0.0375; Time: 00:00:02
Fold 3: Evaluation at Epoch 60
HR@5       NDCG@5     HR@10      NDCG@10   
0.552980   0.370098   0.807947   0.454748  
Loss: 0.0629
Fold 3 Epoch 061; Train loss: 0.0371; Time: 00:00:02
Fold 3 Epoch 062; Train loss: 0.0352; Time: 00:00:02
Fold 3 Epoch 063; Train loss: 0.0337; Time: 00:00:02
Fold 3 Epoch 064; Train loss: 0.0321; Time: 00:00:02
Fold 3 Epoch 065; Train loss: 0.0302; Time: 00:00:02
Fold 3 Epoch 066; Train loss: 0.0297; Time: 00:00:02
Fold 3 Epoch 067; Train loss: 0.0280; Time: 00:00:02
Fold 3 Epoch 068; Train loss: 0.0270; Time: 00:00:02
Fold 3 Epoch 069; Train loss: 0.0254; Time: 00:00:02
Fold 3 Epoch 070; Train loss: 0.0240; Time: 00:00:02
Fold 3: Evaluation at Epoch 70
HR@5       NDCG@5     HR@10      NDCG@10   
0.524834   0.341751   0.821192   0.439178  
Loss: 0.0447
Fold 3 Epoch 071; Train loss: 0.0228; Time: 00:00:02
Fold 3 Epoch 072; Train loss: 0.0214; Time: 00:00:02
Fold 3 Epoch 073; Train loss: 0.0198; Time: 00:00:02
Fold 3 Epoch 074; Train loss: 0.0195; Time: 00:00:02
Fold 3 Epoch 075; Train loss: 0.0182; Time: 00:00:02
Fold 3 Epoch 076; Train loss: 0.0164; Time: 00:00:02
Fold 3 Epoch 077; Train loss: 0.0155; Time: 00:00:02
Fold 3 Epoch 078; Train loss: 0.0147; Time: 00:00:02
Fold 3 Epoch 079; Train loss: 0.0137; Time: 00:00:02
Fold 3 Epoch 080; Train loss: 0.0126; Time: 00:00:01
Fold 3: Evaluation at Epoch 80
HR@5       NDCG@5     HR@10      NDCG@10   
0.468543   0.256000   0.761589   0.354590  
Loss: 0.0305
Fold 3 Epoch 081; Train loss: 0.0116; Time: 00:00:01
Fold 3 Epoch 082; Train loss: 0.0110; Time: 00:00:02
Fold 3 Epoch 083; Train loss: 0.0101; Time: 00:00:02
Fold 3 Epoch 084; Train loss: 0.0094; Time: 00:00:02
Fold 3 Epoch 085; Train loss: 0.0086; Time: 00:00:02
Fold 3 Epoch 086; Train loss: 0.0080; Time: 00:00:02
Fold 3 Epoch 087; Train loss: 0.0074; Time: 00:00:02
Fold 3 Epoch 088; Train loss: 0.0068; Time: 00:00:02
Fold 3 Epoch 089; Train loss: 0.0063; Time: 00:00:02
Fold 3 Epoch 090; Train loss: 0.0060; Time: 00:00:02
Fold 3: Evaluation at Epoch 90
HR@5       NDCG@5     HR@10      NDCG@10   
0.278146   0.135855   0.859272   0.320286  
Loss: 0.0184
Fold 3 Epoch 091; Train loss: 0.0056; Time: 00:00:02
Fold 3 Epoch 092; Train loss: 0.0052; Time: 00:00:01
Fold 3 Epoch 093; Train loss: 0.0049; Time: 00:00:02
Fold 3 Epoch 094; Train loss: 0.0046; Time: 00:00:02
Fold 3 Epoch 095; Train loss: 0.0044; Time: 00:00:02
Fold 3 Epoch 096; Train loss: 0.0041; Time: 00:00:02
Fold 3 Epoch 097; Train loss: 0.0039; Time: 00:00:02
Fold 3 Epoch 098; Train loss: 0.0036; Time: 00:00:02
Fold 3 Epoch 099; Train loss: 0.0034; Time: 00:00:02
Fold 3 Epoch 100; Train loss: 0.0032; Time: 00:00:02
Fold 3: Evaluation at Epoch 100
HR@5       NDCG@5     HR@10      NDCG@10   
0.268212   0.125874   0.884106   0.316997  
Loss: 0.0124
[scheduler candidate reduce_on_plateau] Fold 3: HR@10 = [np.float64(0.7996688741721855), np.float64(0.8261589403973509), np.float64(0.8344370860927153), np.float64(0.8145695364238411), np.float64(0.8360927152317881), np.float64(0.8079470198675497), np.float64(0.8211920529801324), np.float64(0.7615894039735099), np.float64(0.859271523178808), np.float64(0.8841059602649006)]
Tuning scheduler:  33%|███▎      | 1/3 [04:15<08:31, 255.99s/it]
========== Fold 3 ==========
INFO:root:Using statics: seq_size = 10, genre_vocab_size = 18
Fold 3 Epoch 001; Train loss: 0.6726; Time: 00:00:02
Fold 3 Epoch 002; Train loss: 0.3018; Time: 00:00:02
Fold 3 Epoch 003; Train loss: 0.2242; Time: 00:00:02
Fold 3 Epoch 004; Train loss: 0.1946; Time: 00:00:02
Fold 3 Epoch 005; Train loss: 0.1784; Time: 00:00:02
Fold 3 Epoch 006; Train loss: 0.1679; Time: 00:00:02
Fold 3 Epoch 007; Train loss: 0.1603; Time: 00:00:02
Fold 3 Epoch 008; Train loss: 0.1552; Time: 00:00:02
Fold 3 Epoch 009; Train loss: 0.1486; Time: 00:00:02
Fold 3 Epoch 010; Train loss: 0.1441; Time: 00:00:02
Fold 3: Evaluation at Epoch 10
HR@5       NDCG@5     HR@10      NDCG@10   
0.539735   0.449255   0.715232   0.504427  
Loss: 0.1844
Fold 3 Epoch 011; Train loss: 0.1389; Time: 00:00:02
Fold 3 Epoch 012; Train loss: 0.1370; Time: 00:00:02
Fold 3 Epoch 013; Train loss: 0.1350; Time: 00:00:02
Fold 3 Epoch 014; Train loss: 0.1325; Time: 00:00:02
Fold 3 Epoch 015; Train loss: 0.1302; Time: 00:00:02
Fold 3 Epoch 016; Train loss: 0.1281; Time: 00:00:02
Fold 3 Epoch 017; Train loss: 0.1244; Time: 00:00:02
Fold 3 Epoch 018; Train loss: 0.1241; Time: 00:00:02
Fold 3 Epoch 019; Train loss: 0.1217; Time: 00:00:02
Fold 3 Epoch 020; Train loss: 0.1222; Time: 00:00:02
Fold 3: Evaluation at Epoch 20
HR@5       NDCG@5     HR@10      NDCG@10   
0.478477   0.396617   0.614238   0.439670  
Loss: 0.1593
Fold 3 Epoch 021; Train loss: 0.1157; Time: 00:00:02
Fold 3 Epoch 022; Train loss: 0.1155; Time: 00:00:02
Fold 3 Epoch 023; Train loss: 0.1138; Time: 00:00:02
Fold 3 Epoch 024; Train loss: 0.1123; Time: 00:00:02
Fold 3 Epoch 025; Train loss: 0.1074; Time: 00:00:02
Fold 3 Epoch 026; Train loss: 0.1077; Time: 00:00:02
Fold 3 Epoch 027; Train loss: 0.1009; Time: 00:00:02
Fold 3 Epoch 028; Train loss: 0.1005; Time: 00:00:02
Fold 3 Epoch 029; Train loss: 0.0996; Time: 00:00:02
Fold 3 Epoch 030; Train loss: 0.0989; Time: 00:00:02
Fold 3: Evaluation at Epoch 30
HR@5       NDCG@5     HR@10      NDCG@10   
0.521523   0.423878   0.660596   0.467546  
Loss: 0.1321
Fold 3 Epoch 031; Train loss: 0.0945; Time: 00:00:01
Fold 3 Epoch 032; Train loss: 0.0941; Time: 00:00:02
Fold 3 Epoch 033; Train loss: 0.0917; Time: 00:00:02
Fold 3 Epoch 034; Train loss: 0.0877; Time: 00:00:02
Fold 3 Epoch 035; Train loss: 0.0864; Time: 00:00:02
Fold 3 Epoch 036; Train loss: 0.0842; Time: 00:00:02
Fold 3 Epoch 037; Train loss: 0.0845; Time: 00:00:02
Fold 3 Epoch 038; Train loss: 0.0793; Time: 00:00:02
Fold 3 Epoch 039; Train loss: 0.0789; Time: 00:00:02
Fold 3 Epoch 040; Train loss: 0.0787; Time: 00:00:02
Fold 3: Evaluation at Epoch 40
HR@5       NDCG@5     HR@10      NDCG@10   
0.455298   0.346581   0.629139   0.401938  
Loss: 0.1093
Fold 3 Epoch 041; Train loss: 0.0751; Time: 00:00:02
Fold 3 Epoch 042; Train loss: 0.0722; Time: 00:00:02
Fold 3 Epoch 043; Train loss: 0.0718; Time: 00:00:02
Fold 3 Epoch 044; Train loss: 0.0688; Time: 00:00:02
Fold 3 Epoch 045; Train loss: 0.0659; Time: 00:00:03
Fold 3 Epoch 046; Train loss: 0.0661; Time: 00:00:02
Fold 3 Epoch 047; Train loss: 0.0640; Time: 00:00:02
Fold 3 Epoch 048; Train loss: 0.0613; Time: 00:00:02
Fold 3 Epoch 049; Train loss: 0.0610; Time: 00:00:02
Fold 3 Epoch 050; Train loss: 0.0577; Time: 00:00:02
Fold 3: Evaluation at Epoch 50
HR@5       NDCG@5     HR@10      NDCG@10   
0.506623   0.350598   0.673841   0.403972  
Loss: 0.0906
Fold 3 Epoch 051; Train loss: 0.0560; Time: 00:00:01
Fold 3 Epoch 052; Train loss: 0.0563; Time: 00:00:02
Fold 3 Epoch 053; Train loss: 0.0545; Time: 00:00:01
Fold 3 Epoch 054; Train loss: 0.0519; Time: 00:00:02
Fold 3 Epoch 055; Train loss: 0.0499; Time: 00:00:02
Fold 3 Epoch 056; Train loss: 0.0475; Time: 00:00:02
Fold 3 Epoch 057; Train loss: 0.0452; Time: 00:00:02
Fold 3 Epoch 058; Train loss: 0.0446; Time: 00:00:02
Fold 3 Epoch 059; Train loss: 0.0434; Time: 00:00:02
Fold 3 Epoch 060; Train loss: 0.0408; Time: 00:00:02
Fold 3: Evaluation at Epoch 60
HR@5       NDCG@5     HR@10      NDCG@10   
0.432119   0.300937   0.733444   0.398891  
Loss: 0.0621
Fold 3 Epoch 061; Train loss: 0.0395; Time: 00:00:01
Fold 3 Epoch 062; Train loss: 0.0387; Time: 00:00:02
Fold 3 Epoch 063; Train loss: 0.0356; Time: 00:00:02
Fold 3 Epoch 064; Train loss: 0.0357; Time: 00:00:02
Fold 3 Epoch 065; Train loss: 0.0338; Time: 00:00:02
Fold 3 Epoch 066; Train loss: 0.0321; Time: 00:00:02
Fold 3 Epoch 067; Train loss: 0.0289; Time: 00:00:02
Fold 3 Epoch 068; Train loss: 0.0284; Time: 00:00:01
Fold 3 Epoch 069; Train loss: 0.0262; Time: 00:00:02
Fold 3 Epoch 070; Train loss: 0.0251; Time: 00:00:02
Fold 3: Evaluation at Epoch 70
HR@5       NDCG@5     HR@10      NDCG@10   
0.451987   0.305652   0.827815   0.428180  
Loss: 0.0418
Fold 3 Epoch 071; Train loss: 0.0232; Time: 00:00:02
Fold 3 Epoch 072; Train loss: 0.0222; Time: 00:00:02
Fold 3 Epoch 073; Train loss: 0.0205; Time: 00:00:02
Fold 3 Epoch 074; Train loss: 0.0192; Time: 00:00:02
Fold 3 Epoch 075; Train loss: 0.0182; Time: 00:00:02
Fold 3 Epoch 076; Train loss: 0.0168; Time: 00:00:02
Fold 3 Epoch 077; Train loss: 0.0157; Time: 00:00:02
Fold 3 Epoch 078; Train loss: 0.0144; Time: 00:00:02
Fold 3 Epoch 079; Train loss: 0.0135; Time: 00:00:02
Fold 3 Epoch 080; Train loss: 0.0124; Time: 00:00:02
Fold 3: Evaluation at Epoch 80
HR@5       NDCG@5     HR@10      NDCG@10   
0.511589   0.320769   0.882450   0.436512  
Loss: 0.0257
Fold 3 Epoch 081; Train loss: 0.0114; Time: 00:00:02
Fold 3 Epoch 082; Train loss: 0.0105; Time: 00:00:03
Fold 3 Epoch 083; Train loss: 0.0094; Time: 00:00:02
Fold 3 Epoch 084; Train loss: 0.0086; Time: 00:00:02
Fold 3 Epoch 085; Train loss: 0.0077; Time: 00:00:02
Fold 3 Epoch 086; Train loss: 0.0069; Time: 00:00:02
Fold 3 Epoch 087; Train loss: 0.0063; Time: 00:00:02
Fold 3 Epoch 088; Train loss: 0.0057; Time: 00:00:02
Fold 3 Epoch 089; Train loss: 0.0051; Time: 00:00:01
Fold 3 Epoch 090; Train loss: 0.0047; Time: 00:00:02
Fold 3: Evaluation at Epoch 90
HR@5       NDCG@5     HR@10      NDCG@10   
0.394040   0.292547   0.915563   0.465318  
Loss: 0.0162
Fold 3 Epoch 091; Train loss: 0.0042; Time: 00:00:02
Fold 3 Epoch 092; Train loss: 0.0041; Time: 00:00:02
Fold 3 Epoch 093; Train loss: 0.0036; Time: 00:00:02
Fold 3 Epoch 094; Train loss: 0.0033; Time: 00:00:02
Fold 3 Epoch 095; Train loss: 0.0032; Time: 00:00:02
Fold 3 Epoch 096; Train loss: 0.0029; Time: 00:00:02
Fold 3 Epoch 097; Train loss: 0.0028; Time: 00:00:02
Fold 3 Epoch 098; Train loss: 0.0026; Time: 00:00:02
Fold 3 Epoch 099; Train loss: 0.0024; Time: 00:00:03
Fold 3 Epoch 100; Train loss: 0.0023; Time: 00:00:02
Fold 3: Evaluation at Epoch 100
HR@5       NDCG@5     HR@10      NDCG@10   
0.306291   0.174481   0.907285   0.357577  
Loss: 0.0117
[scheduler candidate cosine] Fold 3: HR@10 = [np.float64(0.7152317880794702), np.float64(0.6142384105960265), np.float64(0.6605960264900662), np.float64(0.6291390728476821), np.float64(0.6738410596026491), np.float64(0.7334437086092715), np.float64(0.8278145695364238), np.float64(0.8824503311258278), np.float64(0.9155629139072847), np.float64(0.9072847682119205)]
Tuning scheduler:  67%|██████▋   | 2/3 [08:35<04:18, 258.13s/it]
========== Fold 3 ==========
INFO:root:Using statics: seq_size = 10, genre_vocab_size = 18
Fold 3 Epoch 001; Train loss: 0.5773; Time: 00:00:02
Fold 3 Epoch 002; Train loss: 0.2721; Time: 00:00:02
Fold 3 Epoch 003; Train loss: 0.2082; Time: 00:00:02
Fold 3 Epoch 004; Train loss: 0.1741; Time: 00:00:02
Fold 3 Epoch 005; Train loss: 0.1602; Time: 00:00:02
Fold 3 Epoch 006; Train loss: 0.1497; Time: 00:00:02
Fold 3 Epoch 007; Train loss: 0.1398; Time: 00:00:02
Fold 3 Epoch 008; Train loss: 0.1393; Time: 00:00:02
Fold 3 Epoch 009; Train loss: 0.1331; Time: 00:00:02
Fold 3 Epoch 010; Train loss: 0.1285; Time: 00:00:02
Fold 3: Evaluation at Epoch 10
HR@5       NDCG@5     HR@10      NDCG@10   
0.612583   0.462494   0.849338   0.538502  
Loss: 0.1859
Fold 3 Epoch 011; Train loss: 0.1260; Time: 00:00:01
Fold 3 Epoch 012; Train loss: 0.1244; Time: 00:00:02
Fold 3 Epoch 013; Train loss: 0.1237; Time: 00:00:02
Fold 3 Epoch 014; Train loss: 0.1214; Time: 00:00:02
Fold 3 Epoch 015; Train loss: 0.1189; Time: 00:00:02
Fold 3 Epoch 016; Train loss: 0.1166; Time: 00:00:02
Fold 3 Epoch 017; Train loss: 0.1120; Time: 00:00:02
Fold 3 Epoch 018; Train loss: 0.1094; Time: 00:00:02
Fold 3 Epoch 019; Train loss: 0.1107; Time: 00:00:02
Fold 3 Epoch 020; Train loss: 0.1074; Time: 00:00:02
Fold 3: Evaluation at Epoch 20
HR@5       NDCG@5     HR@10      NDCG@10   
0.562914   0.470210   0.726821   0.522266  
Loss: 0.1489
Fold 3 Epoch 021; Train loss: 0.1036; Time: 00:00:02
Fold 3 Epoch 022; Train loss: 0.1018; Time: 00:00:02
Fold 3 Epoch 023; Train loss: 0.1031; Time: 00:00:02
Fold 3 Epoch 024; Train loss: 0.1003; Time: 00:00:02
Fold 3 Epoch 025; Train loss: 0.0969; Time: 00:00:02
Fold 3 Epoch 026; Train loss: 0.0954; Time: 00:00:02
Fold 3 Epoch 027; Train loss: 0.0930; Time: 00:00:02
Fold 3 Epoch 028; Train loss: 0.0889; Time: 00:00:02
Fold 3 Epoch 029; Train loss: 0.0867; Time: 00:00:02
Fold 3 Epoch 030; Train loss: 0.0856; Time: 00:00:02
Fold 3: Evaluation at Epoch 30
HR@5       NDCG@5     HR@10      NDCG@10   
0.582781   0.494913   0.726821   0.540806  
Loss: 0.1244
Fold 3 Epoch 031; Train loss: 0.0843; Time: 00:00:02
Fold 3 Epoch 032; Train loss: 0.0811; Time: 00:00:02
Fold 3 Epoch 033; Train loss: 0.0799; Time: 00:00:02
Fold 3 Epoch 034; Train loss: 0.0794; Time: 00:00:02
Fold 3 Epoch 035; Train loss: 0.0770; Time: 00:00:02
Fold 3 Epoch 036; Train loss: 0.0754; Time: 00:00:02
Fold 3 Epoch 037; Train loss: 0.0735; Time: 00:00:02
Fold 3 Epoch 038; Train loss: 0.0683; Time: 00:00:02
Fold 3 Epoch 039; Train loss: 0.0687; Time: 00:00:02
Fold 3 Epoch 040; Train loss: 0.0673; Time: 00:00:02
Fold 3: Evaluation at Epoch 40
HR@5       NDCG@5     HR@10      NDCG@10   
0.576159   0.467257   0.725166   0.515647  
Loss: 0.0928
Fold 3 Epoch 041; Train loss: 0.0646; Time: 00:00:02
Fold 3 Epoch 042; Train loss: 0.0617; Time: 00:00:02
Fold 3 Epoch 043; Train loss: 0.0597; Time: 00:00:02
Fold 3 Epoch 044; Train loss: 0.0585; Time: 00:00:01
Fold 3 Epoch 045; Train loss: 0.0568; Time: 00:00:02
Fold 3 Epoch 046; Train loss: 0.0544; Time: 00:00:02
Fold 3 Epoch 047; Train loss: 0.0525; Time: 00:00:02
Fold 3 Epoch 048; Train loss: 0.0513; Time: 00:00:02
Fold 3 Epoch 049; Train loss: 0.0505; Time: 00:00:02
Fold 3 Epoch 050; Train loss: 0.0475; Time: 00:00:02
Fold 3: Evaluation at Epoch 50
HR@5       NDCG@5     HR@10      NDCG@10   
0.552980   0.383374   0.743377   0.444133  
Loss: 0.0733
Fold 3 Epoch 051; Train loss: 0.0441; Time: 00:00:02
Fold 3 Epoch 052; Train loss: 0.0431; Time: 00:00:02
Fold 3 Epoch 053; Train loss: 0.0411; Time: 00:00:02
Fold 3 Epoch 054; Train loss: 0.0399; Time: 00:00:02
Fold 3 Epoch 055; Train loss: 0.0387; Time: 00:00:02
Fold 3 Epoch 056; Train loss: 0.0370; Time: 00:00:02
Fold 3 Epoch 057; Train loss: 0.0354; Time: 00:00:02
Fold 3 Epoch 058; Train loss: 0.0337; Time: 00:00:02
Fold 3 Epoch 059; Train loss: 0.0321; Time: 00:00:02
Fold 3 Epoch 060; Train loss: 0.0299; Time: 00:00:02
Fold 3: Evaluation at Epoch 60
HR@5       NDCG@5     HR@10      NDCG@10   
0.456954   0.313934   0.685430   0.387718  
Loss: 0.0506
Fold 3 Epoch 061; Train loss: 0.0292; Time: 00:00:02
Fold 3 Epoch 062; Train loss: 0.0274; Time: 00:00:02
Fold 3 Epoch 063; Train loss: 0.0259; Time: 00:00:02
Fold 3 Epoch 064; Train loss: 0.0248; Time: 00:00:02
Fold 3 Epoch 065; Train loss: 0.0230; Time: 00:00:02
Fold 3 Epoch 066; Train loss: 0.0221; Time: 00:00:02
Fold 3 Epoch 067; Train loss: 0.0201; Time: 00:00:02
Fold 3 Epoch 068; Train loss: 0.0194; Time: 00:00:02
Fold 3 Epoch 069; Train loss: 0.0182; Time: 00:00:02
Fold 3 Epoch 070; Train loss: 0.0172; Time: 00:00:02
Fold 3: Evaluation at Epoch 70
HR@5       NDCG@5     HR@10      NDCG@10   
0.403974   0.247839   0.794702   0.371376  
Loss: 0.0337
Fold 3 Epoch 071; Train loss: 0.0160; Time: 00:00:02
Fold 3 Epoch 072; Train loss: 0.0147; Time: 00:00:02
Fold 3 Epoch 073; Train loss: 0.0137; Time: 00:00:02
Fold 3 Epoch 074; Train loss: 0.0129; Time: 00:00:02
Fold 3 Epoch 075; Train loss: 0.0118; Time: 00:00:02
Fold 3 Epoch 076; Train loss: 0.0109; Time: 00:00:02
Fold 3 Epoch 077; Train loss: 0.0099; Time: 00:00:02
Fold 3 Epoch 078; Train loss: 0.0092; Time: 00:00:01
Fold 3 Epoch 079; Train loss: 0.0083; Time: 00:00:02
Fold 3 Epoch 080; Train loss: 0.0077; Time: 00:00:02
Fold 3: Evaluation at Epoch 80
HR@5       NDCG@5     HR@10      NDCG@10   
0.225166   0.126476   0.922185   0.350737  
Loss: 0.0216
Fold 3 Epoch 081; Train loss: 0.0071; Time: 00:00:01
Fold 3 Epoch 082; Train loss: 0.0065; Time: 00:00:02
Fold 3 Epoch 083; Train loss: 0.0059; Time: 00:00:02
Fold 3 Epoch 084; Train loss: 0.0055; Time: 00:00:02
Fold 3 Epoch 085; Train loss: 0.0049; Time: 00:00:02
Fold 3 Epoch 086; Train loss: 0.0046; Time: 00:00:02
Fold 3 Epoch 087; Train loss: 0.0042; Time: 00:00:02
Fold 3 Epoch 088; Train loss: 0.0039; Time: 00:00:02
Fold 3 Epoch 089; Train loss: 0.0035; Time: 00:00:02
Fold 3 Epoch 090; Train loss: 0.0033; Time: 00:00:02
Fold 3: Evaluation at Epoch 90
HR@5       NDCG@5     HR@10      NDCG@10   
0.104305   0.070010   0.922185   0.333494  
Loss: 0.0136
Fold 3 Epoch 091; Train loss: 0.0030; Time: 00:00:02
Fold 3 Epoch 092; Train loss: 0.0029; Time: 00:00:02
Fold 3 Epoch 093; Train loss: 0.0026; Time: 00:00:02
Fold 3 Epoch 094; Train loss: 0.0024; Time: 00:00:03
Fold 3 Epoch 095; Train loss: 0.0023; Time: 00:00:02
Fold 3 Epoch 096; Train loss: 0.0023; Time: 00:00:02
Fold 3 Epoch 097; Train loss: 0.0021; Time: 00:00:02
Fold 3 Epoch 098; Train loss: 0.0020; Time: 00:00:02
Fold 3 Epoch 099; Train loss: 0.0018; Time: 00:00:02
Fold 3 Epoch 100; Train loss: 0.0018; Time: 00:00:02
Fold 3: Evaluation at Epoch 100
HR@5       NDCG@5     HR@10      NDCG@10   
0.102649   0.071410   0.922185   0.337303  
Loss: 0.0097
[scheduler candidate step] Fold 3: HR@10 = [np.float64(0.8493377483443708), np.float64(0.7268211920529801), np.float64(0.7268211920529801), np.float64(0.7251655629139073), np.float64(0.7433774834437086), np.float64(0.6854304635761589), np.float64(0.7947019867549668), np.float64(0.9221854304635762), np.float64(0.9221854304635762), np.float64(0.9221854304635762)]
Tuning scheduler: 100%|██████████| 3/3 [13:00<00:00, 261.19s/it]Tuning scheduler: 100%|██████████| 3/3 [13:00<00:00, 260.15s/it]

Best scheduler found: step
Tuning data for lr saved to ./category/tuning_lr.json
Tuning data for optimizer saved to ./category/tuning_optimizer.json
Tuning data for timesteps saved to ./category/tuning_timesteps.json
Tuning data for dropout_rate saved to ./category/tuning_dropout.json
Tuning data for l2_decay saved to ./category/tuning_l2.json
Tuning data for eps saved to ./category/tuning_eps.json
Tuning data for scheduler saved to ./category/tuning_scheduler.json

========== Fold 3 ==========
INFO:root:Using statics: seq_size = 10, genre_vocab_size = 18
Fold 3 Epoch 001; Train loss: 0.5409; Time: 00:00:02
Fold 3 Epoch 002; Train loss: 0.2559; Time: 00:00:02
Fold 3 Epoch 003; Train loss: 0.1907; Time: 00:00:02
Fold 3 Epoch 004; Train loss: 0.1645; Time: 00:00:02
Fold 3 Epoch 005; Train loss: 0.1527; Time: 00:00:02
Fold 3 Epoch 006; Train loss: 0.1448; Time: 00:00:02
Fold 3 Epoch 007; Train loss: 0.1385; Time: 00:00:02
Fold 3 Epoch 008; Train loss: 0.1339; Time: 00:00:02
Fold 3 Epoch 009; Train loss: 0.1287; Time: 00:00:02
Fold 3 Epoch 010; Train loss: 0.1296; Time: 00:00:02
Fold 3: Evaluation at Epoch 10
HR@5       NDCG@5     HR@10      NDCG@10   
0.571192   0.457629   0.730132   0.508356  
Loss: 0.1733
Fold 3 Epoch 011; Train loss: 0.1267; Time: 00:00:02
Fold 3 Epoch 012; Train loss: 0.1221; Time: 00:00:02
Fold 3 Epoch 013; Train loss: 0.1179; Time: 00:00:02
Fold 3 Epoch 014; Train loss: 0.1150; Time: 00:00:02
Fold 3 Epoch 015; Train loss: 0.1157; Time: 00:00:02
Fold 3 Epoch 016; Train loss: 0.1123; Time: 00:00:02
Fold 3 Epoch 017; Train loss: 0.1112; Time: 00:00:02
Fold 3 Epoch 018; Train loss: 0.1100; Time: 00:00:02
Fold 3 Epoch 019; Train loss: 0.1058; Time: 00:00:02
Fold 3 Epoch 020; Train loss: 0.1045; Time: 00:00:02
Fold 3: Evaluation at Epoch 20
HR@5       NDCG@5     HR@10      NDCG@10   
0.591060   0.436878   0.750000   0.487938  
Loss: 0.1460
Fold 3 Epoch 021; Train loss: 0.1011; Time: 00:00:02
Fold 3 Epoch 022; Train loss: 0.1007; Time: 00:00:02
Fold 3 Epoch 023; Train loss: 0.1003; Time: 00:00:02
Fold 3 Epoch 024; Train loss: 0.0952; Time: 00:00:02
Fold 3 Epoch 025; Train loss: 0.0960; Time: 00:00:02
Fold 3 Epoch 026; Train loss: 0.0925; Time: 00:00:02
Fold 3 Epoch 027; Train loss: 0.0897; Time: 00:00:02
Fold 3 Epoch 028; Train loss: 0.0875; Time: 00:00:02
Fold 3 Epoch 029; Train loss: 0.0874; Time: 00:00:02
Fold 3 Epoch 030; Train loss: 0.0853; Time: 00:00:02
Fold 3: Evaluation at Epoch 30
HR@5       NDCG@5     HR@10      NDCG@10   
0.548013   0.450829   0.723510   0.506591  
Loss: 0.1291
Fold 3 Epoch 031; Train loss: 0.0806; Time: 00:00:02
Fold 3 Epoch 032; Train loss: 0.0804; Time: 00:00:02
Fold 3 Epoch 033; Train loss: 0.0771; Time: 00:00:02
Fold 3 Epoch 034; Train loss: 0.0759; Time: 00:00:02
Fold 3 Epoch 035; Train loss: 0.0737; Time: 00:00:02
Fold 3 Epoch 036; Train loss: 0.0723; Time: 00:00:01
Fold 3 Epoch 037; Train loss: 0.0687; Time: 00:00:02
Fold 3 Epoch 038; Train loss: 0.0675; Time: 00:00:01
Fold 3 Epoch 039; Train loss: 0.0658; Time: 00:00:02
Fold 3 Epoch 040; Train loss: 0.0640; Time: 00:00:02
Fold 3: Evaluation at Epoch 40
HR@5       NDCG@5     HR@10      NDCG@10   
0.581126   0.427871   0.723510   0.473900  
Loss: 0.0996
Fold 3 Epoch 041; Train loss: 0.0631; Time: 00:00:02
Fold 3 Epoch 042; Train loss: 0.0614; Time: 00:00:02
Fold 3 Epoch 043; Train loss: 0.0578; Time: 00:00:02
Fold 3 Epoch 044; Train loss: 0.0559; Time: 00:00:02
Fold 3 Epoch 045; Train loss: 0.0554; Time: 00:00:02
Fold 3 Epoch 046; Train loss: 0.0515; Time: 00:00:02
Fold 3 Epoch 047; Train loss: 0.0496; Time: 00:00:02
Fold 3 Epoch 048; Train loss: 0.0487; Time: 00:00:02
Fold 3 Epoch 049; Train loss: 0.0456; Time: 00:00:02
Fold 3 Epoch 050; Train loss: 0.0447; Time: 00:00:02
Fold 3: Evaluation at Epoch 50
HR@5       NDCG@5     HR@10      NDCG@10   
0.620861   0.461171   0.758278   0.505332  
Loss: 0.0784
Fold 3 Epoch 051; Train loss: 0.0431; Time: 00:00:02
Fold 3 Epoch 052; Train loss: 0.0413; Time: 00:00:02
Fold 3 Epoch 053; Train loss: 0.0383; Time: 00:00:02
Fold 3 Epoch 054; Train loss: 0.0378; Time: 00:00:02
Fold 3 Epoch 055; Train loss: 0.0346; Time: 00:00:02
Fold 3 Epoch 056; Train loss: 0.0341; Time: 00:00:02
Fold 3 Epoch 057; Train loss: 0.0316; Time: 00:00:02
Fold 3 Epoch 058; Train loss: 0.0308; Time: 00:00:02
Fold 3 Epoch 059; Train loss: 0.0280; Time: 00:00:02
Fold 3 Epoch 060; Train loss: 0.0270; Time: 00:00:02
Fold 3: Evaluation at Epoch 60
HR@5       NDCG@5     HR@10      NDCG@10   
0.635762   0.467601   0.824503   0.526654  
Loss: 0.0502
Fold 3 Epoch 061; Train loss: 0.0260; Time: 00:00:01
Fold 3 Epoch 062; Train loss: 0.0238; Time: 00:00:02
Fold 3 Epoch 063; Train loss: 0.0223; Time: 00:00:02
Fold 3 Epoch 064; Train loss: 0.0208; Time: 00:00:02
Fold 3 Epoch 065; Train loss: 0.0198; Time: 00:00:02
Fold 3 Epoch 066; Train loss: 0.0183; Time: 00:00:02
Fold 3 Epoch 067; Train loss: 0.0166; Time: 00:00:02
Fold 3 Epoch 068; Train loss: 0.0155; Time: 00:00:02
Fold 3 Epoch 069; Train loss: 0.0142; Time: 00:00:02
Fold 3 Epoch 070; Train loss: 0.0131; Time: 00:00:02
Fold 3: Evaluation at Epoch 70
HR@5       NDCG@5     HR@10      NDCG@10   
0.614238   0.380668   0.817881   0.445941  
Loss: 0.0348
Fold 3 Epoch 071; Train loss: 0.0119; Time: 00:00:02
Fold 3 Epoch 072; Train loss: 0.0108; Time: 00:00:02
Fold 3 Epoch 073; Train loss: 0.0102; Time: 00:00:02
Fold 3 Epoch 074; Train loss: 0.0093; Time: 00:00:02
Fold 3 Epoch 075; Train loss: 0.0083; Time: 00:00:02
Fold 3 Epoch 076; Train loss: 0.0077; Time: 00:00:02
Fold 3 Epoch 077; Train loss: 0.0070; Time: 00:00:02
Fold 3 Epoch 078; Train loss: 0.0065; Time: 00:00:02
Fold 3 Epoch 079; Train loss: 0.0059; Time: 00:00:02
Fold 3 Epoch 080; Train loss: 0.0054; Time: 00:00:02
Fold 3: Evaluation at Epoch 80
HR@5       NDCG@5     HR@10      NDCG@10   
0.149007   0.069170   0.855960   0.301030  
Loss: 0.0216
Fold 3 Epoch 081; Train loss: 0.0050; Time: 00:00:02
Fold 3 Epoch 082; Train loss: 0.0046; Time: 00:00:02
Fold 3 Epoch 083; Train loss: 0.0043; Time: 00:00:02
Fold 3 Epoch 084; Train loss: 0.0040; Time: 00:00:02
Fold 3 Epoch 085; Train loss: 0.0037; Time: 00:00:02
Fold 3 Epoch 086; Train loss: 0.0034; Time: 00:00:02
Fold 3 Epoch 087; Train loss: 0.0032; Time: 00:00:02
Fold 3 Epoch 088; Train loss: 0.0030; Time: 00:00:02
Fold 3 Epoch 089; Train loss: 0.0029; Time: 00:00:02
Fold 3 Epoch 090; Train loss: 0.0027; Time: 00:00:02
Fold 3: Evaluation at Epoch 90
HR@5       NDCG@5     HR@10      NDCG@10   
0.059603   0.029642   0.855960   0.287618  
Loss: 0.0135
Fold 3 Epoch 091; Train loss: 0.0025; Time: 00:00:02
Fold 3 Epoch 092; Train loss: 0.0024; Time: 00:00:02
Fold 3 Epoch 093; Train loss: 0.0023; Time: 00:00:02
Fold 3 Epoch 094; Train loss: 0.0021; Time: 00:00:02
Fold 3 Epoch 095; Train loss: 0.0020; Time: 00:00:02
Fold 3 Epoch 096; Train loss: 0.0019; Time: 00:00:02
Fold 3 Epoch 097; Train loss: 0.0018; Time: 00:00:02
Fold 3 Epoch 098; Train loss: 0.0017; Time: 00:00:02
Fold 3 Epoch 099; Train loss: 0.0017; Time: 00:00:02
Fold 3 Epoch 100; Train loss: 0.0016; Time: 00:00:02
Fold 3: Evaluation at Epoch 100
HR@5       NDCG@5     HR@10      NDCG@10   
0.302980   0.134380   0.859272   0.314736  
Loss: 0.0101
Tuning fold metrics saved to ./category/fold_metrics_tune.txt
Traceback (most recent call last):
  File "/home/nit/Desktop/2PDreamRec/DreamRec_Gen.py", line 479, in <module>
    main()
  File "/home/nit/Desktop/2PDreamRec/DreamRec_Gen.py", line 419, in main
    best_candidates["scheduler_factor"] = args.scheduler_factor
AttributeError: 'Namespace' object has no attribute 'scheduler_factor'
Tuning lr:   0%|          | 0/5 [00:00<?, ?it/s]
========== Fold 3 ==========
INFO:root:Using statics: seq_size = 10, genre_vocab_size = 18
Fold 3 Epoch 001; Train loss: 2223.8496; Time: 00:00:03
Fold 3 Epoch 002; Train loss: 9.9596; Time: 00:00:02
Fold 3 Epoch 003; Train loss: 1.5321; Time: 00:00:02
Fold 3 Epoch 004; Train loss: 1.1314; Time: 00:00:02
Fold 3 Epoch 005; Train loss: 0.8961; Time: 00:00:02
Fold 3 Epoch 006; Train loss: 1.0444; Time: 00:00:02
Fold 3 Epoch 007; Train loss: 0.7289; Time: 00:00:02
Fold 3 Epoch 008; Train loss: 0.4683; Time: 00:00:03
Fold 3 Epoch 009; Train loss: 0.3959; Time: 00:00:03
Fold 3 Epoch 010; Train loss: 0.3385; Time: 00:00:02
Fold 3: Evaluation at Epoch 10
HR@5       NDCG@5     HR@10      NDCG@10   
0.551325   0.403919   0.658940   0.438284  
Loss: 0.3108
/home/nit/Desktop/2PDreamRec/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:243: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
Fold 3 Epoch 011; Train loss: 0.3036; Time: 00:00:02
Fold 3 Epoch 012; Train loss: 0.2614; Time: 00:00:02
Fold 3 Epoch 013; Train loss: 0.2428; Time: 00:00:02
Fold 3 Epoch 014; Train loss: 0.2284; Time: 00:00:02
Fold 3 Epoch 015; Train loss: 0.2257; Time: 00:00:02
Fold 3 Epoch 016; Train loss: 0.2985; Time: 00:00:02
Fold 3 Epoch 017; Train loss: 0.2651; Time: 00:00:02
Fold 3 Epoch 018; Train loss: 0.1780; Time: 00:00:02
Fold 3 Epoch 019; Train loss: 0.1710; Time: 00:00:02
Fold 3 Epoch 020; Train loss: 0.1609; Time: 00:00:02
Fold 3: Evaluation at Epoch 20
HR@5       NDCG@5     HR@10      NDCG@10   
0.534768   0.392353   0.645695   0.428146  
Loss: 0.1685
/home/nit/Desktop/2PDreamRec/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:243: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
Fold 3 Epoch 021; Train loss: 0.1529; Time: 00:00:02
Fold 3 Epoch 022; Train loss: 0.1506; Time: 00:00:01
Fold 3 Epoch 023; Train loss: 0.1507; Time: 00:00:01
Fold 3 Epoch 024; Train loss: 0.1488; Time: 00:00:01
Fold 3 Epoch 025; Train loss: 0.1918; Time: 00:00:01
Fold 3 Epoch 026; Train loss: 0.2350; Time: 00:00:01
Fold 3 Epoch 027; Train loss: 0.1396; Time: 00:00:02
Fold 3 Epoch 028; Train loss: 0.1243; Time: 00:00:02
Fold 3 Epoch 029; Train loss: 0.1191; Time: 00:00:02
Fold 3 Epoch 030; Train loss: 0.1184; Time: 00:00:02
Fold 3: Evaluation at Epoch 30
HR@5       NDCG@5     HR@10      NDCG@10   
0.488411   0.325754   0.637417   0.373927  
Loss: 0.1353
/home/nit/Desktop/2PDreamRec/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:243: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
Fold 3 Epoch 031; Train loss: 0.1149; Time: 00:00:02
Fold 3 Epoch 032; Train loss: 0.1144; Time: 00:00:02
Fold 3 Epoch 033; Train loss: 0.1083; Time: 00:00:02
Fold 3 Epoch 034; Train loss: 0.1078; Time: 00:00:02
Fold 3 Epoch 035; Train loss: 0.1063; Time: 00:00:02
Fold 3 Epoch 036; Train loss: 0.1037; Time: 00:00:02
Fold 3 Epoch 037; Train loss: 0.1045; Time: 00:00:02
Fold 3 Epoch 038; Train loss: 0.1015; Time: 00:00:02
Fold 3 Epoch 039; Train loss: 0.1084; Time: 00:00:02
Fold 3 Epoch 040; Train loss: 0.1273; Time: 00:00:02
Fold 3: Evaluation at Epoch 40
HR@5       NDCG@5     HR@10      NDCG@10   
0.523179   0.390197   0.642384   0.428557  
Loss: 0.1384
/home/nit/Desktop/2PDreamRec/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:243: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
Fold 3 Epoch 041; Train loss: 0.1041; Time: 00:00:02
Fold 3 Epoch 042; Train loss: 0.0954; Time: 00:00:02
Fold 3 Epoch 043; Train loss: 0.0935; Time: 00:00:02
Fold 3 Epoch 044; Train loss: 0.0899; Time: 00:00:02
Fold 3 Epoch 045; Train loss: 0.0905; Time: 00:00:02
Fold 3 Epoch 046; Train loss: 0.0856; Time: 00:00:02
Fold 3 Epoch 047; Train loss: 0.0866; Time: 00:00:02
Fold 3 Epoch 048; Train loss: 0.0840; Time: 00:00:02
Fold 3 Epoch 049; Train loss: 0.0925; Time: 00:00:02
Fold 3 Epoch 050; Train loss: 0.1178; Time: 00:00:02
Fold 3: Evaluation at Epoch 50
HR@5       NDCG@5     HR@10      NDCG@10   
0.250000   0.157311   0.384106   0.200185  
Loss: 0.1262
/home/nit/Desktop/2PDreamRec/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:243: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
Fold 3 Epoch 051; Train loss: 0.1249; Time: 00:00:02
Fold 3 Epoch 052; Train loss: 0.0883; Time: 00:00:02
Fold 3 Epoch 053; Train loss: 0.0770; Time: 00:00:02
Fold 3 Epoch 054; Train loss: 0.0753; Time: 00:00:02
Fold 3 Epoch 055; Train loss: 0.0743; Time: 00:00:01
Fold 3 Epoch 056; Train loss: 0.0735; Time: 00:00:01
Fold 3 Epoch 057; Train loss: 0.0736; Time: 00:00:02
Fold 3 Epoch 058; Train loss: 0.0721; Time: 00:00:02
Fold 3 Epoch 059; Train loss: 0.0778; Time: 00:00:02
Fold 3 Epoch 060; Train loss: 0.0883; Time: 00:00:02
Fold 3: Evaluation at Epoch 60
HR@5       NDCG@5     HR@10      NDCG@10   
0.476821   0.344349   0.620861   0.390655  
Loss: 0.1226
/home/nit/Desktop/2PDreamRec/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:243: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
Fold 3 Epoch 061; Train loss: 0.0939; Time: 00:00:02
Fold 3 Epoch 062; Train loss: 0.0759; Time: 00:00:02
Fold 3 Epoch 063; Train loss: 0.0675; Time: 00:00:02
Fold 3 Epoch 064; Train loss: 0.0658; Time: 00:00:02
Fold 3 Epoch 065; Train loss: 0.0625; Time: 00:00:02
Fold 3 Epoch 066; Train loss: 0.0635; Time: 00:00:02
Fold 3 Epoch 067; Train loss: 0.0613; Time: 00:00:02
Fold 3 Epoch 068; Train loss: 0.0594; Time: 00:00:02
Fold 3 Epoch 069; Train loss: 0.0626; Time: 00:00:02
Fold 3 Epoch 070; Train loss: 0.0926; Time: 00:00:02
Fold 3: Evaluation at Epoch 70
HR@5       NDCG@5     HR@10      NDCG@10   
0.529801   0.369449   0.662252   0.412024  
Loss: 0.1659
/home/nit/Desktop/2PDreamRec/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:243: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
Fold 3 Epoch 071; Train loss: 0.1106; Time: 00:00:02
Fold 3 Epoch 072; Train loss: 0.0654; Time: 00:00:02
Fold 3 Epoch 073; Train loss: 0.0558; Time: 00:00:02
Fold 3 Epoch 074; Train loss: 0.0537; Time: 00:00:02
Fold 3 Epoch 075; Train loss: 0.0534; Time: 00:00:02
Fold 3 Epoch 076; Train loss: 0.0534; Time: 00:00:02
Fold 3 Epoch 077; Train loss: 0.0506; Time: 00:00:02
Fold 3 Epoch 078; Train loss: 0.0495; Time: 00:00:02
Fold 3 Epoch 079; Train loss: 0.0497; Time: 00:00:02
Fold 3 Epoch 080; Train loss: 0.0497; Time: 00:00:02
Fold 3: Evaluation at Epoch 80
HR@5       NDCG@5     HR@10      NDCG@10   
0.465232   0.317275   0.594371   0.358500  
Loss: 0.0608
/home/nit/Desktop/2PDreamRec/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:243: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
Fold 3 Epoch 081; Train loss: 0.0511; Time: 00:00:02
Fold 3 Epoch 082; Train loss: 0.0560; Time: 00:00:01
Fold 3 Epoch 083; Train loss: 0.0563; Time: 00:00:02
Fold 3 Epoch 084; Train loss: 0.0628; Time: 00:00:01
Fold 3 Epoch 085; Train loss: 0.0538; Time: 00:00:01
Fold 3 Epoch 086; Train loss: 0.0473; Time: 00:00:01
Fold 3 Epoch 087; Train loss: 0.0453; Time: 00:00:01
Fold 3 Epoch 088; Train loss: 0.0448; Time: 00:00:01
Fold 3 Epoch 089; Train loss: 0.0476; Time: 00:00:01
Fold 3 Epoch 090; Train loss: 0.0496; Time: 00:00:01
Fold 3: Evaluation at Epoch 90
HR@5       NDCG@5     HR@10      NDCG@10   
0.543046   0.386320   0.650662   0.420170  
Loss: 0.0603
/home/nit/Desktop/2PDreamRec/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:243: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
Fold 3 Epoch 091; Train loss: 0.0459; Time: 00:00:02
Fold 3 Epoch 092; Train loss: 0.0409; Time: 00:00:02
Fold 3 Epoch 093; Train loss: 0.0390; Time: 00:00:02
Fold 3 Epoch 094; Train loss: 0.0391; Time: 00:00:02
Fold 3 Epoch 095; Train loss: 0.0381; Time: 00:00:02
Fold 3 Epoch 096; Train loss: 0.0398; Time: 00:00:02
Fold 3 Epoch 097; Train loss: 0.0423; Time: 00:00:02
Fold 3 Epoch 098; Train loss: 0.0451; Time: 00:00:02
Fold 3 Epoch 099; Train loss: 0.0458; Time: 00:00:02
Fold 3 Epoch 100; Train loss: 0.0458; Time: 00:00:02
Fold 3: Evaluation at Epoch 100
HR@5       NDCG@5     HR@10      NDCG@10   
0.442053   0.295194   0.589404   0.341827  
Loss: 0.0534
/home/nit/Desktop/2PDreamRec/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:243: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
[lr candidate 0.1] Fold 3: NDCG@10 = [np.float64(0.43828359044938225), np.float64(0.42814629273739957), np.float64(0.37392749851283147), np.float64(0.4285567549615882), np.float64(0.20018487543345664), np.float64(0.39065511709087014), np.float64(0.4120244838374548), np.float64(0.3584999126447616), np.float64(0.42017049481393903), np.float64(0.3418269088008255)]
Tuning lr:  20%|██        | 1/5 [04:15<17:03, 255.83s/it]
========== Fold 3 ==========
INFO:root:Using statics: seq_size = 10, genre_vocab_size = 18
Fold 3 Epoch 001; Train loss: 24.2457; Time: 00:00:02
Fold 3 Epoch 002; Train loss: 0.3345; Time: 00:00:02
Fold 3 Epoch 003; Train loss: 0.2462; Time: 00:00:02
Fold 3 Epoch 004; Train loss: 0.2007; Time: 00:00:02
Fold 3 Epoch 005; Train loss: 0.2931; Time: 00:00:02
Fold 3 Epoch 006; Train loss: 0.1820; Time: 00:00:02
Fold 3 Epoch 007; Train loss: 0.1294; Time: 00:00:02
Fold 3 Epoch 008; Train loss: 0.1197; Time: 00:00:02
Fold 3 Epoch 009; Train loss: 0.1102; Time: 00:00:02
Fold 3 Epoch 010; Train loss: 0.1015; Time: 00:00:02
Fold 3: Evaluation at Epoch 10
HR@5       NDCG@5     HR@10      NDCG@10   
0.339404   0.301812   0.425497   0.328394  
Loss: 0.1214
/home/nit/Desktop/2PDreamRec/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:243: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
Fold 3 Epoch 011; Train loss: 0.0954; Time: 00:00:02
Fold 3 Epoch 012; Train loss: 0.0906; Time: 00:00:02
Fold 3 Epoch 013; Train loss: 0.1007; Time: 00:00:01
Fold 3 Epoch 014; Train loss: 0.1641; Time: 00:00:01
Fold 3 Epoch 015; Train loss: 0.0766; Time: 00:00:01
Fold 3 Epoch 016; Train loss: 0.0678; Time: 00:00:01
Fold 3 Epoch 017; Train loss: 0.0621; Time: 00:00:02
Fold 3 Epoch 018; Train loss: 0.0601; Time: 00:00:01
Fold 3 Epoch 019; Train loss: 0.0557; Time: 00:00:01
Fold 3 Epoch 020; Train loss: 0.0520; Time: 00:00:02
Fold 3: Evaluation at Epoch 20
HR@5       NDCG@5     HR@10      NDCG@10   
0.167219   0.115746   0.321192   0.162738  
Loss: 0.0567
/home/nit/Desktop/2PDreamRec/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:243: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
Fold 3 Epoch 021; Train loss: 0.0478; Time: 00:00:02
Fold 3 Epoch 022; Train loss: 0.0452; Time: 00:00:02
Fold 3 Epoch 023; Train loss: 0.0424; Time: 00:00:02
Fold 3 Epoch 024; Train loss: 0.0398; Time: 00:00:02
Fold 3 Epoch 025; Train loss: 0.0354; Time: 00:00:02
Fold 3 Epoch 026; Train loss: 0.0327; Time: 00:00:02
Fold 3 Epoch 027; Train loss: 0.0321; Time: 00:00:02
Fold 3 Epoch 028; Train loss: 0.0445; Time: 00:00:02
Fold 3 Epoch 029; Train loss: 0.0420; Time: 00:00:02
Fold 3 Epoch 030; Train loss: 0.0249; Time: 00:00:02
Fold 3: Evaluation at Epoch 30
HR@5       NDCG@5     HR@10      NDCG@10   
0.293046   0.228960   0.408940   0.265844  
Loss: 0.0239
/home/nit/Desktop/2PDreamRec/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:243: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
Fold 3 Epoch 031; Train loss: 0.0225; Time: 00:00:02
Fold 3 Epoch 032; Train loss: 0.0222; Time: 00:00:01
Fold 3 Epoch 033; Train loss: 0.0207; Time: 00:00:01
Fold 3 Epoch 034; Train loss: 0.0189; Time: 00:00:02
Fold 3 Epoch 035; Train loss: 0.0183; Time: 00:00:01
Fold 3 Epoch 036; Train loss: 0.0168; Time: 00:00:01
Fold 3 Epoch 037; Train loss: 0.0153; Time: 00:00:01
Fold 3 Epoch 038; Train loss: 0.0139; Time: 00:00:01
Fold 3 Epoch 039; Train loss: 0.0126; Time: 00:00:01
Fold 3 Epoch 040; Train loss: 0.0102; Time: 00:00:02
Fold 3: Evaluation at Epoch 40
HR@5       NDCG@5     HR@10      NDCG@10   
0.683775   0.365116   0.812914   0.405435  
Loss: 0.0066
/home/nit/Desktop/2PDreamRec/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:243: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
Fold 3 Epoch 041; Train loss: 0.0085; Time: 00:00:01
Fold 3 Epoch 042; Train loss: 0.0097; Time: 00:00:02
Fold 3 Epoch 043; Train loss: 0.0370; Time: 00:00:01
Fold 3 Epoch 044; Train loss: 0.0059; Time: 00:00:01
Fold 3 Epoch 045; Train loss: 0.0018; Time: 00:00:01
Fold 3 Epoch 046; Train loss: 0.0011; Time: 00:00:01
Fold 3 Epoch 047; Train loss: 0.0006; Time: 00:00:01
Fold 3 Epoch 048; Train loss: 0.0004; Time: 00:00:01
Fold 3 Epoch 049; Train loss: 0.0003; Time: 00:00:01
Fold 3 Epoch 050; Train loss: 0.0003; Time: 00:00:02
Fold 3: Evaluation at Epoch 50
HR@5       NDCG@5     HR@10      NDCG@10   
0.562914   0.401621   0.855960   0.499283  
Loss: 0.0004
/home/nit/Desktop/2PDreamRec/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:243: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
Fold 3 Epoch 051; Train loss: 0.0003; Time: 00:00:01
Fold 3 Epoch 052; Train loss: 0.0002; Time: 00:00:01
Fold 3 Epoch 053; Train loss: 0.0002; Time: 00:00:01
Fold 3 Epoch 054; Train loss: 0.0002; Time: 00:00:01
Fold 3 Epoch 055; Train loss: 0.0002; Time: 00:00:02
Fold 3 Epoch 056; Train loss: 0.0002; Time: 00:00:02
Fold 3 Epoch 057; Train loss: 0.0002; Time: 00:00:02
Fold 3 Epoch 058; Train loss: 0.0002; Time: 00:00:02
Fold 3 Epoch 059; Train loss: 0.0002; Time: 00:00:02
Fold 3 Epoch 060; Train loss: 0.0004; Time: 00:00:02
Fold 3: Evaluation at Epoch 60
HR@5       NDCG@5     HR@10      NDCG@10   
0.562914   0.278095   0.895695   0.393075  
Loss: 0.0004
/home/nit/Desktop/2PDreamRec/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:243: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
Fold 3 Epoch 061; Train loss: 0.0008; Time: 00:00:02
Fold 3 Epoch 062; Train loss: 0.0037; Time: 00:00:02
Fold 3 Epoch 063; Train loss: 0.0104; Time: 00:00:02
Fold 3 Epoch 064; Train loss: 0.0055; Time: 00:00:02
Fold 3 Epoch 065; Train loss: 0.0005; Time: 00:00:02
Fold 3 Epoch 066; Train loss: 0.0002; Time: 00:00:02
Fold 3 Epoch 067; Train loss: 0.0001; Time: 00:00:02
Fold 3 Epoch 068; Train loss: 0.0001; Time: 00:00:03
Fold 3 Epoch 069; Train loss: 0.0001; Time: 00:00:02
Fold 3 Epoch 070; Train loss: 0.0001; Time: 00:00:02
Fold 3: Evaluation at Epoch 70
HR@5       NDCG@5     HR@10      NDCG@10   
0.564570   0.323195   0.895695   0.431754  
Loss: 0.0003
/home/nit/Desktop/2PDreamRec/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:243: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
Fold 3 Epoch 071; Train loss: 0.0001; Time: 00:00:02
Fold 3 Epoch 072; Train loss: 0.0001; Time: 00:00:02
Fold 3 Epoch 073; Train loss: 0.0001; Time: 00:00:02
Fold 3 Epoch 074; Train loss: 0.0001; Time: 00:00:01
Fold 3 Epoch 075; Train loss: 0.0001; Time: 00:00:02
Fold 3 Epoch 076; Train loss: 0.0001; Time: 00:00:02
Fold 3 Epoch 077; Train loss: 0.0001; Time: 00:00:01
Fold 3 Epoch 078; Train loss: 0.0001; Time: 00:00:01
Fold 3 Epoch 079; Train loss: 0.0002; Time: 00:00:01
Fold 3 Epoch 080; Train loss: 0.0023; Time: 00:00:01
Fold 3: Evaluation at Epoch 80
HR@5       NDCG@5     HR@10      NDCG@10   
0.288079   0.141427   0.895695   0.335451  
Loss: 0.0081
/home/nit/Desktop/2PDreamRec/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:243: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
Fold 3 Epoch 081; Train loss: 0.0177; Time: 00:00:01
Fold 3 Epoch 082; Train loss: 0.0074; Time: 00:00:01
Fold 3 Epoch 083; Train loss: 0.0002; Time: 00:00:02
Fold 3 Epoch 084; Train loss: 0.0001; Time: 00:00:02
Fold 3 Epoch 085; Train loss: 0.0001; Time: 00:00:02
Fold 3 Epoch 086; Train loss: 0.0001; Time: 00:00:02
Fold 3 Epoch 087; Train loss: 0.0001; Time: 00:00:02
Fold 3 Epoch 088; Train loss: 0.0001; Time: 00:00:01
Fold 3 Epoch 089; Train loss: 0.0001; Time: 00:00:02
Fold 3 Epoch 090; Train loss: 0.0001; Time: 00:00:01
Fold 3: Evaluation at Epoch 90
HR@5       NDCG@5     HR@10      NDCG@10   
0.819536   0.377370   0.895695   0.400940  
Loss: 0.0002
/home/nit/Desktop/2PDreamRec/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:243: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
Fold 3 Epoch 091; Train loss: 0.0001; Time: 00:00:01
Fold 3 Epoch 092; Train loss: 0.0001; Time: 00:00:02
Fold 3 Epoch 093; Train loss: 0.0001; Time: 00:00:02
Fold 3 Epoch 094; Train loss: 0.0001; Time: 00:00:02
Fold 3 Epoch 095; Train loss: 0.0001; Time: 00:00:02
Fold 3 Epoch 096; Train loss: 0.0001; Time: 00:00:01
Fold 3 Epoch 097; Train loss: 0.0001; Time: 00:00:02
Fold 3 Epoch 098; Train loss: 0.0001; Time: 00:00:02
Fold 3 Epoch 099; Train loss: 0.0001; Time: 00:00:02
Fold 3 Epoch 100; Train loss: 0.0001; Time: 00:00:02
Fold 3: Evaluation at Epoch 100
HR@5       NDCG@5     HR@10      NDCG@10   
0.564570   0.279660   0.895695   0.388219  
Loss: 0.0001
/home/nit/Desktop/2PDreamRec/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:243: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
[lr candidate 0.05] Fold 3: NDCG@10 = [np.float64(0.32839410487740855), np.float64(0.1627384125805598), np.float64(0.2658435595123874), np.float64(0.4054352860538862), np.float64(0.49928319805765164), np.float64(0.39307533492207375), np.float64(0.4317540711027254), np.float64(0.3354511552266228), np.float64(0.40093969108036775), np.float64(0.38821887545078343)]
Tuning lr:  40%|████      | 2/5 [08:11<12:11, 243.72s/it]
========== Fold 3 ==========
INFO:root:Using statics: seq_size = 10, genre_vocab_size = 18
Fold 3 Epoch 001; Train loss: 0.4567; Time: 00:00:02
Fold 3 Epoch 002; Train loss: 0.1924; Time: 00:00:02
Fold 3 Epoch 003; Train loss: 0.1463; Time: 00:00:02
Fold 3 Epoch 004; Train loss: 0.1310; Time: 00:00:02
Fold 3 Epoch 005; Train loss: 0.1227; Time: 00:00:02
Fold 3 Epoch 006; Train loss: 0.1157; Time: 00:00:02
Fold 3 Epoch 007; Train loss: 0.1105; Time: 00:00:02
Fold 3 Epoch 008; Train loss: 0.1023; Time: 00:00:01
Fold 3 Epoch 009; Train loss: 0.0942; Time: 00:00:02
Fold 3 Epoch 010; Train loss: 0.0908; Time: 00:00:01
Fold 3: Evaluation at Epoch 10
HR@5       NDCG@5     HR@10      NDCG@10   
0.574503   0.451880   0.683775   0.487354  
Loss: 0.0990
/home/nit/Desktop/2PDreamRec/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:243: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
Fold 3 Epoch 011; Train loss: 0.0848; Time: 00:00:01
Fold 3 Epoch 012; Train loss: 0.0781; Time: 00:00:01
Fold 3 Epoch 013; Train loss: 0.0720; Time: 00:00:01
Fold 3 Epoch 014; Train loss: 0.0674; Time: 00:00:01
Fold 3 Epoch 015; Train loss: 0.0619; Time: 00:00:01
Fold 3 Epoch 016; Train loss: 0.0558; Time: 00:00:02
Fold 3 Epoch 017; Train loss: 0.0526; Time: 00:00:01
Fold 3 Epoch 018; Train loss: 0.0479; Time: 00:00:02
Fold 3 Epoch 019; Train loss: 0.0423; Time: 00:00:02
Fold 3 Epoch 020; Train loss: 0.0412; Time: 00:00:02
Fold 3: Evaluation at Epoch 20
HR@5       NDCG@5     HR@10      NDCG@10   
0.706954   0.526847   0.849338   0.572953  
Loss: 0.0410
/home/nit/Desktop/2PDreamRec/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:243: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
Fold 3 Epoch 021; Train loss: 0.0374; Time: 00:00:02
Fold 3 Epoch 022; Train loss: 0.0337; Time: 00:00:02
Fold 3 Epoch 023; Train loss: 0.0303; Time: 00:00:02
Fold 3 Epoch 024; Train loss: 0.0275; Time: 00:00:02
Fold 3 Epoch 025; Train loss: 0.0242; Time: 00:00:02
Fold 3 Epoch 026; Train loss: 0.0223; Time: 00:00:02
Fold 3 Epoch 027; Train loss: 0.0183; Time: 00:00:01
Fold 3 Epoch 028; Train loss: 0.0153; Time: 00:00:02
Fold 3 Epoch 029; Train loss: 0.0119; Time: 00:00:02
Fold 3 Epoch 030; Train loss: 0.0094; Time: 00:00:01
Fold 3: Evaluation at Epoch 30
HR@5       NDCG@5     HR@10      NDCG@10   
0.564570   0.376290   0.887417   0.475020  
Loss: 0.0101
/home/nit/Desktop/2PDreamRec/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:243: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
Fold 3 Epoch 031; Train loss: 0.0071; Time: 00:00:01
Fold 3 Epoch 032; Train loss: 0.0056; Time: 00:00:01
Fold 3 Epoch 033; Train loss: 0.0033; Time: 00:00:01
Fold 3 Epoch 034; Train loss: 0.0020; Time: 00:00:01
Fold 3 Epoch 035; Train loss: 0.0011; Time: 00:00:02
Fold 3 Epoch 036; Train loss: 0.0007; Time: 00:00:02
Fold 3 Epoch 037; Train loss: 0.0005; Time: 00:00:02
Fold 3 Epoch 038; Train loss: 0.0004; Time: 00:00:02
Fold 3 Epoch 039; Train loss: 0.0004; Time: 00:00:02
Fold 3 Epoch 040; Train loss: 0.0006; Time: 00:00:01
Fold 3: Evaluation at Epoch 40
HR@5       NDCG@5     HR@10      NDCG@10   
0.556291   0.367004   0.855960   0.458033  
Loss: 0.0011
/home/nit/Desktop/2PDreamRec/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:243: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
Fold 3 Epoch 041; Train loss: 0.0008; Time: 00:00:01
Fold 3 Epoch 042; Train loss: 0.0006; Time: 00:00:01
Fold 3 Epoch 043; Train loss: 0.0003; Time: 00:00:01
Fold 3 Epoch 044; Train loss: 0.0002; Time: 00:00:01
Fold 3 Epoch 045; Train loss: 0.0001; Time: 00:00:01
Fold 3 Epoch 046; Train loss: 0.0002; Time: 00:00:01
Fold 3 Epoch 047; Train loss: 0.0001; Time: 00:00:02
Fold 3 Epoch 048; Train loss: 0.0001; Time: 00:00:01
Fold 3 Epoch 049; Train loss: 0.0001; Time: 00:00:01
Fold 3 Epoch 050; Train loss: 0.0001; Time: 00:00:01
Fold 3: Evaluation at Epoch 50
HR@5       NDCG@5     HR@10      NDCG@10   
0.562914   0.278095   0.855960   0.381322  
Loss: 0.0003
/home/nit/Desktop/2PDreamRec/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:243: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
Fold 3 Epoch 051; Train loss: 0.0001; Time: 00:00:01
Fold 3 Epoch 052; Train loss: 0.0001; Time: 00:00:01
Fold 3 Epoch 053; Train loss: 0.0001; Time: 00:00:01
Fold 3 Epoch 054; Train loss: 0.0002; Time: 00:00:01
Fold 3 Epoch 055; Train loss: 0.0003; Time: 00:00:01
Fold 3 Epoch 056; Train loss: 0.0003; Time: 00:00:01
Fold 3 Epoch 057; Train loss: 0.0001; Time: 00:00:01
Fold 3 Epoch 058; Train loss: 0.0001; Time: 00:00:01
Fold 3 Epoch 059; Train loss: 0.0001; Time: 00:00:01
Fold 3 Epoch 060; Train loss: 0.0001; Time: 00:00:02
Fold 3: Evaluation at Epoch 60
HR@5       NDCG@5     HR@10      NDCG@10   
0.556291   0.395300   0.895695   0.506470  
Loss: 0.0001
/home/nit/Desktop/2PDreamRec/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:243: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
Fold 3 Epoch 061; Train loss: 0.0000; Time: 00:00:02
Fold 3 Epoch 062; Train loss: 0.0000; Time: 00:00:01
Fold 3 Epoch 063; Train loss: 0.0000; Time: 00:00:02
Fold 3 Epoch 064; Train loss: 0.0000; Time: 00:00:01
Fold 3 Epoch 065; Train loss: 0.0000; Time: 00:00:01
Fold 3 Epoch 066; Train loss: 0.0000; Time: 00:00:02
Fold 3 Epoch 067; Train loss: 0.0000; Time: 00:00:01
Fold 3 Epoch 068; Train loss: 0.0000; Time: 00:00:01
Fold 3 Epoch 069; Train loss: 0.0000; Time: 00:00:01
Fold 3 Epoch 070; Train loss: 0.0000; Time: 00:00:01
Fold 3: Evaluation at Epoch 70
HR@5       NDCG@5     HR@10      NDCG@10   
0.564570   0.288512   0.895695   0.402727  
Loss: 0.0001
/home/nit/Desktop/2PDreamRec/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:243: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
Fold 3 Epoch 071; Train loss: 0.0000; Time: 00:00:01
Fold 3 Epoch 072; Train loss: 0.0000; Time: 00:00:01
Fold 3 Epoch 073; Train loss: 0.0000; Time: 00:00:02
Fold 3 Epoch 074; Train loss: 0.0000; Time: 00:00:02
Fold 3 Epoch 075; Train loss: 0.0000; Time: 00:00:02
Fold 3 Epoch 076; Train loss: 0.0000; Time: 00:00:02
Fold 3 Epoch 077; Train loss: 0.0000; Time: 00:00:02
Fold 3 Epoch 078; Train loss: 0.0000; Time: 00:00:02
Fold 3 Epoch 079; Train loss: 0.0000; Time: 00:00:02
Fold 3 Epoch 080; Train loss: 0.0001; Time: 00:00:02
Fold 3: Evaluation at Epoch 80
HR@5       NDCG@5     HR@10      NDCG@10   
0.562914   0.336442   0.895695   0.445254  
Loss: 0.0001
/home/nit/Desktop/2PDreamRec/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:243: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
Fold 3 Epoch 081; Train loss: 0.0001; Time: 00:00:02
Fold 3 Epoch 082; Train loss: 0.0001; Time: 00:00:02
Fold 3 Epoch 083; Train loss: 0.0001; Time: 00:00:02
Fold 3 Epoch 084; Train loss: 0.0000; Time: 00:00:02
Fold 3 Epoch 085; Train loss: 0.0000; Time: 00:00:02
Fold 3 Epoch 086; Train loss: 0.0000; Time: 00:00:02
Fold 3 Epoch 087; Train loss: 0.0000; Time: 00:00:01
Fold 3 Epoch 088; Train loss: 0.0000; Time: 00:00:01
Fold 3 Epoch 089; Train loss: 0.0000; Time: 00:00:01
Fold 3 Epoch 090; Train loss: 0.0000; Time: 00:00:01
Fold 3: Evaluation at Epoch 90
HR@5       NDCG@5     HR@10      NDCG@10   
0.562914   0.322555   0.895695   0.437535  
Loss: 0.0000
/home/nit/Desktop/2PDreamRec/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:243: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
Fold 3 Epoch 091; Train loss: 0.0000; Time: 00:00:02
Fold 3 Epoch 092; Train loss: 0.0000; Time: 00:00:01
Fold 3 Epoch 093; Train loss: 0.0000; Time: 00:00:01
Fold 3 Epoch 094; Train loss: 0.0000; Time: 00:00:01
Fold 3 Epoch 095; Train loss: 0.0000; Time: 00:00:02
Fold 3 Epoch 096; Train loss: 0.0000; Time: 00:00:02
Fold 3 Epoch 097; Train loss: 0.0000; Time: 00:00:02
Fold 3 Epoch 098; Train loss: 0.0000; Time: 00:00:02
Fold 3 Epoch 099; Train loss: 0.0000; Time: 00:00:02
Fold 3 Epoch 100; Train loss: 0.0000; Time: 00:00:02
Fold 3: Evaluation at Epoch 100
HR@5       NDCG@5     HR@10      NDCG@10   
0.562914   0.287871   0.895695   0.402852  
Loss: 0.0000
/home/nit/Desktop/2PDreamRec/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:243: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
[lr candidate 0.01] Fold 3: NDCG@10 = [np.float64(0.4873544925860959), np.float64(0.5729529549592733), np.float64(0.4750196848629396), np.float64(0.4580327569622487), np.float64(0.3813219035976101), np.float64(0.506470434380521), np.float64(0.40272670920366405), np.float64(0.44525370570766104), np.float64(0.43753540862122786), np.float64(0.40285203019170407)]
Tuning lr:  60%|██████    | 3/5 [11:56<07:50, 235.29s/it]
========== Fold 3 ==========
INFO:root:Using statics: seq_size = 10, genre_vocab_size = 18
Fold 3 Epoch 001; Train loss: 0.6052; Time: 00:00:02
Fold 3 Epoch 002; Train loss: 0.2519; Time: 00:00:02
Fold 3 Epoch 003; Train loss: 0.1870; Time: 00:00:01
Fold 3 Epoch 004; Train loss: 0.1670; Time: 00:00:01
Fold 3 Epoch 005; Train loss: 0.1514; Time: 00:00:02
Fold 3 Epoch 006; Train loss: 0.1451; Time: 00:00:01
Fold 3 Epoch 007; Train loss: 0.1378; Time: 00:00:01
Fold 3 Epoch 008; Train loss: 0.1305; Time: 00:00:01
Fold 3 Epoch 009; Train loss: 0.1262; Time: 00:00:01
Fold 3 Epoch 010; Train loss: 0.1228; Time: 00:00:01
Fold 3: Evaluation at Epoch 10
HR@5       NDCG@5     HR@10      NDCG@10   
0.587748   0.472453   0.736755   0.519449  
Loss: 0.1554
/home/nit/Desktop/2PDreamRec/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:243: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
Fold 3 Epoch 011; Train loss: 0.1140; Time: 00:00:01
Fold 3 Epoch 012; Train loss: 0.1125; Time: 00:00:01
Fold 3 Epoch 013; Train loss: 0.1119; Time: 00:00:01
Fold 3 Epoch 014; Train loss: 0.1084; Time: 00:00:01
Fold 3 Epoch 015; Train loss: 0.1051; Time: 00:00:01
Fold 3 Epoch 016; Train loss: 0.1015; Time: 00:00:01
Fold 3 Epoch 017; Train loss: 0.0965; Time: 00:00:01
Fold 3 Epoch 018; Train loss: 0.0976; Time: 00:00:01
Fold 3 Epoch 019; Train loss: 0.0926; Time: 00:00:01
Fold 3 Epoch 020; Train loss: 0.0898; Time: 00:00:01
Fold 3: Evaluation at Epoch 20
HR@5       NDCG@5     HR@10      NDCG@10   
0.539735   0.428239   0.740066   0.492463  
Loss: 0.1037
/home/nit/Desktop/2PDreamRec/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:243: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
Fold 3 Epoch 021; Train loss: 0.0841; Time: 00:00:01
Fold 3 Epoch 022; Train loss: 0.0842; Time: 00:00:02
Fold 3 Epoch 023; Train loss: 0.0823; Time: 00:00:01
Fold 3 Epoch 024; Train loss: 0.0769; Time: 00:00:02
Fold 3 Epoch 025; Train loss: 0.0751; Time: 00:00:01
Fold 3 Epoch 026; Train loss: 0.0744; Time: 00:00:01
Fold 3 Epoch 027; Train loss: 0.0705; Time: 00:00:01
Fold 3 Epoch 028; Train loss: 0.0681; Time: 00:00:01
Fold 3 Epoch 029; Train loss: 0.0661; Time: 00:00:02
Fold 3 Epoch 030; Train loss: 0.0634; Time: 00:00:02
Fold 3: Evaluation at Epoch 30
HR@5       NDCG@5     HR@10      NDCG@10   
0.576159   0.415366   0.754967   0.472491  
Loss: 0.0707
/home/nit/Desktop/2PDreamRec/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:243: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
Fold 3 Epoch 031; Train loss: 0.0591; Time: 00:00:02
Fold 3 Epoch 032; Train loss: 0.0591; Time: 00:00:02
Fold 3 Epoch 033; Train loss: 0.0562; Time: 00:00:02
Fold 3 Epoch 034; Train loss: 0.0542; Time: 00:00:02
Fold 3 Epoch 035; Train loss: 0.0522; Time: 00:00:02
Fold 3 Epoch 036; Train loss: 0.0496; Time: 00:00:02
Fold 3 Epoch 037; Train loss: 0.0491; Time: 00:00:02
Fold 3 Epoch 038; Train loss: 0.0446; Time: 00:00:02
Fold 3 Epoch 039; Train loss: 0.0425; Time: 00:00:02
Fold 3 Epoch 040; Train loss: 0.0407; Time: 00:00:02
Fold 3: Evaluation at Epoch 40
HR@5       NDCG@5     HR@10      NDCG@10   
0.556291   0.393550   0.736755   0.451222  
Loss: 0.0450
/home/nit/Desktop/2PDreamRec/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:243: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
Fold 3 Epoch 041; Train loss: 0.0396; Time: 00:00:01
Fold 3 Epoch 042; Train loss: 0.0369; Time: 00:00:01
Fold 3 Epoch 043; Train loss: 0.0353; Time: 00:00:02
Fold 3 Epoch 044; Train loss: 0.0321; Time: 00:00:01
Fold 3 Epoch 045; Train loss: 0.0307; Time: 00:00:01
Fold 3 Epoch 046; Train loss: 0.0289; Time: 00:00:02
Fold 3 Epoch 047; Train loss: 0.0272; Time: 00:00:01
Fold 3 Epoch 048; Train loss: 0.0237; Time: 00:00:02
Fold 3 Epoch 049; Train loss: 0.0231; Time: 00:00:02
Fold 3 Epoch 050; Train loss: 0.0207; Time: 00:00:02
Fold 3: Evaluation at Epoch 50
HR@5       NDCG@5     HR@10      NDCG@10   
0.360927   0.201344   0.811258   0.344202  
Loss: 0.0236
/home/nit/Desktop/2PDreamRec/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:243: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
Fold 3 Epoch 051; Train loss: 0.0198; Time: 00:00:01
Fold 3 Epoch 052; Train loss: 0.0180; Time: 00:00:01
Fold 3 Epoch 053; Train loss: 0.0161; Time: 00:00:02
Fold 3 Epoch 054; Train loss: 0.0151; Time: 00:00:01
Fold 3 Epoch 055; Train loss: 0.0140; Time: 00:00:01
Fold 3 Epoch 056; Train loss: 0.0129; Time: 00:00:01
Fold 3 Epoch 057; Train loss: 0.0117; Time: 00:00:01
Fold 3 Epoch 058; Train loss: 0.0109; Time: 00:00:01
Fold 3 Epoch 059; Train loss: 0.0104; Time: 00:00:01
Fold 3 Epoch 060; Train loss: 0.0098; Time: 00:00:01
Fold 3: Evaluation at Epoch 60
HR@5       NDCG@5     HR@10      NDCG@10   
0.413907   0.229841   0.874172   0.377659  
Loss: 0.0085
/home/nit/Desktop/2PDreamRec/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:243: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
Fold 3 Epoch 061; Train loss: 0.0094; Time: 00:00:01
Fold 3 Epoch 062; Train loss: 0.0090; Time: 00:00:02
Fold 3 Epoch 063; Train loss: 0.0084; Time: 00:00:02
Fold 3 Epoch 064; Train loss: 0.0078; Time: 00:00:02
Fold 3 Epoch 065; Train loss: 0.0069; Time: 00:00:02
Fold 3 Epoch 066; Train loss: 0.0065; Time: 00:00:02
Fold 3 Epoch 067; Train loss: 0.0058; Time: 00:00:01
Fold 3 Epoch 068; Train loss: 0.0054; Time: 00:00:01
Fold 3 Epoch 069; Train loss: 0.0050; Time: 00:00:01
Fold 3 Epoch 070; Train loss: 0.0046; Time: 00:00:01
Fold 3: Evaluation at Epoch 70
HR@5       NDCG@5     HR@10      NDCG@10   
0.519868   0.262468   0.849338   0.367435  
Loss: 0.0037
/home/nit/Desktop/2PDreamRec/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:243: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
Fold 3 Epoch 071; Train loss: 0.0041; Time: 00:00:02
Fold 3 Epoch 072; Train loss: 0.0034; Time: 00:00:01
Fold 3 Epoch 073; Train loss: 0.0032; Time: 00:00:02
Fold 3 Epoch 074; Train loss: 0.0030; Time: 00:00:02
Fold 3 Epoch 075; Train loss: 0.0023; Time: 00:00:02
Fold 3 Epoch 076; Train loss: 0.0018; Time: 00:00:02
Fold 3 Epoch 077; Train loss: 0.0014; Time: 00:00:02
Fold 3 Epoch 078; Train loss: 0.0012; Time: 00:00:02
Fold 3 Epoch 079; Train loss: 0.0010; Time: 00:00:02
Fold 3 Epoch 080; Train loss: 0.0007; Time: 00:00:02
Fold 3: Evaluation at Epoch 80
HR@5       NDCG@5     HR@10      NDCG@10   
0.819536   0.408585   0.855960   0.420063  
Loss: 0.0010
/home/nit/Desktop/2PDreamRec/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:243: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
Fold 3 Epoch 081; Train loss: 0.0006; Time: 00:00:02
Fold 3 Epoch 082; Train loss: 0.0005; Time: 00:00:02
Fold 3 Epoch 083; Train loss: 0.0004; Time: 00:00:02
Fold 3 Epoch 084; Train loss: 0.0003; Time: 00:00:02
Fold 3 Epoch 085; Train loss: 0.0003; Time: 00:00:02
Fold 3 Epoch 086; Train loss: 0.0003; Time: 00:00:02
Fold 3 Epoch 087; Train loss: 0.0003; Time: 00:00:02
Fold 3 Epoch 088; Train loss: 0.0003; Time: 00:00:02
Fold 3 Epoch 089; Train loss: 0.0002; Time: 00:00:02
Fold 3 Epoch 090; Train loss: 0.0002; Time: 00:00:02
Fold 3: Evaluation at Epoch 90
HR@5       NDCG@5     HR@10      NDCG@10   
0.562914   0.273618   0.855960   0.377113  
Loss: 0.0005
/home/nit/Desktop/2PDreamRec/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:243: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
Fold 3 Epoch 091; Train loss: 0.0002; Time: 00:00:02
Fold 3 Epoch 092; Train loss: 0.0002; Time: 00:00:01
Fold 3 Epoch 093; Train loss: 0.0002; Time: 00:00:01
Fold 3 Epoch 094; Train loss: 0.0001; Time: 00:00:01
Fold 3 Epoch 095; Train loss: 0.0001; Time: 00:00:02
Fold 3 Epoch 096; Train loss: 0.0001; Time: 00:00:02
Fold 3 Epoch 097; Train loss: 0.0001; Time: 00:00:02
Fold 3 Epoch 098; Train loss: 0.0001; Time: 00:00:02
Fold 3 Epoch 099; Train loss: 0.0001; Time: 00:00:02
Fold 3 Epoch 100; Train loss: 0.0000; Time: 00:00:02
Fold 3: Evaluation at Epoch 100
HR@5       NDCG@5     HR@10      NDCG@10   
0.564570   0.278735   0.862583   0.377722  
Loss: 0.0001
/home/nit/Desktop/2PDreamRec/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:243: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
[lr candidate 0.005] Fold 3: NDCG@10 = [np.float64(0.51944861518997), np.float64(0.4924628114654265), np.float64(0.4724913141561446), np.float64(0.4512224687484872), np.float64(0.34420246860054265), np.float64(0.3776593800519125), np.float64(0.3674351001914112), np.float64(0.42006335106637466), np.float64(0.3771131328247776), np.float64(0.37772231441291265)]
Tuning lr:  80%|████████  | 4/5 [15:45<03:52, 232.73s/it]
========== Fold 3 ==========
INFO:root:Using statics: seq_size = 10, genre_vocab_size = 18
Fold 3 Epoch 001; Train loss: 0.8848; Time: 00:00:02
Fold 3 Epoch 002; Train loss: 0.6126; Time: 00:00:02
Fold 3 Epoch 003; Train loss: 0.4657; Time: 00:00:02
Fold 3 Epoch 004; Train loss: 0.3802; Time: 00:00:01
Tuning lr:  80%|████████  | 4/5 [15:54<03:58, 238.63s/it]
Traceback (most recent call last):
  File "/home/nit/Desktop/2PDreamRec/DreamRec_Gen.py", line 482, in <module>
    main()
  File "/home/nit/Desktop/2PDreamRec/DreamRec_Gen.py", line 315, in main
    fm = train_fold(tuning_fold)
  File "/home/nit/Desktop/2PDreamRec/DreamRec_Gen.py", line 193, in train_fold
    loss.backward()
  File "/home/nit/Desktop/2PDreamRec/.venv/lib/python3.10/site-packages/torch/_tensor.py", line 626, in backward
    torch.autograd.backward(
  File "/home/nit/Desktop/2PDreamRec/.venv/lib/python3.10/site-packages/torch/autograd/__init__.py", line 347, in backward
    _engine_run_backward(
  File "/home/nit/Desktop/2PDreamRec/.venv/lib/python3.10/site-packages/torch/autograd/graph.py", line 823, in _engine_run_backward
    return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
KeyboardInterrupt
Tuning lr:   0%|          | 0/5 [00:00<?, ?it/s]
========== Fold 3 ==========
INFO:root:Using statics: seq_size = 10, genre_vocab_size = 18
Fold 3 Epoch 001; Train loss: 2223.8496; Time: 00:00:03
Fold 3 Epoch 002; Train loss: 9.9596; Time: 00:00:01
Fold 3 Epoch 003; Train loss: 1.5321; Time: 00:00:02
Fold 3 Epoch 004; Train loss: 1.1314; Time: 00:00:01
Fold 3 Epoch 005; Train loss: 0.8961; Time: 00:00:02
Fold 3 Epoch 006; Train loss: 1.0444; Time: 00:00:02
Fold 3 Epoch 007; Train loss: 0.7289; Time: 00:00:02
Fold 3 Epoch 008; Train loss: 0.4683; Time: 00:00:02
Fold 3 Epoch 009; Train loss: 0.3959; Time: 00:00:02
Fold 3 Epoch 010; Train loss: 0.3385; Time: 00:00:02
Fold 3: Evaluation at Epoch 10
HR@5       NDCG@5     HR@10      NDCG@10   
0.551325   0.403919   0.658940   0.438284  
Loss: 0.3108
/home/nit/Desktop/2PDreamRec/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:243: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
Fold 3 Epoch 011; Train loss: 0.3036; Time: 00:00:02
Fold 3 Epoch 012; Train loss: 0.2614; Time: 00:00:01
Fold 3 Epoch 013; Train loss: 0.2428; Time: 00:00:01
Fold 3 Epoch 014; Train loss: 0.2284; Time: 00:00:01
Fold 3 Epoch 015; Train loss: 0.2257; Time: 00:00:01
Fold 3 Epoch 016; Train loss: 0.2985; Time: 00:00:02
Fold 3 Epoch 017; Train loss: 0.2651; Time: 00:00:02
Fold 3 Epoch 018; Train loss: 0.1780; Time: 00:00:01
Fold 3 Epoch 019; Train loss: 0.1710; Time: 00:00:01
Fold 3 Epoch 020; Train loss: 0.1609; Time: 00:00:02
Fold 3: Evaluation at Epoch 20
HR@5       NDCG@5     HR@10      NDCG@10   
0.534768   0.392353   0.645695   0.428146  
Loss: 0.1685
/home/nit/Desktop/2PDreamRec/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:243: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
Fold 3 Epoch 021; Train loss: 0.1529; Time: 00:00:02
Fold 3 Epoch 022; Train loss: 0.1506; Time: 00:00:02
Fold 3 Epoch 023; Train loss: 0.1507; Time: 00:00:02
Fold 3 Epoch 024; Train loss: 0.1488; Time: 00:00:03
Fold 3 Epoch 025; Train loss: 0.1918; Time: 00:00:02
Fold 3 Epoch 026; Train loss: 0.2350; Time: 00:00:02
Fold 3 Epoch 027; Train loss: 0.1396; Time: 00:00:02
Fold 3 Epoch 028; Train loss: 0.1243; Time: 00:00:02
Fold 3 Epoch 029; Train loss: 0.1191; Time: 00:00:01
Fold 3 Epoch 030; Train loss: 0.1184; Time: 00:00:01
Fold 3: Evaluation at Epoch 30
HR@5       NDCG@5     HR@10      NDCG@10   
0.488411   0.325754   0.637417   0.373927  
Loss: 0.1353
/home/nit/Desktop/2PDreamRec/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:243: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
Fold 3 Epoch 031; Train loss: 0.1149; Time: 00:00:01
Fold 3 Epoch 032; Train loss: 0.1144; Time: 00:00:01
Fold 3 Epoch 033; Train loss: 0.1083; Time: 00:00:01
Fold 3 Epoch 034; Train loss: 0.1078; Time: 00:00:02
Fold 3 Epoch 035; Train loss: 0.1063; Time: 00:00:01
Fold 3 Epoch 036; Train loss: 0.1037; Time: 00:00:02
Fold 3 Epoch 037; Train loss: 0.1045; Time: 00:00:02
Fold 3 Epoch 038; Train loss: 0.1015; Time: 00:00:02
Fold 3 Epoch 039; Train loss: 0.1084; Time: 00:00:02
Fold 3 Epoch 040; Train loss: 0.1273; Time: 00:00:02
Fold 3: Evaluation at Epoch 40
HR@5       NDCG@5     HR@10      NDCG@10   
0.523179   0.390197   0.642384   0.428557  
Loss: 0.1384
/home/nit/Desktop/2PDreamRec/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:243: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
Fold 3 Epoch 041; Train loss: 0.1041; Time: 00:00:02
Fold 3 Epoch 042; Train loss: 0.0954; Time: 00:00:02
Fold 3 Epoch 043; Train loss: 0.0935; Time: 00:00:02
Fold 3 Epoch 044; Train loss: 0.0899; Time: 00:00:01
Fold 3 Epoch 045; Train loss: 0.0905; Time: 00:00:02
Fold 3 Epoch 046; Train loss: 0.0856; Time: 00:00:02
Fold 3 Epoch 047; Train loss: 0.0866; Time: 00:00:02
Fold 3 Epoch 048; Train loss: 0.0840; Time: 00:00:02
Fold 3 Epoch 049; Train loss: 0.0925; Time: 00:00:02
Fold 3 Epoch 050; Train loss: 0.1178; Time: 00:00:02
Fold 3: Evaluation at Epoch 50
HR@5       NDCG@5     HR@10      NDCG@10   
0.250000   0.157311   0.384106   0.200185  
Loss: 0.1262
/home/nit/Desktop/2PDreamRec/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:243: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
Fold 3 Epoch 051; Train loss: 0.1249; Time: 00:00:02
Fold 3 Epoch 052; Train loss: 0.0883; Time: 00:00:02
Fold 3 Epoch 053; Train loss: 0.0770; Time: 00:00:02
Fold 3 Epoch 054; Train loss: 0.0753; Time: 00:00:02
Fold 3 Epoch 055; Train loss: 0.0743; Time: 00:00:02
Fold 3 Epoch 056; Train loss: 0.0735; Time: 00:00:02
Fold 3 Epoch 057; Train loss: 0.0736; Time: 00:00:01
Fold 3 Epoch 058; Train loss: 0.0721; Time: 00:00:01
Fold 3 Epoch 059; Train loss: 0.0778; Time: 00:00:02
Fold 3 Epoch 060; Train loss: 0.0883; Time: 00:00:02
Fold 3: Evaluation at Epoch 60
HR@5       NDCG@5     HR@10      NDCG@10   
0.476821   0.344349   0.620861   0.390655  
Loss: 0.1226
/home/nit/Desktop/2PDreamRec/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:243: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
Fold 3 Epoch 061; Train loss: 0.0939; Time: 00:00:02
Fold 3 Epoch 062; Train loss: 0.0759; Time: 00:00:01
Fold 3 Epoch 063; Train loss: 0.0675; Time: 00:00:02
Fold 3 Epoch 064; Train loss: 0.0658; Time: 00:00:02
Fold 3 Epoch 065; Train loss: 0.0625; Time: 00:00:02
Fold 3 Epoch 066; Train loss: 0.0635; Time: 00:00:01
Fold 3 Epoch 067; Train loss: 0.0613; Time: 00:00:02
Fold 3 Epoch 068; Train loss: 0.0594; Time: 00:00:01
Fold 3 Epoch 069; Train loss: 0.0626; Time: 00:00:01
Fold 3 Epoch 070; Train loss: 0.0926; Time: 00:00:01
Fold 3: Evaluation at Epoch 70
HR@5       NDCG@5     HR@10      NDCG@10   
0.529801   0.369449   0.662252   0.412024  
Loss: 0.1659
/home/nit/Desktop/2PDreamRec/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:243: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
Fold 3 Epoch 071; Train loss: 0.1106; Time: 00:00:02
Fold 3 Epoch 072; Train loss: 0.0654; Time: 00:00:02
Fold 3 Epoch 073; Train loss: 0.0558; Time: 00:00:02
Fold 3 Epoch 074; Train loss: 0.0537; Time: 00:00:02
Fold 3 Epoch 075; Train loss: 0.0534; Time: 00:00:02
Fold 3 Epoch 076; Train loss: 0.0534; Time: 00:00:02
Fold 3 Epoch 077; Train loss: 0.0506; Time: 00:00:02
Fold 3 Epoch 078; Train loss: 0.0495; Time: 00:00:02
Fold 3 Epoch 079; Train loss: 0.0497; Time: 00:00:02
Fold 3 Epoch 080; Train loss: 0.0497; Time: 00:00:01
Fold 3: Evaluation at Epoch 80
HR@5       NDCG@5     HR@10      NDCG@10   
0.465232   0.317275   0.594371   0.358500  
Loss: 0.0608
/home/nit/Desktop/2PDreamRec/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:243: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
[lr candidate 0.1] Fold 3: NDCG@10 = [np.float64(0.43828359044938225), np.float64(0.42814629273739957), np.float64(0.37392749851283147), np.float64(0.4285567549615882), np.float64(0.20018487543345664), np.float64(0.39065511709087014), np.float64(0.4120244838374548), np.float64(0.3584999126447616)]
Tuning lr:  20%|██        | 1/5 [03:16<13:07, 196.96s/it]
========== Fold 3 ==========
INFO:root:Using statics: seq_size = 10, genre_vocab_size = 18
Fold 3 Epoch 001; Train loss: 27.6160; Time: 00:00:02
Fold 3 Epoch 002; Train loss: 0.7087; Time: 00:00:01
Fold 3 Epoch 003; Train loss: 0.5946; Time: 00:00:01
Fold 3 Epoch 004; Train loss: 0.1872; Time: 00:00:02
Fold 3 Epoch 005; Train loss: 0.1605; Time: 00:00:02
Fold 3 Epoch 006; Train loss: 0.1416; Time: 00:00:02
Fold 3 Epoch 007; Train loss: 0.1298; Time: 00:00:01
Fold 3 Epoch 008; Train loss: 0.1201; Time: 00:00:01
Fold 3 Epoch 009; Train loss: 0.1109; Time: 00:00:01
Fold 3 Epoch 010; Train loss: 0.1062; Time: 00:00:01
Fold 3: Evaluation at Epoch 10
HR@5       NDCG@5     HR@10      NDCG@10   
0.259934   0.173007   0.347682   0.201250  
Loss: 0.1161
/home/nit/Desktop/2PDreamRec/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:243: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
Fold 3 Epoch 011; Train loss: 0.0970; Time: 00:00:02
Fold 3 Epoch 012; Train loss: 0.0900; Time: 00:00:02
Fold 3 Epoch 013; Train loss: 0.0820; Time: 00:00:02
Fold 3 Epoch 014; Train loss: 0.0794; Time: 00:00:02
Fold 3 Epoch 015; Train loss: 0.0704; Time: 00:00:02
Fold 3 Epoch 016; Train loss: 0.0690; Time: 00:00:02
Fold 3 Epoch 017; Train loss: 0.0624; Time: 00:00:01
Fold 3 Epoch 018; Train loss: 0.0595; Time: 00:00:01
Fold 3 Epoch 019; Train loss: 0.0550; Time: 00:00:01
Fold 3 Epoch 020; Train loss: 0.0507; Time: 00:00:01
Fold 3: Evaluation at Epoch 20
HR@5       NDCG@5     HR@10      NDCG@10   
0.317881   0.241172   0.557947   0.316125  
Loss: 0.0600
/home/nit/Desktop/2PDreamRec/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:243: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
Fold 3 Epoch 021; Train loss: 0.0480; Time: 00:00:01
Fold 3 Epoch 022; Train loss: 0.0459; Time: 00:00:02
Fold 3 Epoch 023; Train loss: 0.0421; Time: 00:00:02
Fold 3 Epoch 024; Train loss: 0.0411; Time: 00:00:02
Fold 3 Epoch 025; Train loss: 0.0404; Time: 00:00:02
Fold 3 Epoch 026; Train loss: 0.0422; Time: 00:00:02
Fold 3 Epoch 027; Train loss: 0.0451; Time: 00:00:02
Fold 3 Epoch 028; Train loss: 0.0335; Time: 00:00:02
Fold 3 Epoch 029; Train loss: 0.0296; Time: 00:00:01
Fold 3 Epoch 030; Train loss: 0.0273; Time: 00:00:01
Fold 3: Evaluation at Epoch 30
HR@5       NDCG@5     HR@10      NDCG@10   
0.400662   0.269079   0.710265   0.367077  
Loss: 0.0248
/home/nit/Desktop/2PDreamRec/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:243: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
Fold 3 Epoch 031; Train loss: 0.0267; Time: 00:00:01
Fold 3 Epoch 032; Train loss: 0.0252; Time: 00:00:01
Fold 3 Epoch 033; Train loss: 0.0241; Time: 00:00:02
Fold 3 Epoch 034; Train loss: 0.0222; Time: 00:00:02
Fold 3 Epoch 035; Train loss: 0.0222; Time: 00:00:01
Fold 3 Epoch 036; Train loss: 0.0222; Time: 00:00:01
Fold 3 Epoch 037; Train loss: 0.0263; Time: 00:00:01
Fold 3 Epoch 038; Train loss: 0.0287; Time: 00:00:01
Fold 3 Epoch 039; Train loss: 0.0189; Time: 00:00:01
Fold 3 Epoch 040; Train loss: 0.0160; Time: 00:00:02
Fold 3: Evaluation at Epoch 40
HR@5       NDCG@5     HR@10      NDCG@10   
0.480132   0.284427   0.740066   0.370190  
Loss: 0.0114
/home/nit/Desktop/2PDreamRec/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:243: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
Fold 3 Epoch 041; Train loss: 0.0140; Time: 00:00:02
Fold 3 Epoch 042; Train loss: 0.0121; Time: 00:00:01
Fold 3 Epoch 043; Train loss: 0.0102; Time: 00:00:01
Fold 3 Epoch 044; Train loss: 0.0081; Time: 00:00:01
Fold 3 Epoch 045; Train loss: 0.0061; Time: 00:00:01
Fold 3 Epoch 046; Train loss: 0.0041; Time: 00:00:02
Fold 3 Epoch 047; Train loss: 0.0027; Time: 00:00:02
Fold 3 Epoch 048; Train loss: 0.0021; Time: 00:00:02
Fold 3 Epoch 049; Train loss: 0.0039; Time: 00:00:01
Fold 3 Epoch 050; Train loss: 0.0109; Time: 00:00:01
Fold 3: Evaluation at Epoch 50
HR@5       NDCG@5     HR@10      NDCG@10   
0.554636   0.309376   0.902318   0.429202  
Loss: 0.0138
/home/nit/Desktop/2PDreamRec/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:243: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
Fold 3 Epoch 051; Train loss: 0.0053; Time: 00:00:02
Fold 3 Epoch 052; Train loss: 0.0010; Time: 00:00:02
Fold 3 Epoch 053; Train loss: 0.0003; Time: 00:00:02
Fold 3 Epoch 054; Train loss: 0.0003; Time: 00:00:02
Fold 3 Epoch 055; Train loss: 0.0002; Time: 00:00:02
Fold 3 Epoch 056; Train loss: 0.0002; Time: 00:00:02
Fold 3 Epoch 057; Train loss: 0.0002; Time: 00:00:02
Fold 3 Epoch 058; Train loss: 0.0002; Time: 00:00:02
Fold 3 Epoch 059; Train loss: 0.0002; Time: 00:00:02
Fold 3 Epoch 060; Train loss: 0.0003; Time: 00:00:01
Fold 3: Evaluation at Epoch 60
HR@5       NDCG@5     HR@10      NDCG@10   
0.819536   0.421830   0.855960   0.433914  
Loss: 0.0006
/home/nit/Desktop/2PDreamRec/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:243: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
Fold 3 Epoch 061; Train loss: 0.0006; Time: 00:00:01
Fold 3 Epoch 062; Train loss: 0.0019; Time: 00:00:01
Fold 3 Epoch 063; Train loss: 0.0066; Time: 00:00:01
Fold 3 Epoch 064; Train loss: 0.0057; Time: 00:00:01
Fold 3 Epoch 065; Train loss: 0.0009; Time: 00:00:02
Fold 3 Epoch 066; Train loss: 0.0002; Time: 00:00:01
Fold 3 Epoch 067; Train loss: 0.0002; Time: 00:00:02
Fold 3 Epoch 068; Train loss: 0.0002; Time: 00:00:02
Fold 3 Epoch 069; Train loss: 0.0002; Time: 00:00:02
Fold 3 Epoch 070; Train loss: 0.0001; Time: 00:00:02
Fold 3: Evaluation at Epoch 70
HR@5       NDCG@5     HR@10      NDCG@10   
0.562914   0.410545   0.885762   0.522654  
Loss: 0.0006
/home/nit/Desktop/2PDreamRec/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:243: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
Fold 3 Epoch 071; Train loss: 0.0001; Time: 00:00:02
Fold 3 Epoch 072; Train loss: 0.0001; Time: 00:00:02
Fold 3 Epoch 073; Train loss: 0.0002; Time: 00:00:02
Fold 3 Epoch 074; Train loss: 0.0002; Time: 00:00:02
Fold 3 Epoch 075; Train loss: 0.0002; Time: 00:00:02
Fold 3 Epoch 076; Train loss: 0.0004; Time: 00:00:02
Fold 3 Epoch 077; Train loss: 0.0015; Time: 00:00:02
Fold 3 Epoch 078; Train loss: 0.0049; Time: 00:00:01
Fold 3 Epoch 079; Train loss: 0.0047; Time: 00:00:01
Fold 3 Epoch 080; Train loss: 0.0013; Time: 00:00:01
Fold 3: Evaluation at Epoch 80
HR@5       NDCG@5     HR@10      NDCG@10   
0.354305   0.187164   0.885762   0.355135  
Loss: 0.0009
/home/nit/Desktop/2PDreamRec/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:243: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
[lr candidate 0.05] Fold 3: NDCG@10 = [np.float64(0.20125027483672578), np.float64(0.316124654709142), np.float64(0.3670767133677105), np.float64(0.3701901494030425), np.float64(0.42920180607524266), np.float64(0.433913745190732), np.float64(0.5226541611507028), np.float64(0.3551351305582033)]
Tuning lr:  40%|████      | 2/5 [06:22<09:30, 190.09s/it]
========== Fold 3 ==========
INFO:root:Using statics: seq_size = 10, genre_vocab_size = 18
Fold 3 Epoch 001; Train loss: 0.5112; Time: 00:00:01
Fold 3 Epoch 002; Train loss: 0.2026; Time: 00:00:02
Fold 3 Epoch 003; Train loss: 0.1577; Time: 00:00:02
Fold 3 Epoch 004; Train loss: 0.1444; Time: 00:00:01
Fold 3 Epoch 005; Train loss: 0.1297; Time: 00:00:01
Fold 3 Epoch 006; Train loss: 0.1206; Time: 00:00:01
Fold 3 Epoch 007; Train loss: 0.1140; Time: 00:00:01
Fold 3 Epoch 008; Train loss: 0.1079; Time: 00:00:01
Fold 3 Epoch 009; Train loss: 0.1065; Time: 00:00:01
Fold 3 Epoch 010; Train loss: 0.0976; Time: 00:00:01
Fold 3: Evaluation at Epoch 10
HR@5       NDCG@5     HR@10      NDCG@10   
0.493377   0.341446   0.723510   0.414871  
Loss: 0.1025
/home/nit/Desktop/2PDreamRec/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:243: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
Fold 3 Epoch 011; Train loss: 0.0944; Time: 00:00:02
Fold 3 Epoch 012; Train loss: 0.0863; Time: 00:00:01
Fold 3 Epoch 013; Train loss: 0.0821; Time: 00:00:01
Fold 3 Epoch 014; Train loss: 0.0787; Time: 00:00:01
Fold 3 Epoch 015; Train loss: 0.0737; Time: 00:00:01
Fold 3 Epoch 016; Train loss: 0.0670; Time: 00:00:01
Fold 3 Epoch 017; Train loss: 0.0642; Time: 00:00:02
Fold 3 Epoch 018; Train loss: 0.0591; Time: 00:00:02
Fold 3 Epoch 019; Train loss: 0.0536; Time: 00:00:02
Fold 3 Epoch 020; Train loss: 0.0492; Time: 00:00:02
Fold 3: Evaluation at Epoch 20
HR@5       NDCG@5     HR@10      NDCG@10   
0.635762   0.426961   0.786424   0.476575  
Loss: 0.0570
/home/nit/Desktop/2PDreamRec/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:243: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
Fold 3 Epoch 021; Train loss: 0.0458; Time: 00:00:02
Fold 3 Epoch 022; Train loss: 0.0419; Time: 00:00:01
Fold 3 Epoch 023; Train loss: 0.0408; Time: 00:00:01
Fold 3 Epoch 024; Train loss: 0.0362; Time: 00:00:02
Fold 3 Epoch 025; Train loss: 0.0328; Time: 00:00:01
Fold 3 Epoch 026; Train loss: 0.0274; Time: 00:00:02
Fold 3 Epoch 027; Train loss: 0.0240; Time: 00:00:02
Fold 3 Epoch 028; Train loss: 0.0212; Time: 00:00:01
Fold 3 Epoch 029; Train loss: 0.0198; Time: 00:00:01
Fold 3 Epoch 030; Train loss: 0.0183; Time: 00:00:02
Fold 3: Evaluation at Epoch 30
HR@5       NDCG@5     HR@10      NDCG@10   
0.769868   0.406904   0.867550   0.437909  
Loss: 0.0136
/home/nit/Desktop/2PDreamRec/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:243: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
Fold 3 Epoch 031; Train loss: 0.0151; Time: 00:00:02
Fold 3 Epoch 032; Train loss: 0.0126; Time: 00:00:02
Fold 3 Epoch 033; Train loss: 0.0106; Time: 00:00:02
Fold 3 Epoch 034; Train loss: 0.0089; Time: 00:00:01
Fold 3 Epoch 035; Train loss: 0.0078; Time: 00:00:01
Fold 3 Epoch 036; Train loss: 0.0066; Time: 00:00:01
Fold 3 Epoch 037; Train loss: 0.0054; Time: 00:00:01
Fold 3 Epoch 038; Train loss: 0.0043; Time: 00:00:01
Fold 3 Epoch 039; Train loss: 0.0048; Time: 00:00:01
Fold 3 Epoch 040; Train loss: 0.0033; Time: 00:00:01
Fold 3: Evaluation at Epoch 40
HR@5       NDCG@5     HR@10      NDCG@10   
0.596026   0.328621   0.850993   0.417698  
Loss: 0.0024
/home/nit/Desktop/2PDreamRec/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:243: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
Fold 3 Epoch 041; Train loss: 0.0015; Time: 00:00:02
Fold 3 Epoch 042; Train loss: 0.0009; Time: 00:00:01
Fold 3 Epoch 043; Train loss: 0.0006; Time: 00:00:01
Fold 3 Epoch 044; Train loss: 0.0004; Time: 00:00:01
Fold 3 Epoch 045; Train loss: 0.0003; Time: 00:00:01
Fold 3 Epoch 046; Train loss: 0.0002; Time: 00:00:01
Fold 3 Epoch 047; Train loss: 0.0002; Time: 00:00:01
Fold 3 Epoch 048; Train loss: 0.0002; Time: 00:00:01
Fold 3 Epoch 049; Train loss: 0.0002; Time: 00:00:02
Fold 3 Epoch 050; Train loss: 0.0004; Time: 00:00:01
Fold 3: Evaluation at Epoch 50
HR@5       NDCG@5     HR@10      NDCG@10   
0.819536   0.408585   0.855960   0.420401  
Loss: 0.0005
/home/nit/Desktop/2PDreamRec/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:243: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
Fold 3 Epoch 051; Train loss: 0.0004; Time: 00:00:01
Fold 3 Epoch 052; Train loss: 0.0003; Time: 00:00:01
Fold 3 Epoch 053; Train loss: 0.0002; Time: 00:00:02
Fold 3 Epoch 054; Train loss: 0.0002; Time: 00:00:02
Fold 3 Epoch 055; Train loss: 0.0001; Time: 00:00:02
Fold 3 Epoch 056; Train loss: 0.0002; Time: 00:00:02
Fold 3 Epoch 057; Train loss: 0.0002; Time: 00:00:01
Fold 3 Epoch 058; Train loss: 0.0001; Time: 00:00:02
Fold 3 Epoch 059; Train loss: 0.0001; Time: 00:00:01
Fold 3 Epoch 060; Train loss: 0.0001; Time: 00:00:01
Fold 3: Evaluation at Epoch 60
HR@5       NDCG@5     HR@10      NDCG@10   
0.564570   0.288512   0.855960   0.385585  
Loss: 0.0001
/home/nit/Desktop/2PDreamRec/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:243: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
Fold 3 Epoch 061; Train loss: 0.0001; Time: 00:00:01
Fold 3 Epoch 062; Train loss: 0.0001; Time: 00:00:01
Fold 3 Epoch 063; Train loss: 0.0001; Time: 00:00:02
Fold 3 Epoch 064; Train loss: 0.0001; Time: 00:00:01
Fold 3 Epoch 065; Train loss: 0.0000; Time: 00:00:02
Fold 3 Epoch 066; Train loss: 0.0000; Time: 00:00:02
Fold 3 Epoch 067; Train loss: 0.0000; Time: 00:00:02
Fold 3 Epoch 068; Train loss: 0.0000; Time: 00:00:02
Fold 3 Epoch 069; Train loss: 0.0000; Time: 00:00:02
Fold 3 Epoch 070; Train loss: 0.0000; Time: 00:00:02
Fold 3: Evaluation at Epoch 70
HR@5       NDCG@5     HR@10      NDCG@10   
0.562914   0.278095   0.855960   0.375757  
Loss: 0.0001
/home/nit/Desktop/2PDreamRec/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:243: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
Fold 3 Epoch 071; Train loss: 0.0000; Time: 00:00:02
Fold 3 Epoch 072; Train loss: 0.0000; Time: 00:00:02
Fold 3 Epoch 073; Train loss: 0.0000; Time: 00:00:02
Fold 3 Epoch 074; Train loss: 0.0000; Time: 00:00:02
Fold 3 Epoch 075; Train loss: 0.0000; Time: 00:00:02
Fold 3 Epoch 076; Train loss: 0.0000; Time: 00:00:02
Fold 3 Epoch 077; Train loss: 0.0000; Time: 00:00:02
Fold 3 Epoch 078; Train loss: 0.0000; Time: 00:00:02
Fold 3 Epoch 079; Train loss: 0.0000; Time: 00:00:01
Fold 3 Epoch 080; Train loss: 0.0000; Time: 00:00:01
Fold 3: Evaluation at Epoch 80
HR@5       NDCG@5     HR@10      NDCG@10   
0.564570   0.288512   0.895695   0.397071  
Loss: 0.0001
/home/nit/Desktop/2PDreamRec/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:243: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
[lr candidate 0.01] Fold 3: NDCG@10 = [np.float64(0.4148713835202386), np.float64(0.47657469744162134), np.float64(0.43790882562035366), np.float64(0.41769752489205675), np.float64(0.4204013003424755), np.float64(0.38558467308441136), np.float64(0.3757572400331145), np.float64(0.3970706926732015)]
Tuning lr:  60%|██████    | 3/5 [09:20<06:09, 184.83s/it]
========== Fold 3 ==========
INFO:root:Using statics: seq_size = 10, genre_vocab_size = 18
Fold 3 Epoch 001; Train loss: 0.6824; Time: 00:00:01
Fold 3 Epoch 002; Train loss: 0.2941; Time: 00:00:01
Fold 3 Epoch 003; Train loss: 0.2105; Time: 00:00:01
Fold 3 Epoch 004; Train loss: 0.1825; Time: 00:00:01
Fold 3 Epoch 005; Train loss: 0.1652; Time: 00:00:01
Fold 3 Epoch 006; Train loss: 0.1522; Time: 00:00:01
Fold 3 Epoch 007; Train loss: 0.1518; Time: 00:00:01
Fold 3 Epoch 008; Train loss: 0.1484; Time: 00:00:02
Fold 3 Epoch 009; Train loss: 0.1416; Time: 00:00:01
Fold 3 Epoch 010; Train loss: 0.1348; Time: 00:00:01
Fold 3: Evaluation at Epoch 10
HR@5       NDCG@5     HR@10      NDCG@10   
0.655629   0.528118   0.759934   0.561447  
Loss: 0.1796
/home/nit/Desktop/2PDreamRec/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:243: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
Fold 3 Epoch 011; Train loss: 0.1339; Time: 00:00:01
Fold 3 Epoch 012; Train loss: 0.1264; Time: 00:00:01
Fold 3 Epoch 013; Train loss: 0.1288; Time: 00:00:01
Fold 3 Epoch 014; Train loss: 0.1172; Time: 00:00:02
Fold 3 Epoch 015; Train loss: 0.1209; Time: 00:00:01
Fold 3 Epoch 016; Train loss: 0.1126; Time: 00:00:01
Fold 3 Epoch 017; Train loss: 0.1115; Time: 00:00:01
Fold 3 Epoch 018; Train loss: 0.1060; Time: 00:00:01
Fold 3 Epoch 019; Train loss: 0.1060; Time: 00:00:01
Fold 3 Epoch 020; Train loss: 0.1025; Time: 00:00:02
Fold 3: Evaluation at Epoch 20
HR@5       NDCG@5     HR@10      NDCG@10   
0.653974   0.538016   0.766556   0.573669  
Loss: 0.1293
/home/nit/Desktop/2PDreamRec/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:243: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
Fold 3 Epoch 021; Train loss: 0.0965; Time: 00:00:01
Fold 3 Epoch 022; Train loss: 0.0951; Time: 00:00:01
Fold 3 Epoch 023; Train loss: 0.0901; Time: 00:00:02
Fold 3 Epoch 024; Train loss: 0.0899; Time: 00:00:02
Fold 3 Epoch 025; Train loss: 0.0850; Time: 00:00:02
Fold 3 Epoch 026; Train loss: 0.0836; Time: 00:00:02
Fold 3 Epoch 027; Train loss: 0.0802; Time: 00:00:02
Fold 3 Epoch 028; Train loss: 0.0779; Time: 00:00:02
Fold 3 Epoch 029; Train loss: 0.0762; Time: 00:00:02
Fold 3 Epoch 030; Train loss: 0.0730; Time: 00:00:02
Fold 3: Evaluation at Epoch 30
HR@5       NDCG@5     HR@10      NDCG@10   
0.629139   0.519905   0.701987   0.543425  
Loss: 0.0916
/home/nit/Desktop/2PDreamRec/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:243: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
Fold 3 Epoch 031; Train loss: 0.0699; Time: 00:00:02
Fold 3 Epoch 032; Train loss: 0.0662; Time: 00:00:02
Fold 3 Epoch 033; Train loss: 0.0659; Time: 00:00:02
Fold 3 Epoch 034; Train loss: 0.0612; Time: 00:00:02
Fold 3 Epoch 035; Train loss: 0.0601; Time: 00:00:02
Fold 3 Epoch 036; Train loss: 0.0574; Time: 00:00:02
Fold 3 Epoch 037; Train loss: 0.0578; Time: 00:00:02
Fold 3 Epoch 038; Train loss: 0.0536; Time: 00:00:01
Fold 3 Epoch 039; Train loss: 0.0522; Time: 00:00:02
Fold 3 Epoch 040; Train loss: 0.0490; Time: 00:00:02
Fold 3: Evaluation at Epoch 40
HR@5       NDCG@5     HR@10      NDCG@10   
0.649007   0.526397   0.741722   0.555836  
Loss: 0.0583
/home/nit/Desktop/2PDreamRec/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:243: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
Fold 3 Epoch 041; Train loss: 0.0480; Time: 00:00:02
Fold 3 Epoch 042; Train loss: 0.0460; Time: 00:00:02
Fold 3 Epoch 043; Train loss: 0.0431; Time: 00:00:01
Fold 3 Epoch 044; Train loss: 0.0418; Time: 00:00:01
Fold 3 Epoch 045; Train loss: 0.0394; Time: 00:00:02
Fold 3 Epoch 046; Train loss: 0.0369; Time: 00:00:02
Fold 3 Epoch 047; Train loss: 0.0365; Time: 00:00:01
Fold 3 Epoch 048; Train loss: 0.0343; Time: 00:00:01
Fold 3 Epoch 049; Train loss: 0.0332; Time: 00:00:01
Fold 3 Epoch 050; Train loss: 0.0319; Time: 00:00:01
Fold 3: Evaluation at Epoch 50
HR@5       NDCG@5     HR@10      NDCG@10   
0.607616   0.499605   0.695364   0.527928  
Loss: 0.0306
/home/nit/Desktop/2PDreamRec/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:243: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
Fold 3 Epoch 051; Train loss: 0.0307; Time: 00:00:01
Fold 3 Epoch 052; Train loss: 0.0280; Time: 00:00:01
Fold 3 Epoch 053; Train loss: 0.0274; Time: 00:00:02
Fold 3 Epoch 054; Train loss: 0.0272; Time: 00:00:01
Fold 3 Epoch 055; Train loss: 0.0247; Time: 00:00:01
Fold 3 Epoch 056; Train loss: 0.0237; Time: 00:00:01
Fold 3 Epoch 057; Train loss: 0.0236; Time: 00:00:01
Fold 3 Epoch 058; Train loss: 0.0209; Time: 00:00:01
Fold 3 Epoch 059; Train loss: 0.0208; Time: 00:00:01
Fold 3 Epoch 060; Train loss: 0.0196; Time: 00:00:01
Fold 3: Evaluation at Epoch 60
HR@5       NDCG@5     HR@10      NDCG@10   
0.642384   0.499765   0.806291   0.553960  
Loss: 0.0205
/home/nit/Desktop/2PDreamRec/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:243: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
Fold 3 Epoch 061; Train loss: 0.0186; Time: 00:00:01
Fold 3 Epoch 062; Train loss: 0.0175; Time: 00:00:01
Fold 3 Epoch 063; Train loss: 0.0169; Time: 00:00:01
Fold 3 Epoch 064; Train loss: 0.0160; Time: 00:00:01
Fold 3 Epoch 065; Train loss: 0.0151; Time: 00:00:02
Fold 3 Epoch 066; Train loss: 0.0142; Time: 00:00:02
Fold 3 Epoch 067; Train loss: 0.0138; Time: 00:00:01
Fold 3 Epoch 068; Train loss: 0.0125; Time: 00:00:01
Fold 3 Epoch 069; Train loss: 0.0117; Time: 00:00:01
Fold 3 Epoch 070; Train loss: 0.0114; Time: 00:00:01
Fold 3: Evaluation at Epoch 70
HR@5       NDCG@5     HR@10      NDCG@10   
0.516556   0.366135   0.793046   0.455537  
Loss: 0.0099
/home/nit/Desktop/2PDreamRec/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:243: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
Fold 3 Epoch 071; Train loss: 0.0105; Time: 00:00:01
Fold 3 Epoch 072; Train loss: 0.0102; Time: 00:00:01
Fold 3 Epoch 073; Train loss: 0.0096; Time: 00:00:02
Fold 3 Epoch 074; Train loss: 0.0090; Time: 00:00:02
Fold 3 Epoch 075; Train loss: 0.0084; Time: 00:00:01
Fold 3 Epoch 076; Train loss: 0.0082; Time: 00:00:02
Fold 3 Epoch 077; Train loss: 0.0074; Time: 00:00:01
Fold 3 Epoch 078; Train loss: 0.0070; Time: 00:00:01
Fold 3 Epoch 079; Train loss: 0.0067; Time: 00:00:01
Fold 3 Epoch 080; Train loss: 0.0062; Time: 00:00:01
Fold 3: Evaluation at Epoch 80
HR@5       NDCG@5     HR@10      NDCG@10   
0.604305   0.416485   0.836093   0.493658  
Loss: 0.0049
/home/nit/Desktop/2PDreamRec/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:243: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
[lr candidate 0.005] Fold 3: NDCG@10 = [np.float64(0.5614465610638807), np.float64(0.5736685001185579), np.float64(0.5434248215302682), np.float64(0.5558364321767869), np.float64(0.5279280037761), np.float64(0.5539599304130803), np.float64(0.4555366052239997), np.float64(0.49365819790606674)]
Tuning lr:  80%|████████  | 4/5 [12:16<03:01, 181.18s/it]
========== Fold 3 ==========
INFO:root:Using statics: seq_size = 10, genre_vocab_size = 18
Fold 3 Epoch 001; Train loss: 0.8383; Time: 00:00:01
Fold 3 Epoch 002; Train loss: 0.5711; Time: 00:00:01
Fold 3 Epoch 003; Train loss: 0.4363; Time: 00:00:01
Fold 3 Epoch 004; Train loss: 0.3505; Time: 00:00:01
Fold 3 Epoch 005; Train loss: 0.2966; Time: 00:00:02
Fold 3 Epoch 006; Train loss: 0.2619; Time: 00:00:01
Fold 3 Epoch 007; Train loss: 0.2332; Time: 00:00:01
Fold 3 Epoch 008; Train loss: 0.2155; Time: 00:00:01
Fold 3 Epoch 009; Train loss: 0.1992; Time: 00:00:01
Fold 3 Epoch 010; Train loss: 0.1886; Time: 00:00:01
Fold 3: Evaluation at Epoch 10
HR@5       NDCG@5     HR@10      NDCG@10   
0.592715   0.496546   0.750000   0.547975  
Loss: 0.2558
/home/nit/Desktop/2PDreamRec/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:243: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
Fold 3 Epoch 011; Train loss: 0.1774; Time: 00:00:01
Fold 3 Epoch 012; Train loss: 0.1703; Time: 00:00:01
Fold 3 Epoch 013; Train loss: 0.1671; Time: 00:00:01
Fold 3 Epoch 014; Train loss: 0.1600; Time: 00:00:01
Fold 3 Epoch 015; Train loss: 0.1562; Time: 00:00:02
Fold 3 Epoch 016; Train loss: 0.1525; Time: 00:00:02
Fold 3 Epoch 017; Train loss: 0.1508; Time: 00:00:01
Fold 3 Epoch 018; Train loss: 0.1481; Time: 00:00:01
Fold 3 Epoch 019; Train loss: 0.1456; Time: 00:00:01
Fold 3 Epoch 020; Train loss: 0.1418; Time: 00:00:01
Fold 3: Evaluation at Epoch 20
HR@5       NDCG@5     HR@10      NDCG@10   
0.584437   0.474103   0.748344   0.527169  
Loss: 0.1832
/home/nit/Desktop/2PDreamRec/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:243: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
Fold 3 Epoch 021; Train loss: 0.1404; Time: 00:00:01
Fold 3 Epoch 022; Train loss: 0.1387; Time: 00:00:01
Fold 3 Epoch 023; Train loss: 0.1387; Time: 00:00:01
Fold 3 Epoch 024; Train loss: 0.1346; Time: 00:00:02
Fold 3 Epoch 025; Train loss: 0.1300; Time: 00:00:01
Fold 3 Epoch 026; Train loss: 0.1325; Time: 00:00:01
Fold 3 Epoch 027; Train loss: 0.1336; Time: 00:00:02
Fold 3 Epoch 028; Train loss: 0.1269; Time: 00:00:02
Fold 3 Epoch 029; Train loss: 0.1292; Time: 00:00:02
Fold 3 Epoch 030; Train loss: 0.1268; Time: 00:00:02
Fold 3: Evaluation at Epoch 30
HR@5       NDCG@5     HR@10      NDCG@10   
0.614238   0.506733   0.741722   0.548378  
Loss: 0.1641
/home/nit/Desktop/2PDreamRec/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:243: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
Fold 3 Epoch 031; Train loss: 0.1262; Time: 00:00:02
Fold 3 Epoch 032; Train loss: 0.1271; Time: 00:00:02
Fold 3 Epoch 033; Train loss: 0.1222; Time: 00:00:02
Fold 3 Epoch 034; Train loss: 0.1232; Time: 00:00:02
Fold 3 Epoch 035; Train loss: 0.1258; Time: 00:00:02
Fold 3 Epoch 036; Train loss: 0.1212; Time: 00:00:02
Fold 3 Epoch 037; Train loss: 0.1199; Time: 00:00:02
Fold 3 Epoch 038; Train loss: 0.1197; Time: 00:00:02
Fold 3 Epoch 039; Train loss: 0.1213; Time: 00:00:01
Fold 3 Epoch 040; Train loss: 0.1185; Time: 00:00:01
Fold 3: Evaluation at Epoch 40
HR@5       NDCG@5     HR@10      NDCG@10   
0.579470   0.484579   0.728477   0.533064  
Loss: 0.1439
/home/nit/Desktop/2PDreamRec/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:243: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
Fold 3 Epoch 041; Train loss: 0.1195; Time: 00:00:02
Fold 3 Epoch 042; Train loss: 0.1166; Time: 00:00:02
Fold 3 Epoch 043; Train loss: 0.1136; Time: 00:00:02
Fold 3 Epoch 044; Train loss: 0.1151; Time: 00:00:02
Fold 3 Epoch 045; Train loss: 0.1153; Time: 00:00:02
Fold 3 Epoch 046; Train loss: 0.1154; Time: 00:00:01
Fold 3 Epoch 047; Train loss: 0.1116; Time: 00:00:01
Fold 3 Epoch 048; Train loss: 0.1137; Time: 00:00:01
Fold 3 Epoch 049; Train loss: 0.1115; Time: 00:00:01
Fold 3 Epoch 050; Train loss: 0.1107; Time: 00:00:01
Fold 3: Evaluation at Epoch 50
HR@5       NDCG@5     HR@10      NDCG@10   
0.653974   0.543327   0.768212   0.580118  
Loss: 0.1380
/home/nit/Desktop/2PDreamRec/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:243: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
Fold 3 Epoch 051; Train loss: 0.1092; Time: 00:00:02
Fold 3 Epoch 052; Train loss: 0.1081; Time: 00:00:02
Fold 3 Epoch 053; Train loss: 0.1063; Time: 00:00:02
Fold 3 Epoch 054; Train loss: 0.1092; Time: 00:00:02
Fold 3 Epoch 055; Train loss: 0.1078; Time: 00:00:02
Fold 3 Epoch 056; Train loss: 0.1071; Time: 00:00:02
Fold 3 Epoch 057; Train loss: 0.1038; Time: 00:00:02
Fold 3 Epoch 058; Train loss: 0.1048; Time: 00:00:02
Fold 3 Epoch 059; Train loss: 0.1036; Time: 00:00:02
Fold 3 Epoch 060; Train loss: 0.1040; Time: 00:00:02
Fold 3: Evaluation at Epoch 60
HR@5       NDCG@5     HR@10      NDCG@10   
0.596026   0.479721   0.759934   0.533431  
Loss: 0.1388
/home/nit/Desktop/2PDreamRec/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:243: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
Fold 3 Epoch 061; Train loss: 0.1032; Time: 00:00:02
Fold 3 Epoch 062; Train loss: 0.1033; Time: 00:00:02
Fold 3 Epoch 063; Train loss: 0.1024; Time: 00:00:01
Fold 3 Epoch 064; Train loss: 0.1006; Time: 00:00:01
Fold 3 Epoch 065; Train loss: 0.0999; Time: 00:00:01
Fold 3 Epoch 066; Train loss: 0.0997; Time: 00:00:01
Fold 3 Epoch 067; Train loss: 0.0987; Time: 00:00:02
Fold 3 Epoch 068; Train loss: 0.0969; Time: 00:00:02
Fold 3 Epoch 069; Train loss: 0.0941; Time: 00:00:01
Fold 3 Epoch 070; Train loss: 0.0952; Time: 00:00:01
Fold 3: Evaluation at Epoch 70
HR@5       NDCG@5     HR@10      NDCG@10   
0.592715   0.472815   0.753311   0.523950  
Loss: 0.1275
/home/nit/Desktop/2PDreamRec/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:243: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
Fold 3 Epoch 071; Train loss: 0.0938; Time: 00:00:01
Fold 3 Epoch 072; Train loss: 0.0949; Time: 00:00:01
Fold 3 Epoch 073; Train loss: 0.0949; Time: 00:00:01
Fold 3 Epoch 074; Train loss: 0.0934; Time: 00:00:01
Fold 3 Epoch 075; Train loss: 0.0923; Time: 00:00:01
Fold 3 Epoch 076; Train loss: 0.0911; Time: 00:00:01
Fold 3 Epoch 077; Train loss: 0.0911; Time: 00:00:01
Fold 3 Epoch 078; Train loss: 0.0891; Time: 00:00:02
Fold 3 Epoch 079; Train loss: 0.0885; Time: 00:00:02
Fold 3 Epoch 080; Train loss: 0.0886; Time: 00:00:02
Fold 3: Evaluation at Epoch 80
HR@5       NDCG@5     HR@10      NDCG@10   
0.592715   0.492777   0.750000   0.543664  
Loss: 0.1208
/home/nit/Desktop/2PDreamRec/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:243: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
[lr candidate 0.001] Fold 3: NDCG@10 = [np.float64(0.5479749608391901), np.float64(0.5271687546908658), np.float64(0.5483775911748606), np.float64(0.533064290615517), np.float64(0.5801175508082322), np.float64(0.5334309132884046), np.float64(0.5239499014196933), np.float64(0.5436642938223797)]
Tuning lr: 100%|██████████| 5/5 [15:18<00:00, 181.40s/it]Tuning lr: 100%|██████████| 5/5 [15:18<00:00, 183.64s/it]

Best learning rate found: 0.001
Tuning optimizer:   0%|          | 0/4 [00:00<?, ?it/s]
========== Fold 3 ==========
INFO:root:Using statics: seq_size = 10, genre_vocab_size = 18
Fold 3 Epoch 001; Train loss: 0.8499; Time: 00:00:02
Fold 3 Epoch 002; Train loss: 0.5943; Time: 00:00:02
Fold 3 Epoch 003; Train loss: 0.4573; Time: 00:00:02
Fold 3 Epoch 004; Train loss: 0.3762; Time: 00:00:02
Fold 3 Epoch 005; Train loss: 0.3146; Time: 00:00:02
Fold 3 Epoch 006; Train loss: 0.2717; Time: 00:00:02
Fold 3 Epoch 007; Train loss: 0.2475; Time: 00:00:01
Fold 3 Epoch 008; Train loss: 0.2214; Time: 00:00:01
Fold 3 Epoch 009; Train loss: 0.2041; Time: 00:00:02
Fold 3 Epoch 010; Train loss: 0.1949; Time: 00:00:01
Fold 3: Evaluation at Epoch 10
HR@5       NDCG@5     HR@10      NDCG@10   
0.650662   0.521126   0.802980   0.570330  
Loss: 0.2826
/home/nit/Desktop/2PDreamRec/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:243: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
Fold 3 Epoch 011; Train loss: 0.1879; Time: 00:00:02
Fold 3 Epoch 012; Train loss: 0.1771; Time: 00:00:02
Fold 3 Epoch 013; Train loss: 0.1713; Time: 00:00:02
Fold 3 Epoch 014; Train loss: 0.1664; Time: 00:00:01
Fold 3 Epoch 015; Train loss: 0.1601; Time: 00:00:02
Fold 3 Epoch 016; Train loss: 0.1588; Time: 00:00:01
Fold 3 Epoch 017; Train loss: 0.1577; Time: 00:00:02
Fold 3 Epoch 018; Train loss: 0.1510; Time: 00:00:01
Fold 3 Epoch 019; Train loss: 0.1482; Time: 00:00:02
Fold 3 Epoch 020; Train loss: 0.1481; Time: 00:00:02
Fold 3: Evaluation at Epoch 20
HR@5       NDCG@5     HR@10      NDCG@10   
0.682119   0.555120   0.812914   0.597670  
Loss: 0.1993
/home/nit/Desktop/2PDreamRec/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:243: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
Fold 3 Epoch 021; Train loss: 0.1481; Time: 00:00:01
Fold 3 Epoch 022; Train loss: 0.1436; Time: 00:00:01
Fold 3 Epoch 023; Train loss: 0.1429; Time: 00:00:02
Fold 3 Epoch 024; Train loss: 0.1396; Time: 00:00:02
Fold 3 Epoch 025; Train loss: 0.1382; Time: 00:00:01
Fold 3 Epoch 026; Train loss: 0.1356; Time: 00:00:02
Fold 3 Epoch 027; Train loss: 0.1353; Time: 00:00:01
Fold 3 Epoch 028; Train loss: 0.1333; Time: 00:00:02
Fold 3 Epoch 029; Train loss: 0.1342; Time: 00:00:02
Fold 3 Epoch 030; Train loss: 0.1301; Time: 00:00:01
Fold 3: Evaluation at Epoch 30
HR@5       NDCG@5     HR@10      NDCG@10   
0.630795   0.505619   0.745033   0.542995  
Loss: 0.1626
/home/nit/Desktop/2PDreamRec/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:243: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
Fold 3 Epoch 031; Train loss: 0.1262; Time: 00:00:02
Fold 3 Epoch 032; Train loss: 0.1289; Time: 00:00:02
Fold 3 Epoch 033; Train loss: 0.1266; Time: 00:00:01
Fold 3 Epoch 034; Train loss: 0.1273; Time: 00:00:01
Fold 3 Epoch 035; Train loss: 0.1266; Time: 00:00:01
Fold 3 Epoch 036; Train loss: 0.1256; Time: 00:00:02
Fold 3 Epoch 037; Train loss: 0.1241; Time: 00:00:01
Fold 3 Epoch 038; Train loss: 0.1242; Time: 00:00:01
Fold 3 Epoch 039; Train loss: 0.1247; Time: 00:00:01
Fold 3 Epoch 040; Train loss: 0.1214; Time: 00:00:01
Fold 3: Evaluation at Epoch 40
HR@5       NDCG@5     HR@10      NDCG@10   
0.620861   0.499822   0.759934   0.544808  
Loss: 0.1535
/home/nit/Desktop/2PDreamRec/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:243: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
Fold 3 Epoch 041; Train loss: 0.1217; Time: 00:00:02
Fold 3 Epoch 042; Train loss: 0.1201; Time: 00:00:02
Fold 3 Epoch 043; Train loss: 0.1197; Time: 00:00:01
Fold 3 Epoch 044; Train loss: 0.1209; Time: 00:00:02
Fold 3 Epoch 045; Train loss: 0.1159; Time: 00:00:01
Fold 3 Epoch 046; Train loss: 0.1142; Time: 00:00:01
Fold 3 Epoch 047; Train loss: 0.1142; Time: 00:00:02
Fold 3 Epoch 048; Train loss: 0.1130; Time: 00:00:02
Fold 3 Epoch 049; Train loss: 0.1127; Time: 00:00:02
Fold 3 Epoch 050; Train loss: 0.1117; Time: 00:00:01
Fold 3: Evaluation at Epoch 50
HR@5       NDCG@5     HR@10      NDCG@10   
0.597682   0.486913   0.748344   0.535176  
Loss: 0.1461
/home/nit/Desktop/2PDreamRec/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:243: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
Fold 3 Epoch 051; Train loss: 0.1151; Time: 00:00:01
Fold 3 Epoch 052; Train loss: 0.1118; Time: 00:00:01
Fold 3 Epoch 053; Train loss: 0.1111; Time: 00:00:02
Fold 3 Epoch 054; Train loss: 0.1071; Time: 00:00:01
Fold 3 Epoch 055; Train loss: 0.1103; Time: 00:00:02
Fold 3 Epoch 056; Train loss: 0.1088; Time: 00:00:01
Fold 3 Epoch 057; Train loss: 0.1081; Time: 00:00:02
Fold 3 Epoch 058; Train loss: 0.1076; Time: 00:00:01
Fold 3 Epoch 059; Train loss: 0.1046; Time: 00:00:01
Fold 3 Epoch 060; Train loss: 0.1063; Time: 00:00:01
Fold 3: Evaluation at Epoch 60
HR@5       NDCG@5     HR@10      NDCG@10   
0.680464   0.542321   0.809603   0.583832  
Loss: 0.1321
/home/nit/Desktop/2PDreamRec/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:243: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
Fold 3 Epoch 061; Train loss: 0.1045; Time: 00:00:01
Fold 3 Epoch 062; Train loss: 0.1054; Time: 00:00:02
Fold 3 Epoch 063; Train loss: 0.1030; Time: 00:00:01
Fold 3 Epoch 064; Train loss: 0.1047; Time: 00:00:01
Fold 3 Epoch 065; Train loss: 0.1044; Time: 00:00:02
Fold 3 Epoch 066; Train loss: 0.1021; Time: 00:00:01
Fold 3 Epoch 067; Train loss: 0.0994; Time: 00:00:02
Fold 3 Epoch 068; Train loss: 0.1005; Time: 00:00:01
Fold 3 Epoch 069; Train loss: 0.0972; Time: 00:00:01
Fold 3 Epoch 070; Train loss: 0.0977; Time: 00:00:02
Fold 3: Evaluation at Epoch 70
HR@5       NDCG@5     HR@10      NDCG@10   
0.584437   0.474109   0.756623   0.530129  
Loss: 0.1221
/home/nit/Desktop/2PDreamRec/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:243: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
Fold 3 Epoch 071; Train loss: 0.0961; Time: 00:00:02
Fold 3 Epoch 072; Train loss: 0.0977; Time: 00:00:01
Fold 3 Epoch 073; Train loss: 0.0962; Time: 00:00:01
Fold 3 Epoch 074; Train loss: 0.0959; Time: 00:00:01
Fold 3 Epoch 075; Train loss: 0.0944; Time: 00:00:01
Fold 3 Epoch 076; Train loss: 0.0963; Time: 00:00:01
Fold 3 Epoch 077; Train loss: 0.0929; Time: 00:00:01
Fold 3 Epoch 078; Train loss: 0.0952; Time: 00:00:01
Fold 3 Epoch 079; Train loss: 0.0925; Time: 00:00:01
Fold 3 Epoch 080; Train loss: 0.0928; Time: 00:00:01
Fold 3: Evaluation at Epoch 80
HR@5       NDCG@5     HR@10      NDCG@10   
0.678808   0.549851   0.809603   0.592365  
Loss: 0.1211
/home/nit/Desktop/2PDreamRec/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:243: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
[optimizer candidate nadam] Fold 3: NDCG@10 = [np.float64(0.5703302319223814), np.float64(0.5976702093581148), np.float64(0.5429952552066015), np.float64(0.5448080480794611), np.float64(0.5351762940725353), np.float64(0.5838317589684681), np.float64(0.5301292675424792), np.float64(0.5923648536268971)]
Tuning optimizer:  25%|██▌       | 1/4 [02:45<08:17, 165.87s/it]
========== Fold 3 ==========
INFO:root:Using statics: seq_size = 10, genre_vocab_size = 18
Fold 3 Epoch 001; Train loss: 1.3635; Time: 00:00:01
Fold 3 Epoch 002; Train loss: 1.2852; Time: 00:00:01
Fold 3 Epoch 003; Train loss: 1.2153; Time: 00:00:02
Fold 3 Epoch 004; Train loss: 1.1523; Time: 00:00:02
Fold 3 Epoch 005; Train loss: 1.0923; Time: 00:00:02
Fold 3 Epoch 006; Train loss: 1.0358; Time: 00:00:02
Fold 3 Epoch 007; Train loss: 0.9862; Time: 00:00:01
Fold 3 Epoch 008; Train loss: 0.9398; Time: 00:00:01
Fold 3 Epoch 009; Train loss: 0.8966; Time: 00:00:02
Fold 3 Epoch 010; Train loss: 0.8592; Time: 00:00:01
Fold 3: Evaluation at Epoch 10
HR@5       NDCG@5     HR@10      NDCG@10   
0.461921   0.318021   0.846026   0.439245  
Loss: 0.9007
/home/nit/Desktop/2PDreamRec/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:243: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
Fold 3 Epoch 011; Train loss: 0.8241; Time: 00:00:02
Fold 3 Epoch 012; Train loss: 0.7930; Time: 00:00:01
Fold 3 Epoch 013; Train loss: 0.7634; Time: 00:00:01
Fold 3 Epoch 014; Train loss: 0.7384; Time: 00:00:02
Fold 3 Epoch 015; Train loss: 0.7130; Time: 00:00:01
Fold 3 Epoch 016; Train loss: 0.6917; Time: 00:00:01
Fold 3 Epoch 017; Train loss: 0.6688; Time: 00:00:02
Fold 3 Epoch 018; Train loss: 0.6489; Time: 00:00:02
Fold 3 Epoch 019; Train loss: 0.6284; Time: 00:00:02
Fold 3 Epoch 020; Train loss: 0.6075; Time: 00:00:01
Fold 3: Evaluation at Epoch 20
HR@5       NDCG@5     HR@10      NDCG@10   
0.610927   0.424492   0.827815   0.495304  
Loss: 0.7263
/home/nit/Desktop/2PDreamRec/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:243: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
Fold 3 Epoch 021; Train loss: 0.5888; Time: 00:00:01
Fold 3 Epoch 022; Train loss: 0.5725; Time: 00:00:01
Fold 3 Epoch 023; Train loss: 0.5528; Time: 00:00:01
Fold 3 Epoch 024; Train loss: 0.5371; Time: 00:00:01
Fold 3 Epoch 025; Train loss: 0.5207; Time: 00:00:01
Fold 3 Epoch 026; Train loss: 0.5040; Time: 00:00:02
Fold 3 Epoch 027; Train loss: 0.4891; Time: 00:00:02
Fold 3 Epoch 028; Train loss: 0.4710; Time: 00:00:02
Fold 3 Epoch 029; Train loss: 0.4560; Time: 00:00:01
Fold 3 Epoch 030; Train loss: 0.4453; Time: 00:00:01
Fold 3: Evaluation at Epoch 30
HR@5       NDCG@5     HR@10      NDCG@10   
0.637417   0.463651   0.821192   0.522606  
Loss: 0.5706
/home/nit/Desktop/2PDreamRec/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:243: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
Fold 3 Epoch 031; Train loss: 0.4289; Time: 00:00:02
Fold 3 Epoch 032; Train loss: 0.4179; Time: 00:00:01
Fold 3 Epoch 033; Train loss: 0.4080; Time: 00:00:01
Fold 3 Epoch 034; Train loss: 0.3964; Time: 00:00:01
Fold 3 Epoch 035; Train loss: 0.3845; Time: 00:00:01
Fold 3 Epoch 036; Train loss: 0.3735; Time: 00:00:01
Fold 3 Epoch 037; Train loss: 0.3643; Time: 00:00:02
Fold 3 Epoch 038; Train loss: 0.3531; Time: 00:00:02
Fold 3 Epoch 039; Train loss: 0.3411; Time: 00:00:02
Fold 3 Epoch 040; Train loss: 0.3299; Time: 00:00:02
Fold 3: Evaluation at Epoch 40
HR@5       NDCG@5     HR@10      NDCG@10   
0.662252   0.483967   0.802980   0.529257  
Loss: 0.4553
/home/nit/Desktop/2PDreamRec/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:243: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
Fold 3 Epoch 041; Train loss: 0.3218; Time: 00:00:01
Fold 3 Epoch 042; Train loss: 0.3126; Time: 00:00:02
Fold 3 Epoch 043; Train loss: 0.3058; Time: 00:00:02
Fold 3 Epoch 044; Train loss: 0.2970; Time: 00:00:02
Fold 3 Epoch 045; Train loss: 0.2911; Time: 00:00:02
Fold 3 Epoch 046; Train loss: 0.2829; Time: 00:00:02
Fold 3 Epoch 047; Train loss: 0.2720; Time: 00:00:02
Fold 3 Epoch 048; Train loss: 0.2667; Time: 00:00:02
Fold 3 Epoch 049; Train loss: 0.2630; Time: 00:00:02
Fold 3 Epoch 050; Train loss: 0.2527; Time: 00:00:02
Fold 3: Evaluation at Epoch 50
HR@5       NDCG@5     HR@10      NDCG@10   
0.668874   0.501747   0.789735   0.540649  
Loss: 0.3624
/home/nit/Desktop/2PDreamRec/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:243: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
Fold 3 Epoch 051; Train loss: 0.2496; Time: 00:00:01
Fold 3 Epoch 052; Train loss: 0.2440; Time: 00:00:02
Fold 3 Epoch 053; Train loss: 0.2366; Time: 00:00:01
Fold 3 Epoch 054; Train loss: 0.2315; Time: 00:00:02
Fold 3 Epoch 055; Train loss: 0.2268; Time: 00:00:01
Fold 3 Epoch 056; Train loss: 0.2230; Time: 00:00:01
Fold 3 Epoch 057; Train loss: 0.2169; Time: 00:00:02
Fold 3 Epoch 058; Train loss: 0.2090; Time: 00:00:02
Fold 3 Epoch 059; Train loss: 0.2059; Time: 00:00:03
Fold 3 Epoch 060; Train loss: 0.2044; Time: 00:00:02
Fold 3: Evaluation at Epoch 60
HR@5       NDCG@5     HR@10      NDCG@10   
0.637417   0.499460   0.786424   0.546478  
Loss: 0.2913
/home/nit/Desktop/2PDreamRec/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:243: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
Fold 3 Epoch 061; Train loss: 0.2005; Time: 00:00:01
Fold 3 Epoch 062; Train loss: 0.1985; Time: 00:00:02
Fold 3 Epoch 063; Train loss: 0.1939; Time: 00:00:02
Fold 3 Epoch 064; Train loss: 0.1903; Time: 00:00:01
Fold 3 Epoch 065; Train loss: 0.1821; Time: 00:00:02
Fold 3 Epoch 066; Train loss: 0.1845; Time: 00:00:01
Fold 3 Epoch 067; Train loss: 0.1804; Time: 00:00:01
Fold 3 Epoch 068; Train loss: 0.1813; Time: 00:00:01
Fold 3 Epoch 069; Train loss: 0.1745; Time: 00:00:02
Fold 3 Epoch 070; Train loss: 0.1722; Time: 00:00:01
Fold 3: Evaluation at Epoch 70
HR@5       NDCG@5     HR@10      NDCG@10   
0.617550   0.482914   0.764901   0.529810  
Loss: 0.2406
/home/nit/Desktop/2PDreamRec/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:243: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
Fold 3 Epoch 071; Train loss: 0.1678; Time: 00:00:02
Fold 3 Epoch 072; Train loss: 0.1675; Time: 00:00:02
Fold 3 Epoch 073; Train loss: 0.1657; Time: 00:00:01
Fold 3 Epoch 074; Train loss: 0.1629; Time: 00:00:01
Fold 3 Epoch 075; Train loss: 0.1637; Time: 00:00:02
Fold 3 Epoch 076; Train loss: 0.1605; Time: 00:00:02
Fold 3 Epoch 077; Train loss: 0.1585; Time: 00:00:02
Fold 3 Epoch 078; Train loss: 0.1568; Time: 00:00:02
Fold 3 Epoch 079; Train loss: 0.1568; Time: 00:00:02
Fold 3 Epoch 080; Train loss: 0.1568; Time: 00:00:02
Fold 3: Evaluation at Epoch 80
HR@5       NDCG@5     HR@10      NDCG@10   
0.572848   0.443641   0.720199   0.490170  
Loss: 0.2095
/home/nit/Desktop/2PDreamRec/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:243: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
[optimizer candidate lamb] Fold 3: NDCG@10 = [np.float64(0.4392445307460016), np.float64(0.4953038858386906), np.float64(0.5226064619613515), np.float64(0.5292574127177762), np.float64(0.5406494144336216), np.float64(0.5464783757813216), np.float64(0.529809776355942), np.float64(0.490170455729863)]
Tuning optimizer:  50%|█████     | 2/4 [05:35<05:36, 168.37s/it]
========== Fold 3 ==========
INFO:root:Using statics: seq_size = 10, genre_vocab_size = 18
Fold 3 Epoch 001; Train loss: 0.9368; Time: 00:00:02
Fold 3 Epoch 002; Train loss: 0.6001; Time: 00:00:02
Fold 3 Epoch 003; Train loss: 0.4412; Time: 00:00:02
Fold 3 Epoch 004; Train loss: 0.3521; Time: 00:00:02
Fold 3 Epoch 005; Train loss: 0.2959; Time: 00:00:01
Fold 3 Epoch 006; Train loss: 0.2609; Time: 00:00:02
Fold 3 Epoch 007; Train loss: 0.2396; Time: 00:00:02
Fold 3 Epoch 008; Train loss: 0.2186; Time: 00:00:01
Fold 3 Epoch 009; Train loss: 0.2055; Time: 00:00:02
Fold 3 Epoch 010; Train loss: 0.1947; Time: 00:00:02
Fold 3: Evaluation at Epoch 10
HR@5       NDCG@5     HR@10      NDCG@10   
0.600993   0.451028   0.731788   0.493284  
Loss: 0.2600
/home/nit/Desktop/2PDreamRec/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:243: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
Fold 3 Epoch 011; Train loss: 0.1920; Time: 00:00:02
Fold 3 Epoch 012; Train loss: 0.1831; Time: 00:00:02
Fold 3 Epoch 013; Train loss: 0.1745; Time: 00:00:01
Fold 3 Epoch 014; Train loss: 0.1737; Time: 00:00:01
Fold 3 Epoch 015; Train loss: 0.1677; Time: 00:00:01
Fold 3 Epoch 016; Train loss: 0.1652; Time: 00:00:01
Fold 3 Epoch 017; Train loss: 0.1599; Time: 00:00:02
Fold 3 Epoch 018; Train loss: 0.1590; Time: 00:00:01
Fold 3 Epoch 019; Train loss: 0.1606; Time: 00:00:02
Fold 3 Epoch 020; Train loss: 0.1575; Time: 00:00:01
Fold 3: Evaluation at Epoch 20
HR@5       NDCG@5     HR@10      NDCG@10   
0.614238   0.475495   0.759934   0.522231  
Loss: 0.1911
/home/nit/Desktop/2PDreamRec/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:243: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
Fold 3 Epoch 021; Train loss: 0.1523; Time: 00:00:01
Fold 3 Epoch 022; Train loss: 0.1520; Time: 00:00:02
Fold 3 Epoch 023; Train loss: 0.1496; Time: 00:00:01
Fold 3 Epoch 024; Train loss: 0.1432; Time: 00:00:01
Fold 3 Epoch 025; Train loss: 0.1449; Time: 00:00:02
Fold 3 Epoch 026; Train loss: 0.1443; Time: 00:00:01
Fold 3 Epoch 027; Train loss: 0.1441; Time: 00:00:01
Fold 3 Epoch 028; Train loss: 0.1435; Time: 00:00:01
Fold 3 Epoch 029; Train loss: 0.1389; Time: 00:00:01
Fold 3 Epoch 030; Train loss: 0.1346; Time: 00:00:01
Fold 3: Evaluation at Epoch 30
HR@5       NDCG@5     HR@10      NDCG@10   
0.579470   0.459416   0.711921   0.501352  
Loss: 0.1706
/home/nit/Desktop/2PDreamRec/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:243: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
Fold 3 Epoch 031; Train loss: 0.1375; Time: 00:00:01
Fold 3 Epoch 032; Train loss: 0.1383; Time: 00:00:02
Fold 3 Epoch 033; Train loss: 0.1346; Time: 00:00:01
Fold 3 Epoch 034; Train loss: 0.1354; Time: 00:00:01
Fold 3 Epoch 035; Train loss: 0.1313; Time: 00:00:01
Fold 3 Epoch 036; Train loss: 0.1322; Time: 00:00:01
Fold 3 Epoch 037; Train loss: 0.1310; Time: 00:00:01
Fold 3 Epoch 038; Train loss: 0.1326; Time: 00:00:02
Fold 3 Epoch 039; Train loss: 0.1282; Time: 00:00:01
Fold 3 Epoch 040; Train loss: 0.1302; Time: 00:00:01
Fold 3: Evaluation at Epoch 40
HR@5       NDCG@5     HR@10      NDCG@10   
0.650662   0.510356   0.784768   0.553432  
Loss: 0.1622
/home/nit/Desktop/2PDreamRec/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:243: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
Fold 3 Epoch 041; Train loss: 0.1271; Time: 00:00:01
Fold 3 Epoch 042; Train loss: 0.1252; Time: 00:00:01
Fold 3 Epoch 043; Train loss: 0.1233; Time: 00:00:01
Fold 3 Epoch 044; Train loss: 0.1242; Time: 00:00:01
Fold 3 Epoch 045; Train loss: 0.1213; Time: 00:00:02
Fold 3 Epoch 046; Train loss: 0.1245; Time: 00:00:01
Fold 3 Epoch 047; Train loss: 0.1249; Time: 00:00:02
Fold 3 Epoch 048; Train loss: 0.1195; Time: 00:00:01
Fold 3 Epoch 049; Train loss: 0.1225; Time: 00:00:02
Fold 3 Epoch 050; Train loss: 0.1170; Time: 00:00:02
Fold 3: Evaluation at Epoch 50
HR@5       NDCG@5     HR@10      NDCG@10   
0.599338   0.481107   0.740066   0.525701  
Loss: 0.1499
/home/nit/Desktop/2PDreamRec/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:243: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
Fold 3 Epoch 051; Train loss: 0.1154; Time: 00:00:01
Fold 3 Epoch 052; Train loss: 0.1174; Time: 00:00:02
Fold 3 Epoch 053; Train loss: 0.1170; Time: 00:00:01
Fold 3 Epoch 054; Train loss: 0.1149; Time: 00:00:01
Fold 3 Epoch 055; Train loss: 0.1158; Time: 00:00:01
Fold 3 Epoch 056; Train loss: 0.1153; Time: 00:00:01
Fold 3 Epoch 057; Train loss: 0.1166; Time: 00:00:01
Fold 3 Epoch 058; Train loss: 0.1122; Time: 00:00:02
Fold 3 Epoch 059; Train loss: 0.1151; Time: 00:00:02
Fold 3 Epoch 060; Train loss: 0.1114; Time: 00:00:02
Fold 3: Evaluation at Epoch 60
HR@5       NDCG@5     HR@10      NDCG@10   
0.581126   0.437374   0.698675   0.475151  
Loss: 0.1380
/home/nit/Desktop/2PDreamRec/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:243: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
Fold 3 Epoch 061; Train loss: 0.1099; Time: 00:00:01
Fold 3 Epoch 062; Train loss: 0.1089; Time: 00:00:01
Fold 3 Epoch 063; Train loss: 0.1112; Time: 00:00:01
Fold 3 Epoch 064; Train loss: 0.1074; Time: 00:00:01
Fold 3 Epoch 065; Train loss: 0.1079; Time: 00:00:01
Fold 3 Epoch 066; Train loss: 0.1074; Time: 00:00:02
Fold 3 Epoch 067; Train loss: 0.1072; Time: 00:00:02
Fold 3 Epoch 068; Train loss: 0.1048; Time: 00:00:02
Fold 3 Epoch 069; Train loss: 0.1090; Time: 00:00:01
Fold 3 Epoch 070; Train loss: 0.1058; Time: 00:00:02
Fold 3: Evaluation at Epoch 70
HR@5       NDCG@5     HR@10      NDCG@10   
0.543046   0.440244   0.673841   0.481707  
Loss: 0.1251
/home/nit/Desktop/2PDreamRec/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:243: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
Fold 3 Epoch 071; Train loss: 0.1040; Time: 00:00:01
Fold 3 Epoch 072; Train loss: 0.1041; Time: 00:00:02
Fold 3 Epoch 073; Train loss: 0.1033; Time: 00:00:01
Fold 3 Epoch 074; Train loss: 0.1015; Time: 00:00:01
Fold 3 Epoch 075; Train loss: 0.1024; Time: 00:00:02
Fold 3 Epoch 076; Train loss: 0.1009; Time: 00:00:02
Fold 3 Epoch 077; Train loss: 0.1012; Time: 00:00:01
Fold 3 Epoch 078; Train loss: 0.0990; Time: 00:00:01
Fold 3 Epoch 079; Train loss: 0.0994; Time: 00:00:01
Fold 3 Epoch 080; Train loss: 0.0982; Time: 00:00:01
Fold 3: Evaluation at Epoch 80
HR@5       NDCG@5     HR@10      NDCG@10   
0.592715   0.466385   0.698675   0.499932  
Loss: 0.1222
/home/nit/Desktop/2PDreamRec/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:243: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
[optimizer candidate adamw] Fold 3: NDCG@10 = [np.float64(0.4932842813235386), np.float64(0.5222305801921406), np.float64(0.5013521572385041), np.float64(0.5534319826944643), np.float64(0.5257007829080926), np.float64(0.475150674254195), np.float64(0.48170718853123284), np.float64(0.49993193647401274)]
Tuning optimizer:  75%|███████▌  | 3/4 [08:12<02:42, 162.86s/it]
========== Fold 3 ==========
INFO:root:Using statics: seq_size = 10, genre_vocab_size = 18
[31mPlease check your arguments if you have upgraded adabelief-pytorch from version 0.0.5.
[31mModifications to default arguments:
[31m                           eps  weight_decouple    rectify
-----------------------  -----  -----------------  ---------
adabelief-pytorch=0.0.5  1e-08  False              False
>=0.1.0 (Current 0.2.0)  1e-16  True               True
[34mSGD better than Adam (e.g. CNN for Image Classification)    Adam better than SGD (e.g. Transformer, GAN)
----------------------------------------------------------  ----------------------------------------------
Recommended eps = 1e-8                                      Recommended eps = 1e-16
[34mFor a complete table of recommended hyperparameters, see
[34mhttps://github.com/juntang-zhuang/Adabelief-Optimizer
[32mYou can disable the log message by setting "print_change_log = False", though it is recommended to keep as a reminder.
[0m
Weight decoupling enabled in AdaBelief
Rectification enabled in AdaBelief
Fold 3 Epoch 001; Train loss: 1.3207; Time: 00:00:02
Fold 3 Epoch 002; Train loss: 1.3110; Time: 00:00:01
Fold 3 Epoch 003; Train loss: 1.3002; Time: 00:00:01
Fold 3 Epoch 004; Train loss: 1.2863; Time: 00:00:01
Fold 3 Epoch 005; Train loss: 1.2685; Time: 00:00:02
Fold 3 Epoch 006; Train loss: 1.2494; Time: 00:00:02
Fold 3 Epoch 007; Train loss: 1.2261; Time: 00:00:01
Fold 3 Epoch 008; Train loss: 1.2064; Time: 00:00:02
Fold 3 Epoch 009; Train loss: 1.1834; Time: 00:00:01
Fold 3 Epoch 010; Train loss: 1.1589; Time: 00:00:01
Fold 3: Evaluation at Epoch 10
HR@5       NDCG@5     HR@10      NDCG@10   
0.076159   0.038319   0.422185   0.148117  
Loss: 0.9855
/home/nit/Desktop/2PDreamRec/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:243: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
Fold 3 Epoch 011; Train loss: 1.1377; Time: 00:00:02
Fold 3 Epoch 012; Train loss: 1.1134; Time: 00:00:01
Fold 3 Epoch 013; Train loss: 1.0895; Time: 00:00:02
Fold 3 Epoch 014; Train loss: 1.0655; Time: 00:00:01
Fold 3 Epoch 015; Train loss: 1.0447; Time: 00:00:01
Fold 3 Epoch 016; Train loss: 1.0186; Time: 00:00:02
Fold 3 Epoch 017; Train loss: 0.9958; Time: 00:00:02
Fold 3 Epoch 018; Train loss: 0.9742; Time: 00:00:02
Fold 3 Epoch 019; Train loss: 0.9511; Time: 00:00:01
Fold 3 Epoch 020; Train loss: 0.9301; Time: 00:00:01
Fold 3: Evaluation at Epoch 20
HR@5       NDCG@5     HR@10      NDCG@10   
0.324503   0.153583   0.687086   0.269168  
Loss: 0.8558
/home/nit/Desktop/2PDreamRec/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:243: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
Fold 3 Epoch 021; Train loss: 0.9094; Time: 00:00:01
Fold 3 Epoch 022; Train loss: 0.8957; Time: 00:00:02
Fold 3 Epoch 023; Train loss: 0.8743; Time: 00:00:02
Fold 3 Epoch 024; Train loss: 0.8533; Time: 00:00:02
Fold 3 Epoch 025; Train loss: 0.8375; Time: 00:00:02
Fold 3 Epoch 026; Train loss: 0.8170; Time: 00:00:02
Fold 3 Epoch 027; Train loss: 0.8052; Time: 00:00:01
Fold 3 Epoch 028; Train loss: 0.7846; Time: 00:00:02
Fold 3 Epoch 029; Train loss: 0.7696; Time: 00:00:02
Fold 3 Epoch 030; Train loss: 0.7562; Time: 00:00:02
Fold 3: Evaluation at Epoch 30
HR@5       NDCG@5     HR@10      NDCG@10   
0.402318   0.214056   0.791391   0.341502  
Loss: 0.7510
/home/nit/Desktop/2PDreamRec/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:243: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
Fold 3 Epoch 031; Train loss: 0.7379; Time: 00:00:01
Fold 3 Epoch 032; Train loss: 0.7255; Time: 00:00:01
Fold 3 Epoch 033; Train loss: 0.7135; Time: 00:00:01
Fold 3 Epoch 034; Train loss: 0.6982; Time: 00:00:01
Fold 3 Epoch 035; Train loss: 0.6842; Time: 00:00:01
Fold 3 Epoch 036; Train loss: 0.6738; Time: 00:00:01
Fold 3 Epoch 037; Train loss: 0.6616; Time: 00:00:02
Fold 3 Epoch 038; Train loss: 0.6505; Time: 00:00:01
Fold 3 Epoch 039; Train loss: 0.6390; Time: 00:00:02
Fold 3 Epoch 040; Train loss: 0.6283; Time: 00:00:01
Fold 3: Evaluation at Epoch 40
HR@5       NDCG@5     HR@10      NDCG@10   
0.468543   0.297937   0.846026   0.422313  
Loss: 0.6871
/home/nit/Desktop/2PDreamRec/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:243: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
Fold 3 Epoch 041; Train loss: 0.6171; Time: 00:00:02
Fold 3 Epoch 042; Train loss: 0.6049; Time: 00:00:02
Fold 3 Epoch 043; Train loss: 0.5965; Time: 00:00:01
Fold 3 Epoch 044; Train loss: 0.5885; Time: 00:00:01
Fold 3 Epoch 045; Train loss: 0.5819; Time: 00:00:02
Fold 3 Epoch 046; Train loss: 0.5676; Time: 00:00:01
Fold 3 Epoch 047; Train loss: 0.5638; Time: 00:00:02
Fold 3 Epoch 048; Train loss: 0.5537; Time: 00:00:01
Fold 3 Epoch 049; Train loss: 0.5454; Time: 00:00:01
Fold 3 Epoch 050; Train loss: 0.5358; Time: 00:00:01
Fold 3: Evaluation at Epoch 50
HR@5       NDCG@5     HR@10      NDCG@10   
0.556291   0.351282   0.852649   0.448331  
Loss: 0.6192
/home/nit/Desktop/2PDreamRec/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:243: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
Fold 3 Epoch 051; Train loss: 0.5251; Time: 00:00:01
Fold 3 Epoch 052; Train loss: 0.5210; Time: 00:00:02
Fold 3 Epoch 053; Train loss: 0.5177; Time: 00:00:01
Fold 3 Epoch 054; Train loss: 0.5096; Time: 00:00:02
Fold 3 Epoch 055; Train loss: 0.5043; Time: 00:00:01
Fold 3 Epoch 056; Train loss: 0.4940; Time: 00:00:02
Fold 3 Epoch 057; Train loss: 0.4936; Time: 00:00:01
Fold 3 Epoch 058; Train loss: 0.4885; Time: 00:00:01
Fold 3 Epoch 059; Train loss: 0.4750; Time: 00:00:01
Fold 3 Epoch 060; Train loss: 0.4723; Time: 00:00:01
Fold 3: Evaluation at Epoch 60
HR@5       NDCG@5     HR@10      NDCG@10   
0.581126   0.369630   0.859272   0.461551  
Loss: 0.5889
/home/nit/Desktop/2PDreamRec/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:243: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
Fold 3 Epoch 061; Train loss: 0.4638; Time: 00:00:01
Fold 3 Epoch 062; Train loss: 0.4609; Time: 00:00:01
Fold 3 Epoch 063; Train loss: 0.4559; Time: 00:00:01
Fold 3 Epoch 064; Train loss: 0.4512; Time: 00:00:01
Fold 3 Epoch 065; Train loss: 0.4445; Time: 00:00:01
Fold 3 Epoch 066; Train loss: 0.4413; Time: 00:00:02
Fold 3 Epoch 067; Train loss: 0.4376; Time: 00:00:01
Fold 3 Epoch 068; Train loss: 0.4340; Time: 00:00:01
Fold 3 Epoch 069; Train loss: 0.4279; Time: 00:00:02
Fold 3 Epoch 070; Train loss: 0.4216; Time: 00:00:01
Fold 3: Evaluation at Epoch 70
HR@5       NDCG@5     HR@10      NDCG@10   
0.644040   0.400106   0.850993   0.468366  
Loss: 0.5490
/home/nit/Desktop/2PDreamRec/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:243: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
Fold 3 Epoch 071; Train loss: 0.4185; Time: 00:00:02
Fold 3 Epoch 072; Train loss: 0.4162; Time: 00:00:01
Fold 3 Epoch 073; Train loss: 0.4114; Time: 00:00:01
Fold 3 Epoch 074; Train loss: 0.4064; Time: 00:00:02
Fold 3 Epoch 075; Train loss: 0.4029; Time: 00:00:02
Fold 3 Epoch 076; Train loss: 0.4032; Time: 00:00:01
Fold 3 Epoch 077; Train loss: 0.3949; Time: 00:00:01
Fold 3 Epoch 078; Train loss: 0.3917; Time: 00:00:02
Fold 3 Epoch 079; Train loss: 0.3916; Time: 00:00:01
Fold 3 Epoch 080; Train loss: 0.3869; Time: 00:00:01
Fold 3: Evaluation at Epoch 80
HR@5       NDCG@5     HR@10      NDCG@10   
0.658940   0.425326   0.846026   0.486749  
Loss: 0.5134
/home/nit/Desktop/2PDreamRec/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:243: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
[optimizer candidate adabelief] Fold 3: NDCG@10 = [np.float64(0.14811678418331686), np.float64(0.2691684790815719), np.float64(0.3415021649125781), np.float64(0.4223129144433474), np.float64(0.44833143615516663), np.float64(0.4615511534695991), np.float64(0.46836566050435036), np.float64(0.4867493069891868)]
Tuning optimizer: 100%|██████████| 4/4 [10:50<00:00, 161.15s/it]Tuning optimizer: 100%|██████████| 4/4 [10:50<00:00, 162.71s/it]

Best optimizer found: nadam
Tuning timesteps:   0%|          | 0/5 [00:00<?, ?it/s]
========== Fold 3 ==========
INFO:root:Using statics: seq_size = 10, genre_vocab_size = 18
Fold 3 Epoch 001; Train loss: 0.8880; Time: 00:00:01
Fold 3 Epoch 002; Train loss: 0.5644; Time: 00:00:01
Fold 3 Epoch 003; Train loss: 0.4128; Time: 00:00:01
Fold 3 Epoch 004; Train loss: 0.3181; Time: 00:00:01
Fold 3 Epoch 005; Train loss: 0.2565; Time: 00:00:01
Fold 3 Epoch 006; Train loss: 0.2129; Time: 00:00:01
Fold 3 Epoch 007; Train loss: 0.1853; Time: 00:00:01
Fold 3 Epoch 008; Train loss: 0.1653; Time: 00:00:02
Fold 3 Epoch 009; Train loss: 0.1493; Time: 00:00:02
Fold 3 Epoch 010; Train loss: 0.1376; Time: 00:00:01
Fold 3: Evaluation at Epoch 10
HR@5       NDCG@5     HR@10      NDCG@10   
0.713576   0.558188   0.834437   0.597116  
Loss: 0.1920
/home/nit/Desktop/2PDreamRec/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:243: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
Fold 3 Epoch 011; Train loss: 0.1290; Time: 00:00:01
Fold 3 Epoch 012; Train loss: 0.1202; Time: 00:00:02
Fold 3 Epoch 013; Train loss: 0.1156; Time: 00:00:02
Fold 3 Epoch 014; Train loss: 0.1113; Time: 00:00:01
Fold 3 Epoch 015; Train loss: 0.1063; Time: 00:00:01
Fold 3 Epoch 016; Train loss: 0.1031; Time: 00:00:01
Fold 3 Epoch 017; Train loss: 0.0999; Time: 00:00:01
Fold 3 Epoch 018; Train loss: 0.0979; Time: 00:00:01
Fold 3 Epoch 019; Train loss: 0.0953; Time: 00:00:01
Fold 3 Epoch 020; Train loss: 0.0918; Time: 00:00:01
Fold 3: Evaluation at Epoch 20
HR@5       NDCG@5     HR@10      NDCG@10   
0.607616   0.467050   0.763245   0.517383  
Loss: 0.1199
/home/nit/Desktop/2PDreamRec/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:243: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
Fold 3 Epoch 021; Train loss: 0.0912; Time: 00:00:01
Fold 3 Epoch 022; Train loss: 0.0875; Time: 00:00:01
Fold 3 Epoch 023; Train loss: 0.0877; Time: 00:00:01
Fold 3 Epoch 024; Train loss: 0.0854; Time: 00:00:01
Fold 3 Epoch 025; Train loss: 0.0832; Time: 00:00:01
Fold 3 Epoch 026; Train loss: 0.0837; Time: 00:00:01
Fold 3 Epoch 027; Train loss: 0.0827; Time: 00:00:01
Fold 3 Epoch 028; Train loss: 0.0829; Time: 00:00:02
Fold 3 Epoch 029; Train loss: 0.0810; Time: 00:00:02
Fold 3 Epoch 030; Train loss: 0.0785; Time: 00:00:01
Fold 3: Evaluation at Epoch 30
HR@5       NDCG@5     HR@10      NDCG@10   
0.581126   0.467088   0.730132   0.515492  
Loss: 0.1008
/home/nit/Desktop/2PDreamRec/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:243: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
Fold 3 Epoch 031; Train loss: 0.0798; Time: 00:00:01
Fold 3 Epoch 032; Train loss: 0.0789; Time: 00:00:01
Fold 3 Epoch 033; Train loss: 0.0782; Time: 00:00:01
Fold 3 Epoch 034; Train loss: 0.0764; Time: 00:00:01
Fold 3 Epoch 035; Train loss: 0.0756; Time: 00:00:01
Fold 3 Epoch 036; Train loss: 0.0754; Time: 00:00:01
Fold 3 Epoch 037; Train loss: 0.0751; Time: 00:00:01
Fold 3 Epoch 038; Train loss: 0.0741; Time: 00:00:01
Fold 3 Epoch 039; Train loss: 0.0749; Time: 00:00:01
Fold 3 Epoch 040; Train loss: 0.0734; Time: 00:00:01
Fold 3: Evaluation at Epoch 40
HR@5       NDCG@5     HR@10      NDCG@10   
0.602649   0.472476   0.764901   0.524119  
Loss: 0.0984
/home/nit/Desktop/2PDreamRec/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:243: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
Fold 3 Epoch 041; Train loss: 0.0733; Time: 00:00:01
Fold 3 Epoch 042; Train loss: 0.0732; Time: 00:00:02
Fold 3 Epoch 043; Train loss: 0.0717; Time: 00:00:01
Fold 3 Epoch 044; Train loss: 0.0717; Time: 00:00:01
Fold 3 Epoch 045; Train loss: 0.0719; Time: 00:00:01
Fold 3 Epoch 046; Train loss: 0.0713; Time: 00:00:02
Fold 3 Epoch 047; Train loss: 0.0721; Time: 00:00:01
Fold 3 Epoch 048; Train loss: 0.0695; Time: 00:00:01
Fold 3 Epoch 049; Train loss: 0.0700; Time: 00:00:02
Fold 3 Epoch 050; Train loss: 0.0692; Time: 00:00:02
Fold 3: Evaluation at Epoch 50
HR@5       NDCG@5     HR@10      NDCG@10   
0.579470   0.447234   0.746689   0.501152  
Loss: 0.0864
/home/nit/Desktop/2PDreamRec/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:243: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
Fold 3 Epoch 051; Train loss: 0.0707; Time: 00:00:02
Fold 3 Epoch 052; Train loss: 0.0676; Time: 00:00:02
Fold 3 Epoch 053; Train loss: 0.0681; Time: 00:00:02
Fold 3 Epoch 054; Train loss: 0.0670; Time: 00:00:02
Fold 3 Epoch 055; Train loss: 0.0685; Time: 00:00:02
Fold 3 Epoch 056; Train loss: 0.0668; Time: 00:00:01
Fold 3 Epoch 057; Train loss: 0.0677; Time: 00:00:01
Fold 3 Epoch 058; Train loss: 0.0676; Time: 00:00:01
Fold 3 Epoch 059; Train loss: 0.0661; Time: 00:00:01
Fold 3 Epoch 060; Train loss: 0.0665; Time: 00:00:02
Fold 3: Evaluation at Epoch 60
HR@5       NDCG@5     HR@10      NDCG@10   
0.627483   0.478546   0.771523   0.525079  
Loss: 0.0823
/home/nit/Desktop/2PDreamRec/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:243: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
Fold 3 Epoch 061; Train loss: 0.0653; Time: 00:00:01
Fold 3 Epoch 062; Train loss: 0.0668; Time: 00:00:01
Fold 3 Epoch 063; Train loss: 0.0663; Time: 00:00:02
Fold 3 Epoch 064; Train loss: 0.0659; Time: 00:00:01
Fold 3 Epoch 065; Train loss: 0.0647; Time: 00:00:01
Fold 3 Epoch 066; Train loss: 0.0649; Time: 00:00:01
Fold 3 Epoch 067; Train loss: 0.0642; Time: 00:00:01
Fold 3 Epoch 068; Train loss: 0.0648; Time: 00:00:02
Fold 3 Epoch 069; Train loss: 0.0618; Time: 00:00:02
Fold 3 Epoch 070; Train loss: 0.0640; Time: 00:00:02
Fold 3: Evaluation at Epoch 70
HR@5       NDCG@5     HR@10      NDCG@10   
0.617550   0.468337   0.784768   0.522608  
Loss: 0.0834
/home/nit/Desktop/2PDreamRec/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:243: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
Fold 3 Epoch 071; Train loss: 0.0626; Time: 00:00:01
Fold 3 Epoch 072; Train loss: 0.0619; Time: 00:00:01
Fold 3 Epoch 073; Train loss: 0.0612; Time: 00:00:01
Fold 3 Epoch 074; Train loss: 0.0604; Time: 00:00:01
Fold 3 Epoch 075; Train loss: 0.0633; Time: 00:00:02
Fold 3 Epoch 076; Train loss: 0.0621; Time: 00:00:01
Fold 3 Epoch 077; Train loss: 0.0603; Time: 00:00:01
Fold 3 Epoch 078; Train loss: 0.0621; Time: 00:00:01
Fold 3 Epoch 079; Train loss: 0.0608; Time: 00:00:02
Fold 3 Epoch 080; Train loss: 0.0612; Time: 00:00:02
Fold 3: Evaluation at Epoch 80
HR@5       NDCG@5     HR@10      NDCG@10   
0.662252   0.512575   0.804636   0.558866  
Loss: 0.0769
/home/nit/Desktop/2PDreamRec/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:243: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
[timesteps candidate 100] Fold 3: NDCG@10 = [np.float64(0.5971164329255074), np.float64(0.5173828709433645), np.float64(0.5154918012318784), np.float64(0.5241190114076685), np.float64(0.5011522892058748), np.float64(0.5250785883496436), np.float64(0.5226079136524578), np.float64(0.558866202552783)]
Tuning timesteps:  20%|██        | 1/5 [02:26<09:46, 146.73s/it]
========== Fold 3 ==========
INFO:root:Using statics: seq_size = 10, genre_vocab_size = 18
Fold 3 Epoch 001; Train loss: 1.0263; Time: 00:00:02
Fold 3 Epoch 002; Train loss: 0.6743; Time: 00:00:01
Fold 3 Epoch 003; Train loss: 0.5133; Time: 00:00:01
Fold 3 Epoch 004; Train loss: 0.4136; Time: 00:00:01
Fold 3 Epoch 005; Train loss: 0.3503; Time: 00:00:01
Fold 3 Epoch 006; Train loss: 0.3007; Time: 00:00:02
Fold 3 Epoch 007; Train loss: 0.2721; Time: 00:00:02
Fold 3 Epoch 008; Train loss: 0.2461; Time: 00:00:02
Fold 3 Epoch 009; Train loss: 0.2283; Time: 00:00:02
Fold 3 Epoch 010; Train loss: 0.2156; Time: 00:00:01
Fold 3: Evaluation at Epoch 10
HR@5       NDCG@5     HR@10      NDCG@10   
0.506623   0.400777   0.692053   0.460526  
Loss: 0.3034
/home/nit/Desktop/2PDreamRec/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:243: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
Fold 3 Epoch 011; Train loss: 0.2026; Time: 00:00:01
Fold 3 Epoch 012; Train loss: 0.1997; Time: 00:00:01
Fold 3 Epoch 013; Train loss: 0.1886; Time: 00:00:02
Fold 3 Epoch 014; Train loss: 0.1837; Time: 00:00:01
Fold 3 Epoch 015; Train loss: 0.1784; Time: 00:00:01
Fold 3 Epoch 016; Train loss: 0.1721; Time: 00:00:01
Fold 3 Epoch 017; Train loss: 0.1710; Time: 00:00:01
Fold 3 Epoch 018; Train loss: 0.1660; Time: 00:00:01
Fold 3 Epoch 019; Train loss: 0.1633; Time: 00:00:01
Fold 3 Epoch 020; Train loss: 0.1591; Time: 00:00:01
Fold 3: Evaluation at Epoch 20
HR@5       NDCG@5     HR@10      NDCG@10   
0.518212   0.409635   0.675497   0.460665  
Loss: 0.2134
/home/nit/Desktop/2PDreamRec/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:243: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
Fold 3 Epoch 021; Train loss: 0.1583; Time: 00:00:01
Fold 3 Epoch 022; Train loss: 0.1555; Time: 00:00:01
Fold 3 Epoch 023; Train loss: 0.1514; Time: 00:00:01
Fold 3 Epoch 024; Train loss: 0.1519; Time: 00:00:01
Fold 3 Epoch 025; Train loss: 0.1487; Time: 00:00:02
Fold 3 Epoch 026; Train loss: 0.1476; Time: 00:00:01
Fold 3 Epoch 027; Train loss: 0.1436; Time: 00:00:02
Fold 3 Epoch 028; Train loss: 0.1425; Time: 00:00:02
Fold 3 Epoch 029; Train loss: 0.1429; Time: 00:00:01
Fold 3 Epoch 030; Train loss: 0.1393; Time: 00:00:02
Fold 3: Evaluation at Epoch 30
HR@5       NDCG@5     HR@10      NDCG@10   
0.543046   0.441260   0.667219   0.480977  
Loss: 0.1909
/home/nit/Desktop/2PDreamRec/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:243: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
Fold 3 Epoch 031; Train loss: 0.1404; Time: 00:00:01
Fold 3 Epoch 032; Train loss: 0.1410; Time: 00:00:01
Fold 3 Epoch 033; Train loss: 0.1393; Time: 00:00:02
Fold 3 Epoch 034; Train loss: 0.1352; Time: 00:00:01
Fold 3 Epoch 035; Train loss: 0.1378; Time: 00:00:01
Fold 3 Epoch 036; Train loss: 0.1302; Time: 00:00:01
Fold 3 Epoch 037; Train loss: 0.1325; Time: 00:00:01
Fold 3 Epoch 038; Train loss: 0.1335; Time: 00:00:01
Fold 3 Epoch 039; Train loss: 0.1275; Time: 00:00:02
Fold 3 Epoch 040; Train loss: 0.1318; Time: 00:00:01
Fold 3: Evaluation at Epoch 40
HR@5       NDCG@5     HR@10      NDCG@10   
0.509934   0.418620   0.680464   0.473714  
Loss: 0.1805
/home/nit/Desktop/2PDreamRec/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:243: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
Fold 3 Epoch 041; Train loss: 0.1304; Time: 00:00:02
Fold 3 Epoch 042; Train loss: 0.1291; Time: 00:00:01
Fold 3 Epoch 043; Train loss: 0.1287; Time: 00:00:02
Fold 3 Epoch 044; Train loss: 0.1290; Time: 00:00:02
Fold 3 Epoch 045; Train loss: 0.1253; Time: 00:00:02
Fold 3 Epoch 046; Train loss: 0.1243; Time: 00:00:01
Fold 3 Epoch 047; Train loss: 0.1223; Time: 00:00:01
Fold 3 Epoch 048; Train loss: 0.1244; Time: 00:00:02
Fold 3 Epoch 049; Train loss: 0.1220; Time: 00:00:02
Fold 3 Epoch 050; Train loss: 0.1227; Time: 00:00:02
Fold 3: Evaluation at Epoch 50
HR@5       NDCG@5     HR@10      NDCG@10   
0.544702   0.445484   0.665563   0.483963  
Loss: 0.1640
/home/nit/Desktop/2PDreamRec/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:243: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
Fold 3 Epoch 051; Train loss: 0.1195; Time: 00:00:01
Fold 3 Epoch 052; Train loss: 0.1171; Time: 00:00:02
Fold 3 Epoch 053; Train loss: 0.1188; Time: 00:00:01
Fold 3 Epoch 054; Train loss: 0.1210; Time: 00:00:01
Fold 3 Epoch 055; Train loss: 0.1184; Time: 00:00:02
Fold 3 Epoch 056; Train loss: 0.1150; Time: 00:00:01
Fold 3 Epoch 057; Train loss: 0.1161; Time: 00:00:02
Fold 3 Epoch 058; Train loss: 0.1171; Time: 00:00:01
Fold 3 Epoch 059; Train loss: 0.1195; Time: 00:00:01
Fold 3 Epoch 060; Train loss: 0.1126; Time: 00:00:01
Fold 3: Evaluation at Epoch 60
HR@5       NDCG@5     HR@10      NDCG@10   
0.496689   0.403131   0.644040   0.450006  
Loss: 0.1486
/home/nit/Desktop/2PDreamRec/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:243: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
Fold 3 Epoch 061; Train loss: 0.1106; Time: 00:00:02
Fold 3 Epoch 062; Train loss: 0.1146; Time: 00:00:01
Fold 3 Epoch 063; Train loss: 0.1141; Time: 00:00:02
Fold 3 Epoch 064; Train loss: 0.1135; Time: 00:00:02
Fold 3 Epoch 065; Train loss: 0.1122; Time: 00:00:02
Fold 3 Epoch 066; Train loss: 0.1095; Time: 00:00:01
Fold 3 Epoch 067; Train loss: 0.1079; Time: 00:00:02
Fold 3 Epoch 068; Train loss: 0.1085; Time: 00:00:02
Fold 3 Epoch 069; Train loss: 0.1125; Time: 00:00:01
Fold 3 Epoch 070; Train loss: 0.1067; Time: 00:00:01
Fold 3: Evaluation at Epoch 70
HR@5       NDCG@5     HR@10      NDCG@10   
0.536424   0.427364   0.668874   0.470440  
Loss: 0.1461
/home/nit/Desktop/2PDreamRec/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:243: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
Fold 3 Epoch 071; Train loss: 0.1083; Time: 00:00:02
Fold 3 Epoch 072; Train loss: 0.1061; Time: 00:00:02
Fold 3 Epoch 073; Train loss: 0.1059; Time: 00:00:02
Fold 3 Epoch 074; Train loss: 0.1053; Time: 00:00:01
Fold 3 Epoch 075; Train loss: 0.1052; Time: 00:00:01
Fold 3 Epoch 076; Train loss: 0.1031; Time: 00:00:01
Fold 3 Epoch 077; Train loss: 0.1028; Time: 00:00:02
Fold 3 Epoch 078; Train loss: 0.1022; Time: 00:00:01
Fold 3 Epoch 079; Train loss: 0.1010; Time: 00:00:02
Fold 3 Epoch 080; Train loss: 0.1028; Time: 00:00:01
Fold 3: Evaluation at Epoch 80
HR@5       NDCG@5     HR@10      NDCG@10   
0.513245   0.424341   0.675497   0.476509  
Loss: 0.1342
/home/nit/Desktop/2PDreamRec/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:243: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
[timesteps candidate 200] Fold 3: NDCG@10 = [np.float64(0.460525646402586), np.float64(0.4606648481943454), np.float64(0.48097686071124124), np.float64(0.47371401706137434), np.float64(0.4839626287029501), np.float64(0.450006238415476), np.float64(0.47043952228283226), np.float64(0.4765085065135932)]
Tuning timesteps:  40%|████      | 2/5 [05:04<07:39, 153.17s/it]
========== Fold 3 ==========
INFO:root:Using statics: seq_size = 10, genre_vocab_size = 18
Fold 3 Epoch 001; Train loss: 1.1364; Time: 00:00:01
Fold 3 Epoch 002; Train loss: 0.8102; Time: 00:00:02
Fold 3 Epoch 003; Train loss: 0.6471; Time: 00:00:02
Fold 3 Epoch 004; Train loss: 0.5458; Time: 00:00:01
Fold 3 Epoch 005; Train loss: 0.4756; Time: 00:00:02
Fold 3 Epoch 006; Train loss: 0.4403; Time: 00:00:02
Fold 3 Epoch 007; Train loss: 0.4038; Time: 00:00:01
Fold 3 Epoch 008; Train loss: 0.3832; Time: 00:00:01
Fold 3 Epoch 009; Train loss: 0.3593; Time: 00:00:01
Fold 3 Epoch 010; Train loss: 0.3449; Time: 00:00:01
Fold 3: Evaluation at Epoch 10
HR@5       NDCG@5     HR@10      NDCG@10   
0.582781   0.431169   0.741722   0.483210  
Loss: 0.3884
/home/nit/Desktop/2PDreamRec/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:243: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
Fold 3 Epoch 011; Train loss: 0.3386; Time: 00:00:01
Fold 3 Epoch 012; Train loss: 0.3276; Time: 00:00:01
Fold 3 Epoch 013; Train loss: 0.3220; Time: 00:00:01
Fold 3 Epoch 014; Train loss: 0.3074; Time: 00:00:01
Fold 3 Epoch 015; Train loss: 0.3074; Time: 00:00:02
Fold 3 Epoch 016; Train loss: 0.2995; Time: 00:00:01
Fold 3 Epoch 017; Train loss: 0.2926; Time: 00:00:02
Fold 3 Epoch 018; Train loss: 0.2886; Time: 00:00:01
Fold 3 Epoch 019; Train loss: 0.2876; Time: 00:00:01
Fold 3 Epoch 020; Train loss: 0.2798; Time: 00:00:02
Fold 3: Evaluation at Epoch 20
HR@5       NDCG@5     HR@10      NDCG@10   
0.612583   0.464410   0.738411   0.504678  
Loss: 0.3246
/home/nit/Desktop/2PDreamRec/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:243: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
Fold 3 Epoch 021; Train loss: 0.2778; Time: 00:00:01
Fold 3 Epoch 022; Train loss: 0.2727; Time: 00:00:01
Fold 3 Epoch 023; Train loss: 0.2719; Time: 00:00:02
Fold 3 Epoch 024; Train loss: 0.2651; Time: 00:00:02
Fold 3 Epoch 025; Train loss: 0.2633; Time: 00:00:01
Fold 3 Epoch 026; Train loss: 0.2605; Time: 00:00:01
Fold 3 Epoch 027; Train loss: 0.2597; Time: 00:00:01
Fold 3 Epoch 028; Train loss: 0.2520; Time: 00:00:01
Fold 3 Epoch 029; Train loss: 0.2536; Time: 00:00:02
Fold 3 Epoch 030; Train loss: 0.2434; Time: 00:00:02
Fold 3: Evaluation at Epoch 30
HR@5       NDCG@5     HR@10      NDCG@10   
0.619205   0.475790   0.711921   0.505151  
Loss: 0.2736
/home/nit/Desktop/2PDreamRec/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:243: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
Fold 3 Epoch 031; Train loss: 0.2351; Time: 00:00:01
Fold 3 Epoch 032; Train loss: 0.2423; Time: 00:00:01
Fold 3 Epoch 033; Train loss: 0.2384; Time: 00:00:01
Fold 3 Epoch 034; Train loss: 0.2408; Time: 00:00:01
Fold 3 Epoch 035; Train loss: 0.2386; Time: 00:00:01
Fold 3 Epoch 036; Train loss: 0.2356; Time: 00:00:01
Fold 3 Epoch 037; Train loss: 0.2359; Time: 00:00:02
Fold 3 Epoch 038; Train loss: 0.2292; Time: 00:00:01
Fold 3 Epoch 039; Train loss: 0.2271; Time: 00:00:01
Fold 3 Epoch 040; Train loss: 0.2195; Time: 00:00:02
Fold 3: Evaluation at Epoch 40
HR@5       NDCG@5     HR@10      NDCG@10   
0.604305   0.478411   0.735099   0.520527  
Loss: 0.2541
/home/nit/Desktop/2PDreamRec/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:243: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
Fold 3 Epoch 041; Train loss: 0.2236; Time: 00:00:01
Fold 3 Epoch 042; Train loss: 0.2178; Time: 00:00:01
Fold 3 Epoch 043; Train loss: 0.2188; Time: 00:00:02
Fold 3 Epoch 044; Train loss: 0.2169; Time: 00:00:01
Fold 3 Epoch 045; Train loss: 0.2120; Time: 00:00:02
Fold 3 Epoch 046; Train loss: 0.2157; Time: 00:00:02
Fold 3 Epoch 047; Train loss: 0.2043; Time: 00:00:02
Fold 3 Epoch 048; Train loss: 0.2050; Time: 00:00:01
Fold 3 Epoch 049; Train loss: 0.2007; Time: 00:00:01
Fold 3 Epoch 050; Train loss: 0.2007; Time: 00:00:02
Fold 3: Evaluation at Epoch 50
HR@5       NDCG@5     HR@10      NDCG@10   
0.567881   0.450990   0.725166   0.501601  
Loss: 0.2314
/home/nit/Desktop/2PDreamRec/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:243: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
Fold 3 Epoch 051; Train loss: 0.2010; Time: 00:00:01
Fold 3 Epoch 052; Train loss: 0.2021; Time: 00:00:02
Fold 3 Epoch 053; Train loss: 0.1951; Time: 00:00:02
Fold 3 Epoch 054; Train loss: 0.1958; Time: 00:00:01
Fold 3 Epoch 055; Train loss: 0.1923; Time: 00:00:01
Fold 3 Epoch 056; Train loss: 0.1897; Time: 00:00:01
Fold 3 Epoch 057; Train loss: 0.1920; Time: 00:00:02
Fold 3 Epoch 058; Train loss: 0.1873; Time: 00:00:01
Fold 3 Epoch 059; Train loss: 0.1834; Time: 00:00:01
Fold 3 Epoch 060; Train loss: 0.1814; Time: 00:00:02
Fold 3: Evaluation at Epoch 60
HR@5       NDCG@5     HR@10      NDCG@10   
0.529801   0.433312   0.698675   0.488892  
Loss: 0.2215
/home/nit/Desktop/2PDreamRec/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:243: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
Fold 3 Epoch 061; Train loss: 0.1830; Time: 00:00:02
Fold 3 Epoch 062; Train loss: 0.1774; Time: 00:00:01
Fold 3 Epoch 063; Train loss: 0.1744; Time: 00:00:02
Fold 3 Epoch 064; Train loss: 0.1750; Time: 00:00:02
Fold 3 Epoch 065; Train loss: 0.1736; Time: 00:00:02
Fold 3 Epoch 066; Train loss: 0.1736; Time: 00:00:02
Fold 3 Epoch 067; Train loss: 0.1727; Time: 00:00:01
Fold 3 Epoch 068; Train loss: 0.1693; Time: 00:00:02
Fold 3 Epoch 069; Train loss: 0.1679; Time: 00:00:01
Fold 3 Epoch 070; Train loss: 0.1643; Time: 00:00:02
Fold 3: Evaluation at Epoch 70
HR@5       NDCG@5     HR@10      NDCG@10   
0.528146   0.426850   0.735099   0.494202  
Loss: 0.1859
/home/nit/Desktop/2PDreamRec/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:243: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
Fold 3 Epoch 071; Train loss: 0.1612; Time: 00:00:01
Fold 3 Epoch 072; Train loss: 0.1646; Time: 00:00:01
Fold 3 Epoch 073; Train loss: 0.1575; Time: 00:00:02
Fold 3 Epoch 074; Train loss: 0.1572; Time: 00:00:01
Fold 3 Epoch 075; Train loss: 0.1558; Time: 00:00:02
Fold 3 Epoch 076; Train loss: 0.1533; Time: 00:00:01
Fold 3 Epoch 077; Train loss: 0.1509; Time: 00:00:01
Fold 3 Epoch 078; Train loss: 0.1530; Time: 00:00:01
Fold 3 Epoch 079; Train loss: 0.1466; Time: 00:00:01
Fold 3 Epoch 080; Train loss: 0.1480; Time: 00:00:01
Fold 3: Evaluation at Epoch 80
HR@5       NDCG@5     HR@10      NDCG@10   
0.526490   0.430750   0.733444   0.495547  
Loss: 0.1590
/home/nit/Desktop/2PDreamRec/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:243: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
[timesteps candidate 400] Fold 3: NDCG@10 = [np.float64(0.48321025365642084), np.float64(0.5046776885404579), np.float64(0.5051507758848687), np.float64(0.5205265234295054), np.float64(0.5016014081015022), np.float64(0.4888923402232004), np.float64(0.49420232984997997), np.float64(0.49554737835476353)]
Tuning timesteps:  60%|██████    | 3/5 [07:47<05:15, 157.90s/it]
========== Fold 3 ==========
INFO:root:Using statics: seq_size = 10, genre_vocab_size = 18
Fold 3 Epoch 001; Train loss: 0.9152; Time: 00:00:01
Fold 3 Epoch 002; Train loss: 0.6758; Time: 00:00:01
Fold 3 Epoch 003; Train loss: 0.5680; Time: 00:00:01
Fold 3 Epoch 004; Train loss: 0.5035; Time: 00:00:01
Fold 3 Epoch 005; Train loss: 0.4609; Time: 00:00:02
Fold 3 Epoch 006; Train loss: 0.4250; Time: 00:00:01
Fold 3 Epoch 007; Train loss: 0.4048; Time: 00:00:02
Fold 3 Epoch 008; Train loss: 0.3803; Time: 00:00:01
Fold 3 Epoch 009; Train loss: 0.3683; Time: 00:00:01
Fold 3 Epoch 010; Train loss: 0.3521; Time: 00:00:01
Fold 3: Evaluation at Epoch 10
HR@5       NDCG@5     HR@10      NDCG@10   
0.647351   0.472851   0.801325   0.522113  
Loss: 0.4210
/home/nit/Desktop/2PDreamRec/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:243: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
Fold 3 Epoch 011; Train loss: 0.3481; Time: 00:00:01
Fold 3 Epoch 012; Train loss: 0.3341; Time: 00:00:01
Fold 3 Epoch 013; Train loss: 0.3330; Time: 00:00:01
Fold 3 Epoch 014; Train loss: 0.3181; Time: 00:00:01
Fold 3 Epoch 015; Train loss: 0.3185; Time: 00:00:01
Fold 3 Epoch 016; Train loss: 0.3102; Time: 00:00:02
Fold 3 Epoch 017; Train loss: 0.3054; Time: 00:00:01
Fold 3 Epoch 018; Train loss: 0.3013; Time: 00:00:01
Fold 3 Epoch 019; Train loss: 0.2941; Time: 00:00:01
Fold 3 Epoch 020; Train loss: 0.2948; Time: 00:00:01
Fold 3: Evaluation at Epoch 20
HR@5       NDCG@5     HR@10      NDCG@10   
0.695364   0.530666   0.822848   0.571891  
Loss: 0.3588
/home/nit/Desktop/2PDreamRec/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:243: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
Fold 3 Epoch 021; Train loss: 0.2838; Time: 00:00:02
Fold 3 Epoch 022; Train loss: 0.2827; Time: 00:00:02
Fold 3 Epoch 023; Train loss: 0.2764; Time: 00:00:01
Fold 3 Epoch 024; Train loss: 0.2792; Time: 00:00:01
Fold 3 Epoch 025; Train loss: 0.2664; Time: 00:00:01
Fold 3 Epoch 026; Train loss: 0.2658; Time: 00:00:01
Fold 3 Epoch 027; Train loss: 0.2612; Time: 00:00:01
Fold 3 Epoch 028; Train loss: 0.2567; Time: 00:00:01
Fold 3 Epoch 029; Train loss: 0.2565; Time: 00:00:02
Fold 3 Epoch 030; Train loss: 0.2498; Time: 00:00:02
Fold 3: Evaluation at Epoch 30
HR@5       NDCG@5     HR@10      NDCG@10   
0.710265   0.562790   0.822848   0.598904  
Loss: 0.3014
/home/nit/Desktop/2PDreamRec/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:243: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
Fold 3 Epoch 031; Train loss: 0.2490; Time: 00:00:01
Fold 3 Epoch 032; Train loss: 0.2427; Time: 00:00:01
Fold 3 Epoch 033; Train loss: 0.2477; Time: 00:00:01
Fold 3 Epoch 034; Train loss: 0.2422; Time: 00:00:02
Fold 3 Epoch 035; Train loss: 0.2363; Time: 00:00:02
Fold 3 Epoch 036; Train loss: 0.2347; Time: 00:00:02
Fold 3 Epoch 037; Train loss: 0.2359; Time: 00:00:01
Fold 3 Epoch 038; Train loss: 0.2294; Time: 00:00:01
Fold 3 Epoch 039; Train loss: 0.2280; Time: 00:00:01
Fold 3 Epoch 040; Train loss: 0.2267; Time: 00:00:01
Fold 3: Evaluation at Epoch 40
HR@5       NDCG@5     HR@10      NDCG@10   
0.675497   0.538892   0.806291   0.580711  
Loss: 0.2804
/home/nit/Desktop/2PDreamRec/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:243: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
Fold 3 Epoch 041; Train loss: 0.2204; Time: 00:00:02
Fold 3 Epoch 042; Train loss: 0.2173; Time: 00:00:01
Fold 3 Epoch 043; Train loss: 0.2160; Time: 00:00:01
Fold 3 Epoch 044; Train loss: 0.2118; Time: 00:00:02
Fold 3 Epoch 045; Train loss: 0.2096; Time: 00:00:02
Fold 3 Epoch 046; Train loss: 0.2046; Time: 00:00:01
Fold 3 Epoch 047; Train loss: 0.2005; Time: 00:00:01
Fold 3 Epoch 048; Train loss: 0.2012; Time: 00:00:02
Fold 3 Epoch 049; Train loss: 0.2015; Time: 00:00:01
Fold 3 Epoch 050; Train loss: 0.1981; Time: 00:00:02
Fold 3: Evaluation at Epoch 50
HR@5       NDCG@5     HR@10      NDCG@10   
0.731788   0.559094   0.834437   0.592100  
Loss: 0.2293
/home/nit/Desktop/2PDreamRec/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:243: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
Fold 3 Epoch 051; Train loss: 0.1977; Time: 00:00:02
Fold 3 Epoch 052; Train loss: 0.1944; Time: 00:00:02
Fold 3 Epoch 053; Train loss: 0.1882; Time: 00:00:02
Fold 3 Epoch 054; Train loss: 0.1883; Time: 00:00:01
Fold 3 Epoch 055; Train loss: 0.1862; Time: 00:00:02
Fold 3 Epoch 056; Train loss: 0.1817; Time: 00:00:02
Fold 3 Epoch 057; Train loss: 0.1798; Time: 00:00:02
Fold 3 Epoch 058; Train loss: 0.1766; Time: 00:00:02
Fold 3 Epoch 059; Train loss: 0.1776; Time: 00:00:01
Fold 3 Epoch 060; Train loss: 0.1736; Time: 00:00:01
Fold 3: Evaluation at Epoch 60
HR@5       NDCG@5     HR@10      NDCG@10   
0.723510   0.558478   0.839404   0.595358  
Loss: 0.2005
/home/nit/Desktop/2PDreamRec/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:243: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
Fold 3 Epoch 061; Train loss: 0.1718; Time: 00:00:02
Fold 3 Epoch 062; Train loss: 0.1697; Time: 00:00:01
Fold 3 Epoch 063; Train loss: 0.1687; Time: 00:00:02
Fold 3 Epoch 064; Train loss: 0.1653; Time: 00:00:01
Fold 3 Epoch 065; Train loss: 0.1604; Time: 00:00:01
Fold 3 Epoch 066; Train loss: 0.1605; Time: 00:00:01
Fold 3 Epoch 067; Train loss: 0.1572; Time: 00:00:01
Fold 3 Epoch 068; Train loss: 0.1565; Time: 00:00:01
Fold 3 Epoch 069; Train loss: 0.1510; Time: 00:00:01
Fold 3 Epoch 070; Train loss: 0.1508; Time: 00:00:01
Fold 3: Evaluation at Epoch 70
HR@5       NDCG@5     HR@10      NDCG@10   
0.701987   0.532013   0.837748   0.576090  
Loss: 0.1909
/home/nit/Desktop/2PDreamRec/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:243: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
Fold 3 Epoch 071; Train loss: 0.1508; Time: 00:00:01
Fold 3 Epoch 072; Train loss: 0.1467; Time: 00:00:01
Fold 3 Epoch 073; Train loss: 0.1460; Time: 00:00:02
Fold 3 Epoch 074; Train loss: 0.1428; Time: 00:00:01
Fold 3 Epoch 075; Train loss: 0.1418; Time: 00:00:01
Fold 3 Epoch 076; Train loss: 0.1383; Time: 00:00:01
Fold 3 Epoch 077; Train loss: 0.1374; Time: 00:00:01
Fold 3 Epoch 078; Train loss: 0.1352; Time: 00:00:02
Fold 3 Epoch 079; Train loss: 0.1329; Time: 00:00:01
Fold 3 Epoch 080; Train loss: 0.1318; Time: 00:00:01
Fold 3: Evaluation at Epoch 80
HR@5       NDCG@5     HR@10      NDCG@10   
0.713576   0.552719   0.850993   0.597645  
Loss: 0.1639
/home/nit/Desktop/2PDreamRec/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:243: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
[timesteps candidate 600] Fold 3: NDCG@10 = [np.float64(0.5221131090027851), np.float64(0.5718905159433213), np.float64(0.598904318607283), np.float64(0.5807107324062524), np.float64(0.5921002653090176), np.float64(0.595357632394553), np.float64(0.576090103513069), np.float64(0.5976454334127927)]
Tuning timesteps:  80%|████████  | 4/5 [10:28<02:38, 158.80s/it]
========== Fold 3 ==========
INFO:root:Using statics: seq_size = 10, genre_vocab_size = 18
Fold 3 Epoch 001; Train loss: 0.9315; Time: 00:00:01
Fold 3 Epoch 002; Train loss: 0.6935; Time: 00:00:01
Fold 3 Epoch 003; Train loss: 0.5990; Time: 00:00:01
Fold 3 Epoch 004; Train loss: 0.5366; Time: 00:00:01
Fold 3 Epoch 005; Train loss: 0.4919; Time: 00:00:01
Fold 3 Epoch 006; Train loss: 0.4567; Time: 00:00:01
Fold 3 Epoch 007; Train loss: 0.4366; Time: 00:00:01
Fold 3 Epoch 008; Train loss: 0.4198; Time: 00:00:01
Fold 3 Epoch 009; Train loss: 0.4028; Time: 00:00:02
Fold 3 Epoch 010; Train loss: 0.3873; Time: 00:00:02
Fold 3: Evaluation at Epoch 10
HR@5       NDCG@5     HR@10      NDCG@10   
0.564570   0.488780   0.718543   0.538024  
Loss: 0.5343
/home/nit/Desktop/2PDreamRec/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:243: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
Fold 3 Epoch 011; Train loss: 0.3757; Time: 00:00:01
Fold 3 Epoch 012; Train loss: 0.3664; Time: 00:00:02
Fold 3 Epoch 013; Train loss: 0.3653; Time: 00:00:02
Fold 3 Epoch 014; Train loss: 0.3508; Time: 00:00:02
Fold 3 Epoch 015; Train loss: 0.3470; Time: 00:00:01
Fold 3 Epoch 016; Train loss: 0.3347; Time: 00:00:01
Fold 3 Epoch 017; Train loss: 0.3366; Time: 00:00:02
Fold 3 Epoch 018; Train loss: 0.3302; Time: 00:00:02
Fold 3 Epoch 019; Train loss: 0.3175; Time: 00:00:01
Fold 3 Epoch 020; Train loss: 0.3150; Time: 00:00:01
Fold 3: Evaluation at Epoch 20
HR@5       NDCG@5     HR@10      NDCG@10   
0.615894   0.515923   0.738411   0.554576  
Loss: 0.4393
/home/nit/Desktop/2PDreamRec/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:243: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
Fold 3 Epoch 021; Train loss: 0.3148; Time: 00:00:01
Fold 3 Epoch 022; Train loss: 0.3081; Time: 00:00:01
Fold 3 Epoch 023; Train loss: 0.3038; Time: 00:00:01
Fold 3 Epoch 024; Train loss: 0.2972; Time: 00:00:02
Fold 3 Epoch 025; Train loss: 0.2945; Time: 00:00:01
Fold 3 Epoch 026; Train loss: 0.2879; Time: 00:00:02
Fold 3 Epoch 027; Train loss: 0.2884; Time: 00:00:01
Fold 3 Epoch 028; Train loss: 0.2880; Time: 00:00:01
Fold 3 Epoch 029; Train loss: 0.2806; Time: 00:00:01
Fold 3 Epoch 030; Train loss: 0.2731; Time: 00:00:01
Fold 3: Evaluation at Epoch 30
HR@5       NDCG@5     HR@10      NDCG@10   
0.597682   0.511774   0.720199   0.550809  
Loss: 0.3692
/home/nit/Desktop/2PDreamRec/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:243: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
Fold 3 Epoch 031; Train loss: 0.2744; Time: 00:00:02
Fold 3 Epoch 032; Train loss: 0.2652; Time: 00:00:02
Fold 3 Epoch 033; Train loss: 0.2638; Time: 00:00:02
Fold 3 Epoch 034; Train loss: 0.2609; Time: 00:00:02
Fold 3 Epoch 035; Train loss: 0.2573; Time: 00:00:01
Fold 3 Epoch 036; Train loss: 0.2556; Time: 00:00:01
Fold 3 Epoch 037; Train loss: 0.2490; Time: 00:00:02
Fold 3 Epoch 038; Train loss: 0.2474; Time: 00:00:01
Fold 3 Epoch 039; Train loss: 0.2406; Time: 00:00:01
Fold 3 Epoch 040; Train loss: 0.2371; Time: 00:00:02
Fold 3: Evaluation at Epoch 40
HR@5       NDCG@5     HR@10      NDCG@10   
0.587748   0.494864   0.710265   0.534238  
Loss: 0.3410
/home/nit/Desktop/2PDreamRec/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:243: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
Fold 3 Epoch 041; Train loss: 0.2365; Time: 00:00:02
Fold 3 Epoch 042; Train loss: 0.2345; Time: 00:00:01
Fold 3 Epoch 043; Train loss: 0.2297; Time: 00:00:01
Fold 3 Epoch 044; Train loss: 0.2302; Time: 00:00:01
Fold 3 Epoch 045; Train loss: 0.2258; Time: 00:00:01
Fold 3 Epoch 046; Train loss: 0.2211; Time: 00:00:01
Fold 3 Epoch 047; Train loss: 0.2166; Time: 00:00:01
Fold 3 Epoch 048; Train loss: 0.2126; Time: 00:00:02
Fold 3 Epoch 049; Train loss: 0.2098; Time: 00:00:02
Fold 3 Epoch 050; Train loss: 0.2102; Time: 00:00:01
Fold 3: Evaluation at Epoch 50
HR@5       NDCG@5     HR@10      NDCG@10   
0.630795   0.533133   0.730132   0.564981  
Loss: 0.3053
/home/nit/Desktop/2PDreamRec/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:243: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
Fold 3 Epoch 051; Train loss: 0.2061; Time: 00:00:02
Fold 3 Epoch 052; Train loss: 0.2055; Time: 00:00:02
Fold 3 Epoch 053; Train loss: 0.2001; Time: 00:00:02
Fold 3 Epoch 054; Train loss: 0.1968; Time: 00:00:02
Fold 3 Epoch 055; Train loss: 0.1972; Time: 00:00:01
Fold 3 Epoch 056; Train loss: 0.1910; Time: 00:00:01
Fold 3 Epoch 057; Train loss: 0.1867; Time: 00:00:01
Fold 3 Epoch 058; Train loss: 0.1871; Time: 00:00:01
Fold 3 Epoch 059; Train loss: 0.1863; Time: 00:00:01
Fold 3 Epoch 060; Train loss: 0.1793; Time: 00:00:02
Fold 3: Evaluation at Epoch 60
HR@5       NDCG@5     HR@10      NDCG@10   
0.667219   0.552126   0.753311   0.579744  
Loss: 0.2477
/home/nit/Desktop/2PDreamRec/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:243: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
Fold 3 Epoch 061; Train loss: 0.1783; Time: 00:00:02
Fold 3 Epoch 062; Train loss: 0.1769; Time: 00:00:02
Fold 3 Epoch 063; Train loss: 0.1732; Time: 00:00:01
Fold 3 Epoch 064; Train loss: 0.1725; Time: 00:00:01
Fold 3 Epoch 065; Train loss: 0.1670; Time: 00:00:01
Fold 3 Epoch 066; Train loss: 0.1649; Time: 00:00:01
Fold 3 Epoch 067; Train loss: 0.1641; Time: 00:00:01
Fold 3 Epoch 068; Train loss: 0.1615; Time: 00:00:01
Fold 3 Epoch 069; Train loss: 0.1605; Time: 00:00:01
Fold 3 Epoch 070; Train loss: 0.1579; Time: 00:00:02
Fold 3: Evaluation at Epoch 70
HR@5       NDCG@5     HR@10      NDCG@10   
0.627483   0.532432   0.735099   0.566841  
Loss: 0.2262
/home/nit/Desktop/2PDreamRec/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:243: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
Fold 3 Epoch 071; Train loss: 0.1564; Time: 00:00:01
Fold 3 Epoch 072; Train loss: 0.1529; Time: 00:00:01
Fold 3 Epoch 073; Train loss: 0.1500; Time: 00:00:02
Fold 3 Epoch 074; Train loss: 0.1498; Time: 00:00:02
Fold 3 Epoch 075; Train loss: 0.1458; Time: 00:00:02
Fold 3 Epoch 076; Train loss: 0.1410; Time: 00:00:02
Fold 3 Epoch 077; Train loss: 0.1420; Time: 00:00:01
Fold 3 Epoch 078; Train loss: 0.1387; Time: 00:00:01
Fold 3 Epoch 079; Train loss: 0.1369; Time: 00:00:01
Fold 3 Epoch 080; Train loss: 0.1345; Time: 00:00:01
Fold 3: Evaluation at Epoch 80
HR@5       NDCG@5     HR@10      NDCG@10   
0.645695   0.543642   0.761589   0.580723  
Loss: 0.1959
/home/nit/Desktop/2PDreamRec/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:243: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
[timesteps candidate 800] Fold 3: NDCG@10 = [np.float64(0.5380236497557227), np.float64(0.5545757130815254), np.float64(0.5508091926100133), np.float64(0.5342380570560318), np.float64(0.5649805298958128), np.float64(0.5797442355286564), np.float64(0.5668414404642568), np.float64(0.5807229518957807)]
Tuning timesteps: 100%|██████████| 5/5 [13:20<00:00, 163.86s/it]Tuning timesteps: 100%|██████████| 5/5 [13:20<00:00, 160.19s/it]

Best timesteps found: 600
Tuning dropout_rate:   0%|          | 0/7 [00:00<?, ?it/s]
========== Fold 3 ==========
INFO:root:Using statics: seq_size = 10, genre_vocab_size = 18
Fold 3 Epoch 001; Train loss: 0.9180; Time: 00:00:02
Fold 3 Epoch 002; Train loss: 0.6730; Time: 00:00:01
Fold 3 Epoch 003; Train loss: 0.5655; Time: 00:00:01
Fold 3 Epoch 004; Train loss: 0.4940; Time: 00:00:01
Fold 3 Epoch 005; Train loss: 0.4452; Time: 00:00:01
Fold 3 Epoch 006; Train loss: 0.4102; Time: 00:00:01
Fold 3 Epoch 007; Train loss: 0.3913; Time: 00:00:01
Fold 3 Epoch 008; Train loss: 0.3674; Time: 00:00:01
Fold 3 Epoch 009; Train loss: 0.3554; Time: 00:00:02
Fold 3 Epoch 010; Train loss: 0.3468; Time: 00:00:01
Fold 3: Evaluation at Epoch 10
HR@5       NDCG@5     HR@10      NDCG@10   
0.687086   0.542795   0.793046   0.576713  
Loss: 0.4539
/home/nit/Desktop/2PDreamRec/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:243: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
Fold 3 Epoch 011; Train loss: 0.3316; Time: 00:00:02
Fold 3 Epoch 012; Train loss: 0.3171; Time: 00:00:01
Fold 3 Epoch 013; Train loss: 0.3143; Time: 00:00:02
Fold 3 Epoch 014; Train loss: 0.3142; Time: 00:00:01
Fold 3 Epoch 015; Train loss: 0.2984; Time: 00:00:01
Fold 3 Epoch 016; Train loss: 0.2932; Time: 00:00:01
Fold 3 Epoch 017; Train loss: 0.2909; Time: 00:00:01
Fold 3 Epoch 018; Train loss: 0.2878; Time: 00:00:01
Fold 3 Epoch 019; Train loss: 0.2783; Time: 00:00:01
Fold 3 Epoch 020; Train loss: 0.2779; Time: 00:00:01
Fold 3: Evaluation at Epoch 20
HR@5       NDCG@5     HR@10      NDCG@10   
0.728477   0.568576   0.834437   0.602270  
Loss: 0.3510
/home/nit/Desktop/2PDreamRec/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:243: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
Fold 3 Epoch 021; Train loss: 0.2753; Time: 00:00:01
Fold 3 Epoch 022; Train loss: 0.2680; Time: 00:00:02
Fold 3 Epoch 023; Train loss: 0.2667; Time: 00:00:02
Fold 3 Epoch 024; Train loss: 0.2631; Time: 00:00:01
Fold 3 Epoch 025; Train loss: 0.2587; Time: 00:00:01
Fold 3 Epoch 026; Train loss: 0.2556; Time: 00:00:02
Fold 3 Epoch 027; Train loss: 0.2499; Time: 00:00:02
Fold 3 Epoch 028; Train loss: 0.2501; Time: 00:00:02
Fold 3 Epoch 029; Train loss: 0.2450; Time: 00:00:02
Fold 3 Epoch 030; Train loss: 0.2414; Time: 00:00:01
Fold 3: Evaluation at Epoch 30
HR@5       NDCG@5     HR@10      NDCG@10   
0.670530   0.539327   0.784768   0.576035  
Loss: 0.3136
/home/nit/Desktop/2PDreamRec/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:243: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
Fold 3 Epoch 031; Train loss: 0.2394; Time: 00:00:01
Fold 3 Epoch 032; Train loss: 0.2300; Time: 00:00:01
Fold 3 Epoch 033; Train loss: 0.2324; Time: 00:00:01
Fold 3 Epoch 034; Train loss: 0.2295; Time: 00:00:02
Fold 3 Epoch 035; Train loss: 0.2244; Time: 00:00:01
Fold 3 Epoch 036; Train loss: 0.2224; Time: 00:00:01
Fold 3 Epoch 037; Train loss: 0.2212; Time: 00:00:01
Fold 3 Epoch 038; Train loss: 0.2176; Time: 00:00:01
Fold 3 Epoch 039; Train loss: 0.2144; Time: 00:00:02
Fold 3 Epoch 040; Train loss: 0.2129; Time: 00:00:01
Fold 3: Evaluation at Epoch 40
HR@5       NDCG@5     HR@10      NDCG@10   
0.705298   0.569935   0.821192   0.607216  
Loss: 0.2767
/home/nit/Desktop/2PDreamRec/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:243: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
Fold 3 Epoch 041; Train loss: 0.2088; Time: 00:00:01
Fold 3 Epoch 042; Train loss: 0.2077; Time: 00:00:02
Fold 3 Epoch 043; Train loss: 0.2005; Time: 00:00:02
Fold 3 Epoch 044; Train loss: 0.2003; Time: 00:00:01
Fold 3 Epoch 045; Train loss: 0.2003; Time: 00:00:01
Fold 3 Epoch 046; Train loss: 0.1937; Time: 00:00:01
Fold 3 Epoch 047; Train loss: 0.1972; Time: 00:00:01
Fold 3 Epoch 048; Train loss: 0.1917; Time: 00:00:02
Fold 3 Epoch 049; Train loss: 0.1898; Time: 00:00:02
Fold 3 Epoch 050; Train loss: 0.1888; Time: 00:00:02
Fold 3: Evaluation at Epoch 50
HR@5       NDCG@5     HR@10      NDCG@10   
0.728477   0.582970   0.824503   0.613805  
Loss: 0.2522
/home/nit/Desktop/2PDreamRec/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:243: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
Fold 3 Epoch 051; Train loss: 0.1845; Time: 00:00:02
Fold 3 Epoch 052; Train loss: 0.1816; Time: 00:00:01
Fold 3 Epoch 053; Train loss: 0.1797; Time: 00:00:02
Fold 3 Epoch 054; Train loss: 0.1761; Time: 00:00:01
Fold 3 Epoch 055; Train loss: 0.1766; Time: 00:00:01
Fold 3 Epoch 056; Train loss: 0.1753; Time: 00:00:01
Fold 3 Epoch 057; Train loss: 0.1712; Time: 00:00:01
Fold 3 Epoch 058; Train loss: 0.1668; Time: 00:00:02
Fold 3 Epoch 059; Train loss: 0.1650; Time: 00:00:02
Fold 3 Epoch 060; Train loss: 0.1607; Time: 00:00:01
Fold 3: Evaluation at Epoch 60
HR@5       NDCG@5     HR@10      NDCG@10   
0.716887   0.572358   0.834437   0.609768  
Loss: 0.2295
/home/nit/Desktop/2PDreamRec/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:243: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
Fold 3 Epoch 061; Train loss: 0.1609; Time: 00:00:01
Fold 3 Epoch 062; Train loss: 0.1612; Time: 00:00:01
Fold 3 Epoch 063; Train loss: 0.1558; Time: 00:00:01
Fold 3 Epoch 064; Train loss: 0.1572; Time: 00:00:02
Fold 3 Epoch 065; Train loss: 0.1556; Time: 00:00:02
Fold 3 Epoch 066; Train loss: 0.1505; Time: 00:00:01
Fold 3 Epoch 067; Train loss: 0.1467; Time: 00:00:01
Fold 3 Epoch 068; Train loss: 0.1471; Time: 00:00:02
Fold 3 Epoch 069; Train loss: 0.1424; Time: 00:00:02
Fold 3 Epoch 070; Train loss: 0.1412; Time: 00:00:02
Fold 3: Evaluation at Epoch 70
HR@5       NDCG@5     HR@10      NDCG@10   
0.753311   0.601602   0.842715   0.630208  
Loss: 0.1923
/home/nit/Desktop/2PDreamRec/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:243: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
Fold 3 Epoch 071; Train loss: 0.1380; Time: 00:00:01
Fold 3 Epoch 072; Train loss: 0.1367; Time: 00:00:01
Fold 3 Epoch 073; Train loss: 0.1351; Time: 00:00:01
Fold 3 Epoch 074; Train loss: 0.1328; Time: 00:00:01
Fold 3 Epoch 075; Train loss: 0.1301; Time: 00:00:02
Fold 3 Epoch 076; Train loss: 0.1324; Time: 00:00:02
Fold 3 Epoch 077; Train loss: 0.1265; Time: 00:00:01
Fold 3 Epoch 078; Train loss: 0.1273; Time: 00:00:01
Fold 3 Epoch 079; Train loss: 0.1260; Time: 00:00:02
Fold 3 Epoch 080; Train loss: 0.1229; Time: 00:00:02
Fold 3: Evaluation at Epoch 80
HR@5       NDCG@5     HR@10      NDCG@10   
0.721854   0.581442   0.807947   0.609413  
Loss: 0.1692
/home/nit/Desktop/2PDreamRec/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:243: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
[dropout_rate candidate 0.05] Fold 3: NDCG@10 = [np.float64(0.5767128399983018), np.float64(0.6022704696735915), np.float64(0.5760345183250883), np.float64(0.6072160141964165), np.float64(0.6138049210346539), np.float64(0.6097676367792016), np.float64(0.6302076460186264), np.float64(0.6094130551770521)]
Tuning dropout_rate:  14%|█▍        | 1/7 [02:48<16:49, 168.19s/it]
========== Fold 3 ==========
INFO:root:Using statics: seq_size = 10, genre_vocab_size = 18
Fold 3 Epoch 001; Train loss: 1.0562; Time: 00:00:01
Fold 3 Epoch 002; Train loss: 0.7924; Time: 00:00:02
Fold 3 Epoch 003; Train loss: 0.6600; Time: 00:00:01
Fold 3 Epoch 004; Train loss: 0.5870; Time: 00:00:02
Fold 3 Epoch 005; Train loss: 0.5322; Time: 00:00:02
Fold 3 Epoch 006; Train loss: 0.4932; Time: 00:00:02
Fold 3 Epoch 007; Train loss: 0.4586; Time: 00:00:02
Fold 3 Epoch 008; Train loss: 0.4389; Time: 00:00:01
Fold 3 Epoch 009; Train loss: 0.4230; Time: 00:00:02
Fold 3 Epoch 010; Train loss: 0.4090; Time: 00:00:02
Fold 3: Evaluation at Epoch 10
HR@5       NDCG@5     HR@10      NDCG@10   
0.557947   0.424342   0.715232   0.473701  
Loss: 0.5022
/home/nit/Desktop/2PDreamRec/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:243: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
Fold 3 Epoch 011; Train loss: 0.3970; Time: 00:00:01
Fold 3 Epoch 012; Train loss: 0.3864; Time: 00:00:01
Fold 3 Epoch 013; Train loss: 0.3722; Time: 00:00:01
Fold 3 Epoch 014; Train loss: 0.3676; Time: 00:00:01
Fold 3 Epoch 015; Train loss: 0.3651; Time: 00:00:01
Fold 3 Epoch 016; Train loss: 0.3566; Time: 00:00:02
Fold 3 Epoch 017; Train loss: 0.3487; Time: 00:00:02
Fold 3 Epoch 018; Train loss: 0.3504; Time: 00:00:01
Fold 3 Epoch 019; Train loss: 0.3460; Time: 00:00:02
Fold 3 Epoch 020; Train loss: 0.3341; Time: 00:00:02
Fold 3: Evaluation at Epoch 20
HR@5       NDCG@5     HR@10      NDCG@10   
0.594371   0.458345   0.741722   0.504836  
Loss: 0.4109
/home/nit/Desktop/2PDreamRec/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:243: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
Fold 3 Epoch 021; Train loss: 0.3372; Time: 00:00:01
Fold 3 Epoch 022; Train loss: 0.3319; Time: 00:00:01
Fold 3 Epoch 023; Train loss: 0.3225; Time: 00:00:01
Fold 3 Epoch 024; Train loss: 0.3211; Time: 00:00:02
Fold 3 Epoch 025; Train loss: 0.3138; Time: 00:00:01
Fold 3 Epoch 026; Train loss: 0.3129; Time: 00:00:02
Fold 3 Epoch 027; Train loss: 0.3045; Time: 00:00:01
Fold 3 Epoch 028; Train loss: 0.3026; Time: 00:00:01
Fold 3 Epoch 029; Train loss: 0.3030; Time: 00:00:01
Fold 3 Epoch 030; Train loss: 0.2993; Time: 00:00:01
Fold 3: Evaluation at Epoch 30
HR@5       NDCG@5     HR@10      NDCG@10   
0.564570   0.452010   0.697020   0.494041  
Loss: 0.3234
/home/nit/Desktop/2PDreamRec/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:243: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
Fold 3 Epoch 031; Train loss: 0.2973; Time: 00:00:02
Fold 3 Epoch 032; Train loss: 0.2908; Time: 00:00:01
Fold 3 Epoch 033; Train loss: 0.2876; Time: 00:00:02
Fold 3 Epoch 034; Train loss: 0.2816; Time: 00:00:02
Fold 3 Epoch 035; Train loss: 0.2766; Time: 00:00:02
Fold 3 Epoch 036; Train loss: 0.2811; Time: 00:00:01
Fold 3 Epoch 037; Train loss: 0.2698; Time: 00:00:01
Fold 3 Epoch 038; Train loss: 0.2687; Time: 00:00:01
Fold 3 Epoch 039; Train loss: 0.2624; Time: 00:00:02
Fold 3 Epoch 040; Train loss: 0.2662; Time: 00:00:02
Fold 3: Evaluation at Epoch 40
HR@5       NDCG@5     HR@10      NDCG@10   
0.557947   0.469060   0.710265   0.517484  
Loss: 0.3079
/home/nit/Desktop/2PDreamRec/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:243: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
Fold 3 Epoch 041; Train loss: 0.2677; Time: 00:00:02
Fold 3 Epoch 042; Train loss: 0.2568; Time: 00:00:01
Fold 3 Epoch 043; Train loss: 0.2507; Time: 00:00:01
Fold 3 Epoch 044; Train loss: 0.2537; Time: 00:00:02
Fold 3 Epoch 045; Train loss: 0.2478; Time: 00:00:02
Fold 3 Epoch 046; Train loss: 0.2418; Time: 00:00:02
Fold 3 Epoch 047; Train loss: 0.2458; Time: 00:00:02
Fold 3 Epoch 048; Train loss: 0.2370; Time: 00:00:01
Fold 3 Epoch 049; Train loss: 0.2358; Time: 00:00:01
Fold 3 Epoch 050; Train loss: 0.2396; Time: 00:00:01
Fold 3: Evaluation at Epoch 50
HR@5       NDCG@5     HR@10      NDCG@10   
0.572848   0.474747   0.718543   0.521830  
Loss: 0.2781
/home/nit/Desktop/2PDreamRec/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:243: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
Fold 3 Epoch 051; Train loss: 0.2326; Time: 00:00:02
Fold 3 Epoch 052; Train loss: 0.2265; Time: 00:00:02
Fold 3 Epoch 053; Train loss: 0.2239; Time: 00:00:02
Fold 3 Epoch 054; Train loss: 0.2225; Time: 00:00:02
Fold 3 Epoch 055; Train loss: 0.2171; Time: 00:00:02
Fold 3 Epoch 056; Train loss: 0.2195; Time: 00:00:01
Fold 3 Epoch 057; Train loss: 0.2153; Time: 00:00:01
Fold 3 Epoch 058; Train loss: 0.2171; Time: 00:00:01
Fold 3 Epoch 059; Train loss: 0.2070; Time: 00:00:02
Fold 3 Epoch 060; Train loss: 0.2062; Time: 00:00:02
Fold 3: Evaluation at Epoch 60
HR@5       NDCG@5     HR@10      NDCG@10   
0.584437   0.479364   0.735099   0.528055  
Loss: 0.2465
/home/nit/Desktop/2PDreamRec/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:243: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
Fold 3 Epoch 061; Train loss: 0.2046; Time: 00:00:02
Fold 3 Epoch 062; Train loss: 0.2012; Time: 00:00:02
Fold 3 Epoch 063; Train loss: 0.2014; Time: 00:00:02
Fold 3 Epoch 064; Train loss: 0.1969; Time: 00:00:01
Fold 3 Epoch 065; Train loss: 0.1946; Time: 00:00:01
Fold 3 Epoch 066; Train loss: 0.1948; Time: 00:00:02
Fold 3 Epoch 067; Train loss: 0.1882; Time: 00:00:01
Fold 3 Epoch 068; Train loss: 0.1899; Time: 00:00:01
Fold 3 Epoch 069; Train loss: 0.1861; Time: 00:00:01
Fold 3 Epoch 070; Train loss: 0.1858; Time: 00:00:01
Fold 3: Evaluation at Epoch 70
HR@5       NDCG@5     HR@10      NDCG@10   
0.589404   0.496110   0.708609   0.534666  
Loss: 0.2197
/home/nit/Desktop/2PDreamRec/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:243: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
Fold 3 Epoch 071; Train loss: 0.1810; Time: 00:00:02
Fold 3 Epoch 072; Train loss: 0.1763; Time: 00:00:01
Fold 3 Epoch 073; Train loss: 0.1759; Time: 00:00:01
Fold 3 Epoch 074; Train loss: 0.1763; Time: 00:00:01
Fold 3 Epoch 075; Train loss: 0.1710; Time: 00:00:02
Fold 3 Epoch 076; Train loss: 0.1693; Time: 00:00:01
Fold 3 Epoch 077; Train loss: 0.1666; Time: 00:00:02
Fold 3 Epoch 078; Train loss: 0.1664; Time: 00:00:01
Fold 3 Epoch 079; Train loss: 0.1648; Time: 00:00:01
Fold 3 Epoch 080; Train loss: 0.1635; Time: 00:00:01
Fold 3: Evaluation at Epoch 80
HR@5       NDCG@5     HR@10      NDCG@10   
0.600993   0.496590   0.726821   0.537411  
Loss: 0.1922
/home/nit/Desktop/2PDreamRec/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:243: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
[dropout_rate candidate 0.07] Fold 3: NDCG@10 = [np.float64(0.4737013219308892), np.float64(0.5048362422758995), np.float64(0.49404136294452056), np.float64(0.5174842581698327), np.float64(0.5218304920791849), np.float64(0.528054601472851), np.float64(0.5346657327366412), np.float64(0.5374109831061438)]
Tuning dropout_rate:  29%|██▊       | 2/7 [05:40<14:14, 170.84s/it]
========== Fold 3 ==========
INFO:root:Using statics: seq_size = 10, genre_vocab_size = 18
Fold 3 Epoch 001; Train loss: 0.9202; Time: 00:00:01
Fold 3 Epoch 002; Train loss: 0.6669; Time: 00:00:02
Fold 3 Epoch 003; Train loss: 0.5538; Time: 00:00:02
Fold 3 Epoch 004; Train loss: 0.4811; Time: 00:00:01
Fold 3 Epoch 005; Train loss: 0.4386; Time: 00:00:01
Fold 3 Epoch 006; Train loss: 0.4099; Time: 00:00:02
Fold 3 Epoch 007; Train loss: 0.3862; Time: 00:00:02
Fold 3 Epoch 008; Train loss: 0.3687; Time: 00:00:02
Fold 3 Epoch 009; Train loss: 0.3491; Time: 00:00:02
Fold 3 Epoch 010; Train loss: 0.3464; Time: 00:00:01
Fold 3: Evaluation at Epoch 10
HR@5       NDCG@5     HR@10      NDCG@10   
0.620861   0.503054   0.735099   0.538902  
Loss: 0.4316
/home/nit/Desktop/2PDreamRec/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:243: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
Fold 3 Epoch 011; Train loss: 0.3335; Time: 00:00:01
Fold 3 Epoch 012; Train loss: 0.3264; Time: 00:00:01
Fold 3 Epoch 013; Train loss: 0.3212; Time: 00:00:01
Fold 3 Epoch 014; Train loss: 0.3101; Time: 00:00:02
Fold 3 Epoch 015; Train loss: 0.3018; Time: 00:00:02
Fold 3 Epoch 016; Train loss: 0.3004; Time: 00:00:01
Fold 3 Epoch 017; Train loss: 0.2972; Time: 00:00:01
Fold 3 Epoch 018; Train loss: 0.2890; Time: 00:00:02
Fold 3 Epoch 019; Train loss: 0.2851; Time: 00:00:02
Fold 3 Epoch 020; Train loss: 0.2853; Time: 00:00:02
Fold 3: Evaluation at Epoch 20
HR@5       NDCG@5     HR@10      NDCG@10   
0.649007   0.532305   0.758278   0.567071  
Loss: 0.3257
/home/nit/Desktop/2PDreamRec/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:243: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
Fold 3 Epoch 021; Train loss: 0.2805; Time: 00:00:02
Fold 3 Epoch 022; Train loss: 0.2763; Time: 00:00:02
Fold 3 Epoch 023; Train loss: 0.2700; Time: 00:00:02
Fold 3 Epoch 024; Train loss: 0.2633; Time: 00:00:01
Fold 3 Epoch 025; Train loss: 0.2700; Time: 00:00:01
Fold 3 Epoch 026; Train loss: 0.2601; Time: 00:00:01
Fold 3 Epoch 027; Train loss: 0.2585; Time: 00:00:02
Fold 3 Epoch 028; Train loss: 0.2558; Time: 00:00:02
Fold 3 Epoch 029; Train loss: 0.2531; Time: 00:00:02
Fold 3 Epoch 030; Train loss: 0.2472; Time: 00:00:01
Fold 3: Evaluation at Epoch 30
HR@5       NDCG@5     HR@10      NDCG@10   
0.637417   0.543140   0.748344   0.577764  
Loss: 0.2776
/home/nit/Desktop/2PDreamRec/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:243: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
Fold 3 Epoch 031; Train loss: 0.2407; Time: 00:00:01
Fold 3 Epoch 032; Train loss: 0.2416; Time: 00:00:02
Fold 3 Epoch 033; Train loss: 0.2377; Time: 00:00:01
Fold 3 Epoch 034; Train loss: 0.2365; Time: 00:00:01
Fold 3 Epoch 035; Train loss: 0.2282; Time: 00:00:01
Fold 3 Epoch 036; Train loss: 0.2336; Time: 00:00:01
Fold 3 Epoch 037; Train loss: 0.2255; Time: 00:00:01
Fold 3 Epoch 038; Train loss: 0.2202; Time: 00:00:01
Fold 3 Epoch 039; Train loss: 0.2208; Time: 00:00:01
Fold 3 Epoch 040; Train loss: 0.2190; Time: 00:00:01
Fold 3: Evaluation at Epoch 40
HR@5       NDCG@5     HR@10      NDCG@10   
0.625828   0.542087   0.751656   0.581720  
Loss: 0.2448
/home/nit/Desktop/2PDreamRec/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:243: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
Fold 3 Epoch 041; Train loss: 0.2164; Time: 00:00:02
Fold 3 Epoch 042; Train loss: 0.2105; Time: 00:00:01
Fold 3 Epoch 043; Train loss: 0.2094; Time: 00:00:01
Fold 3 Epoch 044; Train loss: 0.2130; Time: 00:00:02
Fold 3 Epoch 045; Train loss: 0.2056; Time: 00:00:02
Fold 3 Epoch 046; Train loss: 0.2004; Time: 00:00:01
Fold 3 Epoch 047; Train loss: 0.1980; Time: 00:00:01
Fold 3 Epoch 048; Train loss: 0.1958; Time: 00:00:01
Fold 3 Epoch 049; Train loss: 0.1942; Time: 00:00:02
Fold 3 Epoch 050; Train loss: 0.1939; Time: 00:00:02
Fold 3: Evaluation at Epoch 50
HR@5       NDCG@5     HR@10      NDCG@10   
0.599338   0.499667   0.693709   0.529996  
Loss: 0.2179
/home/nit/Desktop/2PDreamRec/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:243: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
Fold 3 Epoch 051; Train loss: 0.1897; Time: 00:00:01
Fold 3 Epoch 052; Train loss: 0.1875; Time: 00:00:01
Fold 3 Epoch 053; Train loss: 0.1870; Time: 00:00:01
Fold 3 Epoch 054; Train loss: 0.1871; Time: 00:00:01
Fold 3 Epoch 055; Train loss: 0.1795; Time: 00:00:02
Fold 3 Epoch 056; Train loss: 0.1767; Time: 00:00:01
Fold 3 Epoch 057; Train loss: 0.1779; Time: 00:00:01
Fold 3 Epoch 058; Train loss: 0.1708; Time: 00:00:01
Fold 3 Epoch 059; Train loss: 0.1709; Time: 00:00:01
Fold 3 Epoch 060; Train loss: 0.1673; Time: 00:00:01
Fold 3: Evaluation at Epoch 60
HR@5       NDCG@5     HR@10      NDCG@10   
0.612583   0.523986   0.725166   0.559676  
Loss: 0.1869
/home/nit/Desktop/2PDreamRec/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:243: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
Fold 3 Epoch 061; Train loss: 0.1684; Time: 00:00:01
Fold 3 Epoch 062; Train loss: 0.1628; Time: 00:00:02
Fold 3 Epoch 063; Train loss: 0.1623; Time: 00:00:01
Fold 3 Epoch 064; Train loss: 0.1602; Time: 00:00:02
Fold 3 Epoch 065; Train loss: 0.1584; Time: 00:00:02
Fold 3 Epoch 066; Train loss: 0.1583; Time: 00:00:02
Fold 3 Epoch 067; Train loss: 0.1529; Time: 00:00:02
Fold 3 Epoch 068; Train loss: 0.1499; Time: 00:00:02
Fold 3 Epoch 069; Train loss: 0.1507; Time: 00:00:02
Fold 3 Epoch 070; Train loss: 0.1464; Time: 00:00:01
Fold 3: Evaluation at Epoch 70
HR@5       NDCG@5     HR@10      NDCG@10   
0.629139   0.540105   0.735099   0.573861  
Loss: 0.1751
/home/nit/Desktop/2PDreamRec/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:243: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
Fold 3 Epoch 071; Train loss: 0.1451; Time: 00:00:01
Fold 3 Epoch 072; Train loss: 0.1440; Time: 00:00:02
Fold 3 Epoch 073; Train loss: 0.1420; Time: 00:00:01
Fold 3 Epoch 074; Train loss: 0.1395; Time: 00:00:02
Fold 3 Epoch 075; Train loss: 0.1365; Time: 00:00:02
Fold 3 Epoch 076; Train loss: 0.1337; Time: 00:00:01
Fold 3 Epoch 077; Train loss: 0.1334; Time: 00:00:01
Fold 3 Epoch 078; Train loss: 0.1328; Time: 00:00:02
Fold 3 Epoch 079; Train loss: 0.1299; Time: 00:00:01
Fold 3 Epoch 080; Train loss: 0.1287; Time: 00:00:01
Fold 3: Evaluation at Epoch 80
HR@5       NDCG@5     HR@10      NDCG@10   
0.629139   0.528534   0.753311   0.568109  
Loss: 0.1493
/home/nit/Desktop/2PDreamRec/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:243: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
[dropout_rate candidate 0.1] Fold 3: NDCG@10 = [np.float64(0.5389024230352467), np.float64(0.5670707928054528), np.float64(0.5777643857959085), np.float64(0.5817198891157138), np.float64(0.5299961792596026), np.float64(0.5596756069990725), np.float64(0.573860517921658), np.float64(0.5681089574280851)]
Tuning dropout_rate:  43%|████▎     | 3/7 [08:31<11:23, 170.96s/it]
========== Fold 3 ==========
INFO:root:Using statics: seq_size = 10, genre_vocab_size = 18
Fold 3 Epoch 001; Train loss: 0.9637; Time: 00:00:01
Fold 3 Epoch 002; Train loss: 0.7086; Time: 00:00:01
Fold 3 Epoch 003; Train loss: 0.5962; Time: 00:00:01
Fold 3 Epoch 004; Train loss: 0.5226; Time: 00:00:01
Fold 3 Epoch 005; Train loss: 0.4775; Time: 00:00:01
Fold 3 Epoch 006; Train loss: 0.4499; Time: 00:00:01
Fold 3 Epoch 007; Train loss: 0.4181; Time: 00:00:02
Fold 3 Epoch 008; Train loss: 0.3986; Time: 00:00:02
Fold 3 Epoch 009; Train loss: 0.3908; Time: 00:00:02
Fold 3 Epoch 010; Train loss: 0.3723; Time: 00:00:02
Fold 3: Evaluation at Epoch 10
HR@5       NDCG@5     HR@10      NDCG@10   
0.632450   0.500675   0.726821   0.531375  
Loss: 0.4587
/home/nit/Desktop/2PDreamRec/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:243: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
Fold 3 Epoch 011; Train loss: 0.3611; Time: 00:00:01
Fold 3 Epoch 012; Train loss: 0.3534; Time: 00:00:01
Fold 3 Epoch 013; Train loss: 0.3459; Time: 00:00:01
Fold 3 Epoch 014; Train loss: 0.3411; Time: 00:00:01
Fold 3 Epoch 015; Train loss: 0.3315; Time: 00:00:01
Fold 3 Epoch 016; Train loss: 0.3249; Time: 00:00:01
Fold 3 Epoch 017; Train loss: 0.3194; Time: 00:00:02
Fold 3 Epoch 018; Train loss: 0.3165; Time: 00:00:02
Fold 3 Epoch 019; Train loss: 0.3130; Time: 00:00:01
Fold 3 Epoch 020; Train loss: 0.3108; Time: 00:00:02
Fold 3: Evaluation at Epoch 20
HR@5       NDCG@5     HR@10      NDCG@10   
0.644040   0.519698   0.725166   0.545634  
Loss: 0.3890
/home/nit/Desktop/2PDreamRec/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:243: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
Fold 3 Epoch 021; Train loss: 0.2980; Time: 00:00:01
Fold 3 Epoch 022; Train loss: 0.2978; Time: 00:00:01
Fold 3 Epoch 023; Train loss: 0.2905; Time: 00:00:02
Fold 3 Epoch 024; Train loss: 0.2867; Time: 00:00:02
Fold 3 Epoch 025; Train loss: 0.2850; Time: 00:00:02
Fold 3 Epoch 026; Train loss: 0.2825; Time: 00:00:01
Fold 3 Epoch 027; Train loss: 0.2733; Time: 00:00:01
Fold 3 Epoch 028; Train loss: 0.2716; Time: 00:00:01
Fold 3 Epoch 029; Train loss: 0.2691; Time: 00:00:02
Fold 3 Epoch 030; Train loss: 0.2670; Time: 00:00:01
Fold 3: Evaluation at Epoch 30
HR@5       NDCG@5     HR@10      NDCG@10   
0.649007   0.523284   0.730132   0.549568  
Loss: 0.3399
/home/nit/Desktop/2PDreamRec/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:243: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
Fold 3 Epoch 031; Train loss: 0.2623; Time: 00:00:01
Fold 3 Epoch 032; Train loss: 0.2584; Time: 00:00:02
Fold 3 Epoch 033; Train loss: 0.2570; Time: 00:00:02
Fold 3 Epoch 034; Train loss: 0.2548; Time: 00:00:01
Fold 3 Epoch 035; Train loss: 0.2488; Time: 00:00:01
Fold 3 Epoch 036; Train loss: 0.2497; Time: 00:00:02
Fold 3 Epoch 037; Train loss: 0.2452; Time: 00:00:01
Fold 3 Epoch 038; Train loss: 0.2370; Time: 00:00:02
Fold 3 Epoch 039; Train loss: 0.2350; Time: 00:00:02
Fold 3 Epoch 040; Train loss: 0.2317; Time: 00:00:01
Fold 3: Evaluation at Epoch 40
HR@5       NDCG@5     HR@10      NDCG@10   
0.615894   0.511944   0.715232   0.544206  
Loss: 0.3264
/home/nit/Desktop/2PDreamRec/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:243: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
Fold 3 Epoch 041; Train loss: 0.2296; Time: 00:00:02
Fold 3 Epoch 042; Train loss: 0.2284; Time: 00:00:02
Fold 3 Epoch 043; Train loss: 0.2312; Time: 00:00:01
Fold 3 Epoch 044; Train loss: 0.2228; Time: 00:00:01
Fold 3 Epoch 045; Train loss: 0.2174; Time: 00:00:01
Fold 3 Epoch 046; Train loss: 0.2156; Time: 00:00:01
Fold 3 Epoch 047; Train loss: 0.2144; Time: 00:00:02
Fold 3 Epoch 048; Train loss: 0.2125; Time: 00:00:02
Fold 3 Epoch 049; Train loss: 0.2117; Time: 00:00:01
Fold 3 Epoch 050; Train loss: 0.2059; Time: 00:00:01
Fold 3: Evaluation at Epoch 50
HR@5       NDCG@5     HR@10      NDCG@10   
0.632450   0.521816   0.711921   0.547072  
Loss: 0.2737
/home/nit/Desktop/2PDreamRec/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:243: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
Fold 3 Epoch 051; Train loss: 0.2018; Time: 00:00:01
Fold 3 Epoch 052; Train loss: 0.2048; Time: 00:00:01
Fold 3 Epoch 053; Train loss: 0.1997; Time: 00:00:01
Fold 3 Epoch 054; Train loss: 0.1931; Time: 00:00:02
Fold 3 Epoch 055; Train loss: 0.1945; Time: 00:00:01
Fold 3 Epoch 056; Train loss: 0.1913; Time: 00:00:01
Fold 3 Epoch 057; Train loss: 0.1930; Time: 00:00:01
Fold 3 Epoch 058; Train loss: 0.1870; Time: 00:00:02
Fold 3 Epoch 059; Train loss: 0.1894; Time: 00:00:02
Fold 3 Epoch 060; Train loss: 0.1822; Time: 00:00:01
Fold 3: Evaluation at Epoch 60
HR@5       NDCG@5     HR@10      NDCG@10   
0.650662   0.527876   0.740066   0.556700  
Loss: 0.2592
/home/nit/Desktop/2PDreamRec/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:243: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
Fold 3 Epoch 061; Train loss: 0.1794; Time: 00:00:01
Fold 3 Epoch 062; Train loss: 0.1777; Time: 00:00:01
Fold 3 Epoch 063; Train loss: 0.1751; Time: 00:00:02
Fold 3 Epoch 064; Train loss: 0.1719; Time: 00:00:01
Fold 3 Epoch 065; Train loss: 0.1727; Time: 00:00:02
Fold 3 Epoch 066; Train loss: 0.1654; Time: 00:00:02
Fold 3 Epoch 067; Train loss: 0.1635; Time: 00:00:01
Fold 3 Epoch 068; Train loss: 0.1616; Time: 00:00:01
Fold 3 Epoch 069; Train loss: 0.1586; Time: 00:00:02
Fold 3 Epoch 070; Train loss: 0.1563; Time: 00:00:01
Fold 3: Evaluation at Epoch 70
HR@5       NDCG@5     HR@10      NDCG@10   
0.635762   0.514683   0.751656   0.551559  
Loss: 0.2202
/home/nit/Desktop/2PDreamRec/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:243: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
Fold 3 Epoch 071; Train loss: 0.1550; Time: 00:00:01
Fold 3 Epoch 072; Train loss: 0.1569; Time: 00:00:02
Fold 3 Epoch 073; Train loss: 0.1537; Time: 00:00:02
Fold 3 Epoch 074; Train loss: 0.1493; Time: 00:00:02
Fold 3 Epoch 075; Train loss: 0.1482; Time: 00:00:02
Fold 3 Epoch 076; Train loss: 0.1485; Time: 00:00:02
Fold 3 Epoch 077; Train loss: 0.1425; Time: 00:00:02
Fold 3 Epoch 078; Train loss: 0.1444; Time: 00:00:02
Fold 3 Epoch 079; Train loss: 0.1408; Time: 00:00:02
Fold 3 Epoch 080; Train loss: 0.1381; Time: 00:00:02
Fold 3: Evaluation at Epoch 80
HR@5       NDCG@5     HR@10      NDCG@10   
0.663907   0.547372   0.789735   0.587794  
Loss: 0.2082
/home/nit/Desktop/2PDreamRec/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:243: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
[dropout_rate candidate 0.2] Fold 3: NDCG@10 = [np.float64(0.5313745372720475), np.float64(0.5456343923789642), np.float64(0.5495678691245899), np.float64(0.5442059956071423), np.float64(0.547071884266346), np.float64(0.5566996678948346), np.float64(0.5515589565716305), np.float64(0.5877939623563838)]
Tuning dropout_rate:  57%|█████▋    | 4/7 [11:21<08:31, 170.36s/it]
========== Fold 3 ==========
INFO:root:Using statics: seq_size = 10, genre_vocab_size = 18
Fold 3 Epoch 001; Train loss: 0.9866; Time: 00:00:01
Fold 3 Epoch 002; Train loss: 0.7172; Time: 00:00:02
Fold 3 Epoch 003; Train loss: 0.6045; Time: 00:00:02
Fold 3 Epoch 004; Train loss: 0.5319; Time: 00:00:02
Fold 3 Epoch 005; Train loss: 0.4765; Time: 00:00:02
Fold 3 Epoch 006; Train loss: 0.4440; Time: 00:00:01
Fold 3 Epoch 007; Train loss: 0.4253; Time: 00:00:01
Fold 3 Epoch 008; Train loss: 0.4038; Time: 00:00:01
Fold 3 Epoch 009; Train loss: 0.3807; Time: 00:00:01
Fold 3 Epoch 010; Train loss: 0.3685; Time: 00:00:01
Fold 3: Evaluation at Epoch 10
HR@5       NDCG@5     HR@10      NDCG@10   
0.644040   0.495782   0.725166   0.522548  
Loss: 0.4823
/home/nit/Desktop/2PDreamRec/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:243: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
Fold 3 Epoch 011; Train loss: 0.3577; Time: 00:00:01
Fold 3 Epoch 012; Train loss: 0.3517; Time: 00:00:01
Fold 3 Epoch 013; Train loss: 0.3397; Time: 00:00:01
Fold 3 Epoch 014; Train loss: 0.3352; Time: 00:00:02
Fold 3 Epoch 015; Train loss: 0.3229; Time: 00:00:02
Fold 3 Epoch 016; Train loss: 0.3178; Time: 00:00:01
Fold 3 Epoch 017; Train loss: 0.3107; Time: 00:00:01
Fold 3 Epoch 018; Train loss: 0.3055; Time: 00:00:01
Fold 3 Epoch 019; Train loss: 0.3022; Time: 00:00:01
Fold 3 Epoch 020; Train loss: 0.2967; Time: 00:00:01
Fold 3: Evaluation at Epoch 20
HR@5       NDCG@5     HR@10      NDCG@10   
0.602649   0.504549   0.713576   0.539999  
Loss: 0.3936
/home/nit/Desktop/2PDreamRec/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:243: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
Fold 3 Epoch 021; Train loss: 0.2905; Time: 00:00:01
Fold 3 Epoch 022; Train loss: 0.2864; Time: 00:00:02
Fold 3 Epoch 023; Train loss: 0.2903; Time: 00:00:01
Fold 3 Epoch 024; Train loss: 0.2798; Time: 00:00:01
Fold 3 Epoch 025; Train loss: 0.2809; Time: 00:00:01
Fold 3 Epoch 026; Train loss: 0.2779; Time: 00:00:01
Fold 3 Epoch 027; Train loss: 0.2713; Time: 00:00:01
Fold 3 Epoch 028; Train loss: 0.2737; Time: 00:00:01
Fold 3 Epoch 029; Train loss: 0.2640; Time: 00:00:02
Fold 3 Epoch 030; Train loss: 0.2604; Time: 00:00:01
Fold 3: Evaluation at Epoch 30
HR@5       NDCG@5     HR@10      NDCG@10   
0.579470   0.489673   0.688742   0.524548  
Loss: 0.3449
/home/nit/Desktop/2PDreamRec/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:243: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
Fold 3 Epoch 031; Train loss: 0.2549; Time: 00:00:01
Fold 3 Epoch 032; Train loss: 0.2587; Time: 00:00:02
Fold 3 Epoch 033; Train loss: 0.2483; Time: 00:00:02
Fold 3 Epoch 034; Train loss: 0.2480; Time: 00:00:02
Fold 3 Epoch 035; Train loss: 0.2505; Time: 00:00:02
Fold 3 Epoch 036; Train loss: 0.2409; Time: 00:00:01
Fold 3 Epoch 037; Train loss: 0.2401; Time: 00:00:01
Fold 3 Epoch 038; Train loss: 0.2415; Time: 00:00:02
Fold 3 Epoch 039; Train loss: 0.2367; Time: 00:00:02
Fold 3 Epoch 040; Train loss: 0.2346; Time: 00:00:02
Fold 3: Evaluation at Epoch 40
HR@5       NDCG@5     HR@10      NDCG@10   
0.572848   0.495194   0.692053   0.533057  
Loss: 0.2924
/home/nit/Desktop/2PDreamRec/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:243: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
Fold 3 Epoch 041; Train loss: 0.2285; Time: 00:00:01
Fold 3 Epoch 042; Train loss: 0.2288; Time: 00:00:01
Fold 3 Epoch 043; Train loss: 0.2220; Time: 00:00:01
Fold 3 Epoch 044; Train loss: 0.2197; Time: 00:00:01
Fold 3 Epoch 045; Train loss: 0.2173; Time: 00:00:02
Fold 3 Epoch 046; Train loss: 0.2111; Time: 00:00:01
Fold 3 Epoch 047; Train loss: 0.2135; Time: 00:00:02
Fold 3 Epoch 048; Train loss: 0.2097; Time: 00:00:01
Fold 3 Epoch 049; Train loss: 0.2077; Time: 00:00:01
Fold 3 Epoch 050; Train loss: 0.2026; Time: 00:00:01
Fold 3: Evaluation at Epoch 50
HR@5       NDCG@5     HR@10      NDCG@10   
0.587748   0.500849   0.690397   0.534164  
Loss: 0.2813
/home/nit/Desktop/2PDreamRec/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:243: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
Fold 3 Epoch 051; Train loss: 0.1992; Time: 00:00:01
Fold 3 Epoch 052; Train loss: 0.1988; Time: 00:00:02
Fold 3 Epoch 053; Train loss: 0.1973; Time: 00:00:01
Fold 3 Epoch 054; Train loss: 0.1925; Time: 00:00:01
Fold 3 Epoch 055; Train loss: 0.1929; Time: 00:00:01
Fold 3 Epoch 056; Train loss: 0.1869; Time: 00:00:01
Fold 3 Epoch 057; Train loss: 0.1886; Time: 00:00:01
Fold 3 Epoch 058; Train loss: 0.1847; Time: 00:00:02
Fold 3 Epoch 059; Train loss: 0.1816; Time: 00:00:02
Fold 3 Epoch 060; Train loss: 0.1800; Time: 00:00:02
Fold 3: Evaluation at Epoch 60
HR@5       NDCG@5     HR@10      NDCG@10   
0.579470   0.495729   0.685430   0.529716  
Loss: 0.2366
/home/nit/Desktop/2PDreamRec/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:243: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
Fold 3 Epoch 061; Train loss: 0.1806; Time: 00:00:01
Fold 3 Epoch 062; Train loss: 0.1738; Time: 00:00:01
Fold 3 Epoch 063; Train loss: 0.1757; Time: 00:00:01
Fold 3 Epoch 064; Train loss: 0.1741; Time: 00:00:01
Fold 3 Epoch 065; Train loss: 0.1707; Time: 00:00:01
Fold 3 Epoch 066; Train loss: 0.1672; Time: 00:00:01
Fold 3 Epoch 067; Train loss: 0.1649; Time: 00:00:01
Fold 3 Epoch 068; Train loss: 0.1609; Time: 00:00:01
Fold 3 Epoch 069; Train loss: 0.1625; Time: 00:00:01
Fold 3 Epoch 070; Train loss: 0.1580; Time: 00:00:02
Fold 3: Evaluation at Epoch 70
HR@5       NDCG@5     HR@10      NDCG@10   
0.604305   0.501068   0.703642   0.533101  
Loss: 0.2196
/home/nit/Desktop/2PDreamRec/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:243: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
Fold 3 Epoch 071; Train loss: 0.1577; Time: 00:00:01
Fold 3 Epoch 072; Train loss: 0.1541; Time: 00:00:01
Fold 3 Epoch 073; Train loss: 0.1497; Time: 00:00:01
Fold 3 Epoch 074; Train loss: 0.1514; Time: 00:00:01
Fold 3 Epoch 075; Train loss: 0.1479; Time: 00:00:01
Fold 3 Epoch 076; Train loss: 0.1470; Time: 00:00:01
Fold 3 Epoch 077; Train loss: 0.1423; Time: 00:00:02
Fold 3 Epoch 078; Train loss: 0.1448; Time: 00:00:01
Fold 3 Epoch 079; Train loss: 0.1400; Time: 00:00:01
Fold 3 Epoch 080; Train loss: 0.1383; Time: 00:00:01
Fold 3: Evaluation at Epoch 80
HR@5       NDCG@5     HR@10      NDCG@10   
0.617550   0.513447   0.708609   0.542922  
Loss: 0.1928
/home/nit/Desktop/2PDreamRec/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:243: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
[dropout_rate candidate 0.3] Fold 3: NDCG@10 = [np.float64(0.5225477116570788), np.float64(0.5399987689736401), np.float64(0.524547909474624), np.float64(0.5330568604026215), np.float64(0.534164250002542), np.float64(0.529715801904723), np.float64(0.5331005920121136), np.float64(0.5429219153601301)]
Tuning dropout_rate:  71%|███████▏  | 5/7 [14:05<05:36, 168.22s/it]
========== Fold 3 ==========
INFO:root:Using statics: seq_size = 10, genre_vocab_size = 18
Fold 3 Epoch 001; Train loss: 0.9960; Time: 00:00:02
Fold 3 Epoch 002; Train loss: 0.7484; Time: 00:00:02
Fold 3 Epoch 003; Train loss: 0.6265; Time: 00:00:01
Fold 3 Epoch 004; Train loss: 0.5418; Time: 00:00:01
Fold 3 Epoch 005; Train loss: 0.4974; Time: 00:00:01
Fold 3 Epoch 006; Train loss: 0.4557; Time: 00:00:01
Fold 3 Epoch 007; Train loss: 0.4335; Time: 00:00:01
Fold 3 Epoch 008; Train loss: 0.4012; Time: 00:00:02
Fold 3 Epoch 009; Train loss: 0.3928; Time: 00:00:01
Fold 3 Epoch 010; Train loss: 0.3715; Time: 00:00:01
Fold 3: Evaluation at Epoch 10
HR@5       NDCG@5     HR@10      NDCG@10   
0.692053   0.554529   0.793046   0.587682  
Loss: 0.4812
/home/nit/Desktop/2PDreamRec/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:243: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
Fold 3 Epoch 011; Train loss: 0.3644; Time: 00:00:01
Fold 3 Epoch 012; Train loss: 0.3554; Time: 00:00:02
Fold 3 Epoch 013; Train loss: 0.3466; Time: 00:00:02
Fold 3 Epoch 014; Train loss: 0.3421; Time: 00:00:02
Fold 3 Epoch 015; Train loss: 0.3384; Time: 00:00:02
Fold 3 Epoch 016; Train loss: 0.3247; Time: 00:00:02
Fold 3 Epoch 017; Train loss: 0.3233; Time: 00:00:01
Fold 3 Epoch 018; Train loss: 0.3212; Time: 00:00:01
Fold 3 Epoch 019; Train loss: 0.3066; Time: 00:00:01
Fold 3 Epoch 020; Train loss: 0.3099; Time: 00:00:01
Fold 3: Evaluation at Epoch 20
HR@5       NDCG@5     HR@10      NDCG@10   
0.642384   0.529725   0.761589   0.570140  
Loss: 0.4484
/home/nit/Desktop/2PDreamRec/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:243: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
Fold 3 Epoch 021; Train loss: 0.3037; Time: 00:00:01
Fold 3 Epoch 022; Train loss: 0.3036; Time: 00:00:01
Fold 3 Epoch 023; Train loss: 0.2958; Time: 00:00:01
Fold 3 Epoch 024; Train loss: 0.2869; Time: 00:00:01
Fold 3 Epoch 025; Train loss: 0.2904; Time: 00:00:01
Fold 3 Epoch 026; Train loss: 0.2862; Time: 00:00:02
Fold 3 Epoch 027; Train loss: 0.2773; Time: 00:00:01
Fold 3 Epoch 028; Train loss: 0.2780; Time: 00:00:01
Fold 3 Epoch 029; Train loss: 0.2758; Time: 00:00:01
Fold 3 Epoch 030; Train loss: 0.2702; Time: 00:00:01
Fold 3: Evaluation at Epoch 30
HR@5       NDCG@5     HR@10      NDCG@10   
0.660596   0.548685   0.786424   0.590489  
Loss: 0.3737
/home/nit/Desktop/2PDreamRec/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:243: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
Fold 3 Epoch 031; Train loss: 0.2634; Time: 00:00:01
Fold 3 Epoch 032; Train loss: 0.2668; Time: 00:00:01
Fold 3 Epoch 033; Train loss: 0.2627; Time: 00:00:01
Fold 3 Epoch 034; Train loss: 0.2554; Time: 00:00:02
Fold 3 Epoch 035; Train loss: 0.2541; Time: 00:00:02
Fold 3 Epoch 036; Train loss: 0.2528; Time: 00:00:02
Fold 3 Epoch 037; Train loss: 0.2510; Time: 00:00:01
Fold 3 Epoch 038; Train loss: 0.2424; Time: 00:00:01
Fold 3 Epoch 039; Train loss: 0.2441; Time: 00:00:01
Fold 3 Epoch 040; Train loss: 0.2344; Time: 00:00:02
Fold 3: Evaluation at Epoch 40
HR@5       NDCG@5     HR@10      NDCG@10   
0.673841   0.554427   0.788079   0.591750  
Loss: 0.3334
/home/nit/Desktop/2PDreamRec/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:243: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
Fold 3 Epoch 041; Train loss: 0.2322; Time: 00:00:01
Fold 3 Epoch 042; Train loss: 0.2316; Time: 00:00:02
Fold 3 Epoch 043; Train loss: 0.2318; Time: 00:00:02
Fold 3 Epoch 044; Train loss: 0.2313; Time: 00:00:01
Fold 3 Epoch 045; Train loss: 0.2226; Time: 00:00:02
Fold 3 Epoch 046; Train loss: 0.2217; Time: 00:00:02
Fold 3 Epoch 047; Train loss: 0.2182; Time: 00:00:02
Fold 3 Epoch 048; Train loss: 0.2149; Time: 00:00:01
Fold 3 Epoch 049; Train loss: 0.2132; Time: 00:00:01
Fold 3 Epoch 050; Train loss: 0.2118; Time: 00:00:02
Fold 3: Evaluation at Epoch 50
HR@5       NDCG@5     HR@10      NDCG@10   
0.673841   0.552164   0.778146   0.586321  
Loss: 0.2937
/home/nit/Desktop/2PDreamRec/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:243: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
Fold 3 Epoch 051; Train loss: 0.2066; Time: 00:00:01
Fold 3 Epoch 052; Train loss: 0.2044; Time: 00:00:01
Fold 3 Epoch 053; Train loss: 0.2032; Time: 00:00:01
Fold 3 Epoch 054; Train loss: 0.2006; Time: 00:00:01
Fold 3 Epoch 055; Train loss: 0.1931; Time: 00:00:01
Fold 3 Epoch 056; Train loss: 0.1926; Time: 00:00:02
Fold 3 Epoch 057; Train loss: 0.1901; Time: 00:00:02
Fold 3 Epoch 058; Train loss: 0.1899; Time: 00:00:01
Fold 3 Epoch 059; Train loss: 0.1856; Time: 00:00:02
Fold 3 Epoch 060; Train loss: 0.1876; Time: 00:00:01
Fold 3: Evaluation at Epoch 60
HR@5       NDCG@5     HR@10      NDCG@10   
0.675497   0.552500   0.786424   0.589093  
Loss: 0.2731
/home/nit/Desktop/2PDreamRec/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:243: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
Fold 3 Epoch 061; Train loss: 0.1820; Time: 00:00:02
Fold 3 Epoch 062; Train loss: 0.1831; Time: 00:00:02
Fold 3 Epoch 063; Train loss: 0.1775; Time: 00:00:01
Fold 3 Epoch 064; Train loss: 0.1784; Time: 00:00:01
Fold 3 Epoch 065; Train loss: 0.1744; Time: 00:00:02
Fold 3 Epoch 066; Train loss: 0.1713; Time: 00:00:01
Fold 3 Epoch 067; Train loss: 0.1712; Time: 00:00:02
Fold 3 Epoch 068; Train loss: 0.1648; Time: 00:00:02
Fold 3 Epoch 069; Train loss: 0.1695; Time: 00:00:02
Fold 3 Epoch 070; Train loss: 0.1620; Time: 00:00:02
Fold 3: Evaluation at Epoch 70
HR@5       NDCG@5     HR@10      NDCG@10   
0.682119   0.560407   0.773179   0.590266  
Loss: 0.2215
/home/nit/Desktop/2PDreamRec/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:243: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
Fold 3 Epoch 071; Train loss: 0.1619; Time: 00:00:01
Fold 3 Epoch 072; Train loss: 0.1604; Time: 00:00:01
Fold 3 Epoch 073; Train loss: 0.1567; Time: 00:00:01
Fold 3 Epoch 074; Train loss: 0.1561; Time: 00:00:02
Fold 3 Epoch 075; Train loss: 0.1502; Time: 00:00:02
Fold 3 Epoch 076; Train loss: 0.1530; Time: 00:00:02
Fold 3 Epoch 077; Train loss: 0.1479; Time: 00:00:01
Fold 3 Epoch 078; Train loss: 0.1457; Time: 00:00:02
Fold 3 Epoch 079; Train loss: 0.1468; Time: 00:00:02
Fold 3 Epoch 080; Train loss: 0.1428; Time: 00:00:01
Fold 3: Evaluation at Epoch 80
HR@5       NDCG@5     HR@10      NDCG@10   
0.670530   0.545328   0.779801   0.581133  
Loss: 0.2086
/home/nit/Desktop/2PDreamRec/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:243: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
[dropout_rate candidate 0.4] Fold 3: NDCG@10 = [np.float64(0.5876821164612644), np.float64(0.5701399211532024), np.float64(0.5904887457583982), np.float64(0.591750202216208), np.float64(0.5863209531893859), np.float64(0.5890929484800194), np.float64(0.5902663551679982), np.float64(0.5811330002945193)]
Tuning dropout_rate:  86%|████████▌ | 6/7 [16:53<02:47, 167.98s/it]
========== Fold 3 ==========
INFO:root:Using statics: seq_size = 10, genre_vocab_size = 18
Fold 3 Epoch 001; Train loss: 1.0021; Time: 00:00:01
Fold 3 Epoch 002; Train loss: 0.7465; Time: 00:00:02
Fold 3 Epoch 003; Train loss: 0.6252; Time: 00:00:01
Fold 3 Epoch 004; Train loss: 0.5469; Time: 00:00:01
Fold 3 Epoch 005; Train loss: 0.4922; Time: 00:00:01
Fold 3 Epoch 006; Train loss: 0.4577; Time: 00:00:01
Fold 3 Epoch 007; Train loss: 0.4341; Time: 00:00:01
Fold 3 Epoch 008; Train loss: 0.4156; Time: 00:00:01
Fold 3 Epoch 009; Train loss: 0.4019; Time: 00:00:01
Fold 3 Epoch 010; Train loss: 0.3839; Time: 00:00:01
Fold 3: Evaluation at Epoch 10
HR@5       NDCG@5     HR@10      NDCG@10   
0.531457   0.414032   0.647351   0.451586  
Loss: 0.4500
/home/nit/Desktop/2PDreamRec/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:243: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
Fold 3 Epoch 011; Train loss: 0.3749; Time: 00:00:01
Fold 3 Epoch 012; Train loss: 0.3655; Time: 00:00:01
Fold 3 Epoch 013; Train loss: 0.3573; Time: 00:00:02
Fold 3 Epoch 014; Train loss: 0.3414; Time: 00:00:01
Fold 3 Epoch 015; Train loss: 0.3411; Time: 00:00:01
Fold 3 Epoch 016; Train loss: 0.3289; Time: 00:00:02
Fold 3 Epoch 017; Train loss: 0.3257; Time: 00:00:02
Fold 3 Epoch 018; Train loss: 0.3184; Time: 00:00:02
Fold 3 Epoch 019; Train loss: 0.3156; Time: 00:00:02
Fold 3 Epoch 020; Train loss: 0.3125; Time: 00:00:02
Fold 3: Evaluation at Epoch 20
HR@5       NDCG@5     HR@10      NDCG@10   
0.561258   0.415561   0.688742   0.456170  
Loss: 0.3848
/home/nit/Desktop/2PDreamRec/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:243: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
Fold 3 Epoch 021; Train loss: 0.3056; Time: 00:00:01
Fold 3 Epoch 022; Train loss: 0.3045; Time: 00:00:02
Fold 3 Epoch 023; Train loss: 0.2940; Time: 00:00:02
Fold 3 Epoch 024; Train loss: 0.2955; Time: 00:00:01
Fold 3 Epoch 025; Train loss: 0.2896; Time: 00:00:02
Fold 3 Epoch 026; Train loss: 0.2863; Time: 00:00:01
Fold 3 Epoch 027; Train loss: 0.2790; Time: 00:00:01
Fold 3 Epoch 028; Train loss: 0.2839; Time: 00:00:02
Fold 3 Epoch 029; Train loss: 0.2793; Time: 00:00:01
Fold 3 Epoch 030; Train loss: 0.2758; Time: 00:00:01
Fold 3: Evaluation at Epoch 30
HR@5       NDCG@5     HR@10      NDCG@10   
0.557947   0.427989   0.677152   0.465978  
Loss: 0.3254
/home/nit/Desktop/2PDreamRec/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:243: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
Fold 3 Epoch 031; Train loss: 0.2715; Time: 00:00:01
Fold 3 Epoch 032; Train loss: 0.2673; Time: 00:00:01
Fold 3 Epoch 033; Train loss: 0.2627; Time: 00:00:01
Fold 3 Epoch 034; Train loss: 0.2626; Time: 00:00:01
Fold 3 Epoch 035; Train loss: 0.2570; Time: 00:00:01
Fold 3 Epoch 036; Train loss: 0.2545; Time: 00:00:02
Fold 3 Epoch 037; Train loss: 0.2513; Time: 00:00:02
Fold 3 Epoch 038; Train loss: 0.2465; Time: 00:00:01
Fold 3 Epoch 039; Train loss: 0.2435; Time: 00:00:01
Fold 3 Epoch 040; Train loss: 0.2377; Time: 00:00:02
Fold 3: Evaluation at Epoch 40
HR@5       NDCG@5     HR@10      NDCG@10   
0.569536   0.443605   0.672185   0.476182  
Loss: 0.2903
/home/nit/Desktop/2PDreamRec/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:243: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
Fold 3 Epoch 041; Train loss: 0.2344; Time: 00:00:01
Fold 3 Epoch 042; Train loss: 0.2372; Time: 00:00:01
Fold 3 Epoch 043; Train loss: 0.2302; Time: 00:00:02
Fold 3 Epoch 044; Train loss: 0.2278; Time: 00:00:02
Fold 3 Epoch 045; Train loss: 0.2247; Time: 00:00:01
Fold 3 Epoch 046; Train loss: 0.2243; Time: 00:00:01
Fold 3 Epoch 047; Train loss: 0.2221; Time: 00:00:02
Fold 3 Epoch 048; Train loss: 0.2160; Time: 00:00:01
Fold 3 Epoch 049; Train loss: 0.2163; Time: 00:00:01
Fold 3 Epoch 050; Train loss: 0.2186; Time: 00:00:01
Fold 3: Evaluation at Epoch 50
HR@5       NDCG@5     HR@10      NDCG@10   
0.552980   0.436719   0.668874   0.473914  
Loss: 0.2634
/home/nit/Desktop/2PDreamRec/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:243: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
Fold 3 Epoch 051; Train loss: 0.2079; Time: 00:00:01
Fold 3 Epoch 052; Train loss: 0.2039; Time: 00:00:01
Fold 3 Epoch 053; Train loss: 0.2033; Time: 00:00:01
Fold 3 Epoch 054; Train loss: 0.2028; Time: 00:00:01
Fold 3 Epoch 055; Train loss: 0.1972; Time: 00:00:01
Fold 3 Epoch 056; Train loss: 0.1957; Time: 00:00:01
Fold 3 Epoch 057; Train loss: 0.1936; Time: 00:00:01
Fold 3 Epoch 058; Train loss: 0.1880; Time: 00:00:01
Fold 3 Epoch 059; Train loss: 0.1908; Time: 00:00:02
Fold 3 Epoch 060; Train loss: 0.1898; Time: 00:00:01
Fold 3: Evaluation at Epoch 60
HR@5       NDCG@5     HR@10      NDCG@10   
0.554636   0.432474   0.680464   0.472732  
Loss: 0.2152
/home/nit/Desktop/2PDreamRec/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:243: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
Fold 3 Epoch 061; Train loss: 0.1840; Time: 00:00:01
Fold 3 Epoch 062; Train loss: 0.1823; Time: 00:00:02
Fold 3 Epoch 063; Train loss: 0.1816; Time: 00:00:02
Fold 3 Epoch 064; Train loss: 0.1761; Time: 00:00:01
Fold 3 Epoch 065; Train loss: 0.1750; Time: 00:00:01
Fold 3 Epoch 066; Train loss: 0.1730; Time: 00:00:01
Fold 3 Epoch 067; Train loss: 0.1696; Time: 00:00:01
Fold 3 Epoch 068; Train loss: 0.1668; Time: 00:00:01
Fold 3 Epoch 069; Train loss: 0.1658; Time: 00:00:01
Fold 3 Epoch 070; Train loss: 0.1627; Time: 00:00:01
Fold 3: Evaluation at Epoch 70
HR@5       NDCG@5     HR@10      NDCG@10   
0.561258   0.423919   0.673841   0.459869  
Loss: 0.1909
/home/nit/Desktop/2PDreamRec/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:243: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
Fold 3 Epoch 071; Train loss: 0.1584; Time: 00:00:01
Fold 3 Epoch 072; Train loss: 0.1584; Time: 00:00:01
Fold 3 Epoch 073; Train loss: 0.1562; Time: 00:00:02
Fold 3 Epoch 074; Train loss: 0.1566; Time: 00:00:02
Fold 3 Epoch 075; Train loss: 0.1500; Time: 00:00:01
Fold 3 Epoch 076; Train loss: 0.1475; Time: 00:00:01
Fold 3 Epoch 077; Train loss: 0.1440; Time: 00:00:02
Fold 3 Epoch 078; Train loss: 0.1445; Time: 00:00:01
Fold 3 Epoch 079; Train loss: 0.1445; Time: 00:00:02
Fold 3 Epoch 080; Train loss: 0.1397; Time: 00:00:02
Fold 3: Evaluation at Epoch 80
HR@5       NDCG@5     HR@10      NDCG@10   
0.566225   0.428855   0.682119   0.466160  
Loss: 0.1783
/home/nit/Desktop/2PDreamRec/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:243: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
[dropout_rate candidate 0.5] Fold 3: NDCG@10 = [np.float64(0.4515864174181106), np.float64(0.456170127728597), np.float64(0.46597846784007985), np.float64(0.47618218369796544), np.float64(0.4739143158152849), np.float64(0.47273170615932897), np.float64(0.4598686543229408), np.float64(0.46616012729190354)]
Tuning dropout_rate: 100%|██████████| 7/7 [19:38<00:00, 167.04s/it]Tuning dropout_rate: 100%|██████████| 7/7 [19:38<00:00, 168.35s/it]

Best dropout_rate found: 0.05
Tuning l2_decay:   0%|          | 0/5 [00:00<?, ?it/s]
========== Fold 3 ==========
INFO:root:Using statics: seq_size = 10, genre_vocab_size = 18
Fold 3 Epoch 001; Train loss: 1.0361; Time: 00:00:02
Fold 3 Epoch 002; Train loss: 0.7945; Time: 00:00:01
Fold 3 Epoch 003; Train loss: 0.6554; Time: 00:00:01
Fold 3 Epoch 004; Train loss: 0.5773; Time: 00:00:02
Fold 3 Epoch 005; Train loss: 0.5296; Time: 00:00:01
Fold 3 Epoch 006; Train loss: 0.4967; Time: 00:00:01
Fold 3 Epoch 007; Train loss: 0.4629; Time: 00:00:02
Fold 3 Epoch 008; Train loss: 0.4421; Time: 00:00:02
Fold 3 Epoch 009; Train loss: 0.4292; Time: 00:00:02
Fold 3 Epoch 010; Train loss: 0.4173; Time: 00:00:02
Fold 3: Evaluation at Epoch 10
HR@5       NDCG@5     HR@10      NDCG@10   
0.594371   0.406874   0.759934   0.457492  
Loss: 0.5284
/home/nit/Desktop/2PDreamRec/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:243: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
Fold 3 Epoch 011; Train loss: 0.4065; Time: 00:00:01
Fold 3 Epoch 012; Train loss: 0.3961; Time: 00:00:01
Fold 3 Epoch 013; Train loss: 0.3872; Time: 00:00:01
Fold 3 Epoch 014; Train loss: 0.3801; Time: 00:00:01
Fold 3 Epoch 015; Train loss: 0.3731; Time: 00:00:02
Fold 3 Epoch 016; Train loss: 0.3642; Time: 00:00:02
Fold 3 Epoch 017; Train loss: 0.3608; Time: 00:00:01
Fold 3 Epoch 018; Train loss: 0.3521; Time: 00:00:01
Fold 3 Epoch 019; Train loss: 0.3477; Time: 00:00:01
Fold 3 Epoch 020; Train loss: 0.3494; Time: 00:00:02
Fold 3: Evaluation at Epoch 20
HR@5       NDCG@5     HR@10      NDCG@10   
0.566225   0.380883   0.756623   0.440947  
Loss: 0.4527
/home/nit/Desktop/2PDreamRec/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:243: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
Fold 3 Epoch 021; Train loss: 0.3404; Time: 00:00:01
Fold 3 Epoch 022; Train loss: 0.3329; Time: 00:00:01
Fold 3 Epoch 023; Train loss: 0.3275; Time: 00:00:01
Fold 3 Epoch 024; Train loss: 0.3239; Time: 00:00:01
Fold 3 Epoch 025; Train loss: 0.3212; Time: 00:00:01
Fold 3 Epoch 026; Train loss: 0.3122; Time: 00:00:01
Fold 3 Epoch 027; Train loss: 0.3108; Time: 00:00:02
Fold 3 Epoch 028; Train loss: 0.3056; Time: 00:00:01
Fold 3 Epoch 029; Train loss: 0.3041; Time: 00:00:01
Fold 3 Epoch 030; Train loss: 0.3006; Time: 00:00:02
Fold 3: Evaluation at Epoch 30
HR@5       NDCG@5     HR@10      NDCG@10   
0.579470   0.386983   0.771523   0.448495  
Loss: 0.3865
/home/nit/Desktop/2PDreamRec/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:243: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
Fold 3 Epoch 031; Train loss: 0.2932; Time: 00:00:01
Fold 3 Epoch 032; Train loss: 0.2899; Time: 00:00:01
Fold 3 Epoch 033; Train loss: 0.2854; Time: 00:00:02
Fold 3 Epoch 034; Train loss: 0.2802; Time: 00:00:02
Fold 3 Epoch 035; Train loss: 0.2791; Time: 00:00:01
Fold 3 Epoch 036; Train loss: 0.2761; Time: 00:00:01
Fold 3 Epoch 037; Train loss: 0.2701; Time: 00:00:02
Fold 3 Epoch 038; Train loss: 0.2667; Time: 00:00:01
Fold 3 Epoch 039; Train loss: 0.2622; Time: 00:00:01
Fold 3 Epoch 040; Train loss: 0.2602; Time: 00:00:01
Fold 3: Evaluation at Epoch 40
HR@5       NDCG@5     HR@10      NDCG@10   
0.579470   0.378796   0.774834   0.444322  
Loss: 0.3306
/home/nit/Desktop/2PDreamRec/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:243: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
Fold 3 Epoch 041; Train loss: 0.2582; Time: 00:00:02
Fold 3 Epoch 042; Train loss: 0.2553; Time: 00:00:02
Fold 3 Epoch 043; Train loss: 0.2503; Time: 00:00:01
Fold 3 Epoch 044; Train loss: 0.2479; Time: 00:00:02
Fold 3 Epoch 045; Train loss: 0.2437; Time: 00:00:01
Fold 3 Epoch 046; Train loss: 0.2423; Time: 00:00:02
Fold 3 Epoch 047; Train loss: 0.2397; Time: 00:00:01
Fold 3 Epoch 048; Train loss: 0.2380; Time: 00:00:01
Fold 3 Epoch 049; Train loss: 0.2300; Time: 00:00:01
Fold 3 Epoch 050; Train loss: 0.2297; Time: 00:00:02
Fold 3: Evaluation at Epoch 50
HR@5       NDCG@5     HR@10      NDCG@10   
0.438742   0.307829   0.751656   0.412090  
Loss: 0.2879
/home/nit/Desktop/2PDreamRec/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:243: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
Fold 3 Epoch 051; Train loss: 0.2260; Time: 00:00:01
Fold 3 Epoch 052; Train loss: 0.2231; Time: 00:00:01
Fold 3 Epoch 053; Train loss: 0.2200; Time: 00:00:02
Fold 3 Epoch 054; Train loss: 0.2169; Time: 00:00:02
Fold 3 Epoch 055; Train loss: 0.2149; Time: 00:00:01
Fold 3 Epoch 056; Train loss: 0.2136; Time: 00:00:01
Fold 3 Epoch 057; Train loss: 0.2081; Time: 00:00:01
Fold 3 Epoch 058; Train loss: 0.2049; Time: 00:00:01
Fold 3 Epoch 059; Train loss: 0.2030; Time: 00:00:01
Fold 3 Epoch 060; Train loss: 0.2005; Time: 00:00:02
Fold 3: Evaluation at Epoch 60
HR@5       NDCG@5     HR@10      NDCG@10   
0.341060   0.266017   0.766556   0.406393  
Loss: 0.2458
/home/nit/Desktop/2PDreamRec/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:243: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
Fold 3 Epoch 061; Train loss: 0.1968; Time: 00:00:02
Fold 3 Epoch 062; Train loss: 0.1947; Time: 00:00:02
Fold 3 Epoch 063; Train loss: 0.1929; Time: 00:00:01
Fold 3 Epoch 064; Train loss: 0.1910; Time: 00:00:01
Fold 3 Epoch 065; Train loss: 0.1883; Time: 00:00:01
Fold 3 Epoch 066; Train loss: 0.1830; Time: 00:00:01
Fold 3 Epoch 067; Train loss: 0.1833; Time: 00:00:01
Fold 3 Epoch 068; Train loss: 0.1802; Time: 00:00:01
Fold 3 Epoch 069; Train loss: 0.1771; Time: 00:00:02
Fold 3 Epoch 070; Train loss: 0.1754; Time: 00:00:02
Fold 3: Evaluation at Epoch 70
HR@5       NDCG@5     HR@10      NDCG@10   
0.355960   0.270225   0.779801   0.410128  
Loss: 0.2116
/home/nit/Desktop/2PDreamRec/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:243: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
Fold 3 Epoch 071; Train loss: 0.1723; Time: 00:00:01
Fold 3 Epoch 072; Train loss: 0.1700; Time: 00:00:01
Fold 3 Epoch 073; Train loss: 0.1685; Time: 00:00:01
Fold 3 Epoch 074; Train loss: 0.1655; Time: 00:00:02
Fold 3 Epoch 075; Train loss: 0.1635; Time: 00:00:02
Fold 3 Epoch 076; Train loss: 0.1619; Time: 00:00:02
Fold 3 Epoch 077; Train loss: 0.1598; Time: 00:00:01
Fold 3 Epoch 078; Train loss: 0.1574; Time: 00:00:01
Fold 3 Epoch 079; Train loss: 0.1558; Time: 00:00:01
Fold 3 Epoch 080; Train loss: 0.1528; Time: 00:00:01
Fold 3: Evaluation at Epoch 80
HR@5       NDCG@5     HR@10      NDCG@10   
0.347682   0.267456   0.647351   0.366791  
Loss: 0.1801
/home/nit/Desktop/2PDreamRec/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:243: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
[l2_decay candidate 0.1] Fold 3: NDCG@10 = [np.float64(0.4574923384186404), np.float64(0.4409470282178216), np.float64(0.4484947347099021), np.float64(0.4443216136397711), np.float64(0.41208966681088804), np.float64(0.4063929898294761), np.float64(0.4101280689191687), np.float64(0.3667907620192419)]
Tuning l2_decay:  20%|██        | 1/5 [02:49<11:17, 169.49s/it]
========== Fold 3 ==========
INFO:root:Using statics: seq_size = 10, genre_vocab_size = 18
Fold 3 Epoch 001; Train loss: 1.0106; Time: 00:00:01
Fold 3 Epoch 002; Train loss: 0.7079; Time: 00:00:01
Fold 3 Epoch 003; Train loss: 0.5888; Time: 00:00:01
Fold 3 Epoch 004; Train loss: 0.5103; Time: 00:00:02
Fold 3 Epoch 005; Train loss: 0.4664; Time: 00:00:02
Fold 3 Epoch 006; Train loss: 0.4264; Time: 00:00:01
Fold 3 Epoch 007; Train loss: 0.4020; Time: 00:00:01
Fold 3 Epoch 008; Train loss: 0.3890; Time: 00:00:02
Fold 3 Epoch 009; Train loss: 0.3744; Time: 00:00:01
Fold 3 Epoch 010; Train loss: 0.3525; Time: 00:00:02
Fold 3: Evaluation at Epoch 10
HR@5       NDCG@5     HR@10      NDCG@10   
0.624172   0.465204   0.741722   0.502398  
Loss: 0.4497
/home/nit/Desktop/2PDreamRec/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:243: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
Fold 3 Epoch 011; Train loss: 0.3435; Time: 00:00:01
Fold 3 Epoch 012; Train loss: 0.3346; Time: 00:00:01
Fold 3 Epoch 013; Train loss: 0.3275; Time: 00:00:01
Fold 3 Epoch 014; Train loss: 0.3206; Time: 00:00:01
Fold 3 Epoch 015; Train loss: 0.3099; Time: 00:00:01
Fold 3 Epoch 016; Train loss: 0.3057; Time: 00:00:01
Fold 3 Epoch 017; Train loss: 0.2960; Time: 00:00:01
Fold 3 Epoch 018; Train loss: 0.2974; Time: 00:00:01
Fold 3 Epoch 019; Train loss: 0.2877; Time: 00:00:02
Fold 3 Epoch 020; Train loss: 0.2889; Time: 00:00:01
Fold 3: Evaluation at Epoch 20
HR@5       NDCG@5     HR@10      NDCG@10   
0.609272   0.465006   0.716887   0.498944  
Loss: 0.3775
/home/nit/Desktop/2PDreamRec/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:243: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
Fold 3 Epoch 021; Train loss: 0.2839; Time: 00:00:01
Fold 3 Epoch 022; Train loss: 0.2814; Time: 00:00:02
Fold 3 Epoch 023; Train loss: 0.2759; Time: 00:00:01
Fold 3 Epoch 024; Train loss: 0.2708; Time: 00:00:01
Fold 3 Epoch 025; Train loss: 0.2645; Time: 00:00:01
Fold 3 Epoch 026; Train loss: 0.2633; Time: 00:00:01
Fold 3 Epoch 027; Train loss: 0.2577; Time: 00:00:02
Fold 3 Epoch 028; Train loss: 0.2554; Time: 00:00:01
Fold 3 Epoch 029; Train loss: 0.2492; Time: 00:00:01
Fold 3 Epoch 030; Train loss: 0.2509; Time: 00:00:01
Fold 3: Evaluation at Epoch 30
HR@5       NDCG@5     HR@10      NDCG@10   
0.627483   0.505874   0.743377   0.543089  
Loss: 0.3145
/home/nit/Desktop/2PDreamRec/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:243: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
Fold 3 Epoch 031; Train loss: 0.2452; Time: 00:00:01
Fold 3 Epoch 032; Train loss: 0.2382; Time: 00:00:01
Fold 3 Epoch 033; Train loss: 0.2333; Time: 00:00:02
Fold 3 Epoch 034; Train loss: 0.2320; Time: 00:00:02
Fold 3 Epoch 035; Train loss: 0.2303; Time: 00:00:02
Fold 3 Epoch 036; Train loss: 0.2278; Time: 00:00:02
Fold 3 Epoch 037; Train loss: 0.2275; Time: 00:00:01
Fold 3 Epoch 038; Train loss: 0.2210; Time: 00:00:01
Fold 3 Epoch 039; Train loss: 0.2174; Time: 00:00:01
Fold 3 Epoch 040; Train loss: 0.2142; Time: 00:00:02
Fold 3: Evaluation at Epoch 40
HR@5       NDCG@5     HR@10      NDCG@10   
0.591060   0.466304   0.697020   0.500295  
Loss: 0.2849
/home/nit/Desktop/2PDreamRec/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:243: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
Fold 3 Epoch 041; Train loss: 0.2133; Time: 00:00:02
Fold 3 Epoch 042; Train loss: 0.2096; Time: 00:00:02
Fold 3 Epoch 043; Train loss: 0.2035; Time: 00:00:01
Fold 3 Epoch 044; Train loss: 0.2030; Time: 00:00:01
Fold 3 Epoch 045; Train loss: 0.1979; Time: 00:00:02
Fold 3 Epoch 046; Train loss: 0.1996; Time: 00:00:01
Fold 3 Epoch 047; Train loss: 0.1956; Time: 00:00:01
Fold 3 Epoch 048; Train loss: 0.1947; Time: 00:00:02
Fold 3 Epoch 049; Train loss: 0.1911; Time: 00:00:01
Fold 3 Epoch 050; Train loss: 0.1857; Time: 00:00:01
Fold 3: Evaluation at Epoch 50
HR@5       NDCG@5     HR@10      NDCG@10   
0.615894   0.474364   0.706954   0.503076  
Loss: 0.2589
/home/nit/Desktop/2PDreamRec/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:243: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
Fold 3 Epoch 051; Train loss: 0.1806; Time: 00:00:01
Fold 3 Epoch 052; Train loss: 0.1812; Time: 00:00:01
Fold 3 Epoch 053; Train loss: 0.1787; Time: 00:00:01
Fold 3 Epoch 054; Train loss: 0.1803; Time: 00:00:02
Fold 3 Epoch 055; Train loss: 0.1751; Time: 00:00:02
Fold 3 Epoch 056; Train loss: 0.1711; Time: 00:00:01
Fold 3 Epoch 057; Train loss: 0.1716; Time: 00:00:02
Fold 3 Epoch 058; Train loss: 0.1671; Time: 00:00:01
Fold 3 Epoch 059; Train loss: 0.1662; Time: 00:00:02
Fold 3 Epoch 060; Train loss: 0.1630; Time: 00:00:01
Fold 3: Evaluation at Epoch 60
HR@5       NDCG@5     HR@10      NDCG@10   
0.571192   0.447684   0.706954   0.492083  
Loss: 0.2173
/home/nit/Desktop/2PDreamRec/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:243: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
Fold 3 Epoch 061; Train loss: 0.1598; Time: 00:00:01
Fold 3 Epoch 062; Train loss: 0.1573; Time: 00:00:01
Fold 3 Epoch 063; Train loss: 0.1562; Time: 00:00:01
Fold 3 Epoch 064; Train loss: 0.1544; Time: 00:00:01
Fold 3 Epoch 065; Train loss: 0.1524; Time: 00:00:02
Fold 3 Epoch 066; Train loss: 0.1507; Time: 00:00:01
Fold 3 Epoch 067; Train loss: 0.1489; Time: 00:00:01
Fold 3 Epoch 068; Train loss: 0.1460; Time: 00:00:01
Fold 3 Epoch 069; Train loss: 0.1451; Time: 00:00:01
Fold 3 Epoch 070; Train loss: 0.1404; Time: 00:00:02
Fold 3: Evaluation at Epoch 70
HR@5       NDCG@5     HR@10      NDCG@10   
0.572848   0.450748   0.710265   0.495782  
Loss: 0.1895
/home/nit/Desktop/2PDreamRec/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:243: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
Fold 3 Epoch 071; Train loss: 0.1378; Time: 00:00:01
Fold 3 Epoch 072; Train loss: 0.1378; Time: 00:00:01
Fold 3 Epoch 073; Train loss: 0.1342; Time: 00:00:01
Fold 3 Epoch 074; Train loss: 0.1325; Time: 00:00:01
Fold 3 Epoch 075; Train loss: 0.1348; Time: 00:00:02
Fold 3 Epoch 076; Train loss: 0.1323; Time: 00:00:01
Fold 3 Epoch 077; Train loss: 0.1292; Time: 00:00:01
Fold 3 Epoch 078; Train loss: 0.1275; Time: 00:00:02
Fold 3 Epoch 079; Train loss: 0.1255; Time: 00:00:02
Fold 3 Epoch 080; Train loss: 0.1254; Time: 00:00:01
Fold 3: Evaluation at Epoch 80
HR@5       NDCG@5     HR@10      NDCG@10   
0.596026   0.460339   0.735099   0.507010  
Loss: 0.1654
/home/nit/Desktop/2PDreamRec/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:243: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
[l2_decay candidate 0.01] Fold 3: NDCG@10 = [np.float64(0.5023978194138252), np.float64(0.4989443987187964), np.float64(0.5430888180949061), np.float64(0.5002946860073098), np.float64(0.5030762616309139), np.float64(0.4920831803222039), np.float64(0.4957820755853673), np.float64(0.5070096671152838)]
Tuning l2_decay:  40%|████      | 2/5 [05:33<08:18, 166.23s/it]
========== Fold 3 ==========
INFO:root:Using statics: seq_size = 10, genre_vocab_size = 18
Fold 3 Epoch 001; Train loss: 0.9686; Time: 00:00:02
Fold 3 Epoch 002; Train loss: 0.7130; Time: 00:00:02
Fold 3 Epoch 003; Train loss: 0.5928; Time: 00:00:01
Fold 3 Epoch 004; Train loss: 0.5218; Time: 00:00:02
Fold 3 Epoch 005; Train loss: 0.4747; Time: 00:00:01
Fold 3 Epoch 006; Train loss: 0.4432; Time: 00:00:02
Fold 3 Epoch 007; Train loss: 0.4153; Time: 00:00:01
Fold 3 Epoch 008; Train loss: 0.3907; Time: 00:00:01
Fold 3 Epoch 009; Train loss: 0.3869; Time: 00:00:01
Fold 3 Epoch 010; Train loss: 0.3719; Time: 00:00:02
Fold 3: Evaluation at Epoch 10
HR@5       NDCG@5     HR@10      NDCG@10   
0.637417   0.460985   0.730132   0.490868  
Loss: 0.4506
/home/nit/Desktop/2PDreamRec/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:243: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
Fold 3 Epoch 011; Train loss: 0.3573; Time: 00:00:02
Fold 3 Epoch 012; Train loss: 0.3429; Time: 00:00:02
Fold 3 Epoch 013; Train loss: 0.3379; Time: 00:00:02
Fold 3 Epoch 014; Train loss: 0.3303; Time: 00:00:01
Fold 3 Epoch 015; Train loss: 0.3248; Time: 00:00:01
Fold 3 Epoch 016; Train loss: 0.3191; Time: 00:00:02
Fold 3 Epoch 017; Train loss: 0.3144; Time: 00:00:02
Fold 3 Epoch 018; Train loss: 0.3092; Time: 00:00:01
Fold 3 Epoch 019; Train loss: 0.3023; Time: 00:00:01
Fold 3 Epoch 020; Train loss: 0.2967; Time: 00:00:01
Fold 3: Evaluation at Epoch 20
HR@5       NDCG@5     HR@10      NDCG@10   
0.665563   0.499183   0.771523   0.533585  
Loss: 0.3523
/home/nit/Desktop/2PDreamRec/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:243: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
Fold 3 Epoch 021; Train loss: 0.2992; Time: 00:00:02
Fold 3 Epoch 022; Train loss: 0.2907; Time: 00:00:01
Fold 3 Epoch 023; Train loss: 0.2864; Time: 00:00:01
Fold 3 Epoch 024; Train loss: 0.2833; Time: 00:00:01
Fold 3 Epoch 025; Train loss: 0.2819; Time: 00:00:01
Fold 3 Epoch 026; Train loss: 0.2772; Time: 00:00:01
Fold 3 Epoch 027; Train loss: 0.2674; Time: 00:00:01
Fold 3 Epoch 028; Train loss: 0.2631; Time: 00:00:02
Fold 3 Epoch 029; Train loss: 0.2651; Time: 00:00:02
Fold 3 Epoch 030; Train loss: 0.2612; Time: 00:00:01
Fold 3: Evaluation at Epoch 30
HR@5       NDCG@5     HR@10      NDCG@10   
0.642384   0.493400   0.746689   0.527773  
Loss: 0.3238
/home/nit/Desktop/2PDreamRec/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:243: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
Fold 3 Epoch 031; Train loss: 0.2527; Time: 00:00:01
Fold 3 Epoch 032; Train loss: 0.2566; Time: 00:00:02
Fold 3 Epoch 033; Train loss: 0.2508; Time: 00:00:01
Fold 3 Epoch 034; Train loss: 0.2504; Time: 00:00:01
Fold 3 Epoch 035; Train loss: 0.2481; Time: 00:00:01
Fold 3 Epoch 036; Train loss: 0.2405; Time: 00:00:02
Fold 3 Epoch 037; Train loss: 0.2384; Time: 00:00:01
Fold 3 Epoch 038; Train loss: 0.2344; Time: 00:00:01
Fold 3 Epoch 039; Train loss: 0.2300; Time: 00:00:02
Fold 3 Epoch 040; Train loss: 0.2323; Time: 00:00:02
Fold 3: Evaluation at Epoch 40
HR@5       NDCG@5     HR@10      NDCG@10   
0.642384   0.528164   0.778146   0.572686  
Loss: 0.2671
/home/nit/Desktop/2PDreamRec/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:243: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
Fold 3 Epoch 041; Train loss: 0.2243; Time: 00:00:02
Fold 3 Epoch 042; Train loss: 0.2215; Time: 00:00:01
Fold 3 Epoch 043; Train loss: 0.2207; Time: 00:00:02
Fold 3 Epoch 044; Train loss: 0.2185; Time: 00:00:02
Fold 3 Epoch 045; Train loss: 0.2099; Time: 00:00:02
Fold 3 Epoch 046; Train loss: 0.2162; Time: 00:00:02
Fold 3 Epoch 047; Train loss: 0.2096; Time: 00:00:02
Fold 3 Epoch 048; Train loss: 0.2074; Time: 00:00:01
Fold 3 Epoch 049; Train loss: 0.2043; Time: 00:00:01
Fold 3 Epoch 050; Train loss: 0.2006; Time: 00:00:02
Fold 3: Evaluation at Epoch 50
HR@5       NDCG@5     HR@10      NDCG@10   
0.622517   0.515501   0.759934   0.560586  
Loss: 0.2445
/home/nit/Desktop/2PDreamRec/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:243: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
Fold 3 Epoch 051; Train loss: 0.1999; Time: 00:00:01
Fold 3 Epoch 052; Train loss: 0.1990; Time: 00:00:01
Fold 3 Epoch 053; Train loss: 0.1930; Time: 00:00:01
Fold 3 Epoch 054; Train loss: 0.1896; Time: 00:00:01
Fold 3 Epoch 055; Train loss: 0.1929; Time: 00:00:01
Fold 3 Epoch 056; Train loss: 0.1882; Time: 00:00:02
Fold 3 Epoch 057; Train loss: 0.1830; Time: 00:00:01
Fold 3 Epoch 058; Train loss: 0.1845; Time: 00:00:02
Fold 3 Epoch 059; Train loss: 0.1792; Time: 00:00:01
Fold 3 Epoch 060; Train loss: 0.1770; Time: 00:00:01
Fold 3: Evaluation at Epoch 60
HR@5       NDCG@5     HR@10      NDCG@10   
0.675497   0.555150   0.799669   0.596039  
Loss: 0.2157
/home/nit/Desktop/2PDreamRec/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:243: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
Fold 3 Epoch 061; Train loss: 0.1709; Time: 00:00:01
Fold 3 Epoch 062; Train loss: 0.1676; Time: 00:00:01
Fold 3 Epoch 063; Train loss: 0.1678; Time: 00:00:01
Fold 3 Epoch 064; Train loss: 0.1642; Time: 00:00:01
Fold 3 Epoch 065; Train loss: 0.1643; Time: 00:00:01
Fold 3 Epoch 066; Train loss: 0.1627; Time: 00:00:01
Fold 3 Epoch 067; Train loss: 0.1560; Time: 00:00:02
Fold 3 Epoch 068; Train loss: 0.1582; Time: 00:00:02
Fold 3 Epoch 069; Train loss: 0.1579; Time: 00:00:01
Fold 3 Epoch 070; Train loss: 0.1514; Time: 00:00:01
Fold 3: Evaluation at Epoch 70
HR@5       NDCG@5     HR@10      NDCG@10   
0.697020   0.562979   0.837748   0.608777  
Loss: 0.1968
/home/nit/Desktop/2PDreamRec/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:243: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
Fold 3 Epoch 071; Train loss: 0.1501; Time: 00:00:01
Fold 3 Epoch 072; Train loss: 0.1469; Time: 00:00:02
Fold 3 Epoch 073; Train loss: 0.1480; Time: 00:00:01
Fold 3 Epoch 074; Train loss: 0.1441; Time: 00:00:01
Fold 3 Epoch 075; Train loss: 0.1422; Time: 00:00:02
Fold 3 Epoch 076; Train loss: 0.1424; Time: 00:00:02
Fold 3 Epoch 077; Train loss: 0.1390; Time: 00:00:01
Fold 3 Epoch 078; Train loss: 0.1395; Time: 00:00:01
Fold 3 Epoch 079; Train loss: 0.1353; Time: 00:00:01
Fold 3 Epoch 080; Train loss: 0.1323; Time: 00:00:01
Fold 3: Evaluation at Epoch 80
HR@5       NDCG@5     HR@10      NDCG@10   
0.645695   0.526829   0.779801   0.571273  
Loss: 0.1660
/home/nit/Desktop/2PDreamRec/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:243: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
[l2_decay candidate 0.0001] Fold 3: NDCG@10 = [np.float64(0.4908675872302293), np.float64(0.5335847826992549), np.float64(0.5277725799594397), np.float64(0.5726856158763686), np.float64(0.5605856904209253), np.float64(0.596038773444315), np.float64(0.6087772912435766), np.float64(0.5712728163399284)]
Tuning l2_decay:  60%|██████    | 3/5 [08:23<05:36, 168.17s/it]
========== Fold 3 ==========
INFO:root:Using statics: seq_size = 10, genre_vocab_size = 18
Fold 3 Epoch 001; Train loss: 0.9005; Time: 00:00:01
Fold 3 Epoch 002; Train loss: 0.6527; Time: 00:00:02
Fold 3 Epoch 003; Train loss: 0.5482; Time: 00:00:01
Fold 3 Epoch 004; Train loss: 0.4778; Time: 00:00:01
Fold 3 Epoch 005; Train loss: 0.4410; Time: 00:00:02
Fold 3 Epoch 006; Train loss: 0.4042; Time: 00:00:02
Fold 3 Epoch 007; Train loss: 0.3867; Time: 00:00:02
Fold 3 Epoch 008; Train loss: 0.3637; Time: 00:00:02
Fold 3 Epoch 009; Train loss: 0.3487; Time: 00:00:02
Fold 3 Epoch 010; Train loss: 0.3384; Time: 00:00:02
Fold 3: Evaluation at Epoch 10
HR@5       NDCG@5     HR@10      NDCG@10   
0.604305   0.495544   0.726821   0.534716  
Loss: 0.4223
/home/nit/Desktop/2PDreamRec/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:243: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
Fold 3 Epoch 011; Train loss: 0.3312; Time: 00:00:01
Fold 3 Epoch 012; Train loss: 0.3150; Time: 00:00:01
Fold 3 Epoch 013; Train loss: 0.3144; Time: 00:00:02
Fold 3 Epoch 014; Train loss: 0.3091; Time: 00:00:01
Fold 3 Epoch 015; Train loss: 0.3007; Time: 00:00:01
Fold 3 Epoch 016; Train loss: 0.2943; Time: 00:00:01
Fold 3 Epoch 017; Train loss: 0.2858; Time: 00:00:01
Fold 3 Epoch 018; Train loss: 0.2876; Time: 00:00:01
Fold 3 Epoch 019; Train loss: 0.2788; Time: 00:00:02
Fold 3 Epoch 020; Train loss: 0.2750; Time: 00:00:01
Fold 3: Evaluation at Epoch 20
HR@5       NDCG@5     HR@10      NDCG@10   
0.591060   0.498547   0.746689   0.547841  
Loss: 0.3539
/home/nit/Desktop/2PDreamRec/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:243: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
Fold 3 Epoch 021; Train loss: 0.2746; Time: 00:00:01
Fold 3 Epoch 022; Train loss: 0.2676; Time: 00:00:01
Fold 3 Epoch 023; Train loss: 0.2690; Time: 00:00:02
Fold 3 Epoch 024; Train loss: 0.2649; Time: 00:00:01
Fold 3 Epoch 025; Train loss: 0.2570; Time: 00:00:01
Fold 3 Epoch 026; Train loss: 0.2565; Time: 00:00:02
Fold 3 Epoch 027; Train loss: 0.2528; Time: 00:00:01
Fold 3 Epoch 028; Train loss: 0.2489; Time: 00:00:01
Fold 3 Epoch 029; Train loss: 0.2464; Time: 00:00:01
Fold 3 Epoch 030; Train loss: 0.2428; Time: 00:00:01
Fold 3: Evaluation at Epoch 30
HR@5       NDCG@5     HR@10      NDCG@10   
0.576159   0.489045   0.759934   0.547625  
Loss: 0.3117
/home/nit/Desktop/2PDreamRec/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:243: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
Fold 3 Epoch 031; Train loss: 0.2393; Time: 00:00:01
Fold 3 Epoch 032; Train loss: 0.2341; Time: 00:00:01
Fold 3 Epoch 033; Train loss: 0.2277; Time: 00:00:02
Fold 3 Epoch 034; Train loss: 0.2314; Time: 00:00:01
Fold 3 Epoch 035; Train loss: 0.2239; Time: 00:00:01
Fold 3 Epoch 036; Train loss: 0.2203; Time: 00:00:01
Fold 3 Epoch 037; Train loss: 0.2171; Time: 00:00:02
Fold 3 Epoch 038; Train loss: 0.2153; Time: 00:00:01
Fold 3 Epoch 039; Train loss: 0.2184; Time: 00:00:01
Fold 3 Epoch 040; Train loss: 0.2101; Time: 00:00:02
Fold 3: Evaluation at Epoch 40
HR@5       NDCG@5     HR@10      NDCG@10   
0.609272   0.513023   0.763245   0.561626  
Loss: 0.2530
/home/nit/Desktop/2PDreamRec/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:243: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
Fold 3 Epoch 041; Train loss: 0.2083; Time: 00:00:01
Fold 3 Epoch 042; Train loss: 0.2063; Time: 00:00:01
Fold 3 Epoch 043; Train loss: 0.2033; Time: 00:00:01
Fold 3 Epoch 044; Train loss: 0.1968; Time: 00:00:01
Fold 3 Epoch 045; Train loss: 0.2003; Time: 00:00:01
Fold 3 Epoch 046; Train loss: 0.1996; Time: 00:00:01
Fold 3 Epoch 047; Train loss: 0.1940; Time: 00:00:01
Fold 3 Epoch 048; Train loss: 0.1891; Time: 00:00:01
Fold 3 Epoch 049; Train loss: 0.1853; Time: 00:00:01
Fold 3 Epoch 050; Train loss: 0.1866; Time: 00:00:01
Fold 3: Evaluation at Epoch 50
HR@5       NDCG@5     HR@10      NDCG@10   
0.599338   0.495903   0.748344   0.542925  
Loss: 0.2387
/home/nit/Desktop/2PDreamRec/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:243: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
Fold 3 Epoch 051; Train loss: 0.1832; Time: 00:00:01
Fold 3 Epoch 052; Train loss: 0.1835; Time: 00:00:02
Fold 3 Epoch 053; Train loss: 0.1773; Time: 00:00:01
Fold 3 Epoch 054; Train loss: 0.1788; Time: 00:00:01
Fold 3 Epoch 055; Train loss: 0.1712; Time: 00:00:01
Fold 3 Epoch 056; Train loss: 0.1733; Time: 00:00:01
Fold 3 Epoch 057; Train loss: 0.1701; Time: 00:00:01
Fold 3 Epoch 058; Train loss: 0.1674; Time: 00:00:01
Fold 3 Epoch 059; Train loss: 0.1621; Time: 00:00:01
Fold 3 Epoch 060; Train loss: 0.1611; Time: 00:00:02
Fold 3: Evaluation at Epoch 60
HR@5       NDCG@5     HR@10      NDCG@10   
0.610927   0.507415   0.778146   0.561113  
Loss: 0.2185
/home/nit/Desktop/2PDreamRec/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:243: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
Fold 3 Epoch 061; Train loss: 0.1596; Time: 00:00:01
Fold 3 Epoch 062; Train loss: 0.1539; Time: 00:00:01
Fold 3 Epoch 063; Train loss: 0.1565; Time: 00:00:02
Fold 3 Epoch 064; Train loss: 0.1516; Time: 00:00:02
Fold 3 Epoch 065; Train loss: 0.1500; Time: 00:00:01
Fold 3 Epoch 066; Train loss: 0.1469; Time: 00:00:02
Fold 3 Epoch 067; Train loss: 0.1457; Time: 00:00:01
Fold 3 Epoch 068; Train loss: 0.1440; Time: 00:00:01
Fold 3 Epoch 069; Train loss: 0.1431; Time: 00:00:01
Fold 3 Epoch 070; Train loss: 0.1374; Time: 00:00:01
Fold 3: Evaluation at Epoch 70
HR@5       NDCG@5     HR@10      NDCG@10   
0.622517   0.510269   0.806291   0.569849  
Loss: 0.1823
/home/nit/Desktop/2PDreamRec/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:243: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
Fold 3 Epoch 071; Train loss: 0.1363; Time: 00:00:01
Fold 3 Epoch 072; Train loss: 0.1360; Time: 00:00:01
Fold 3 Epoch 073; Train loss: 0.1328; Time: 00:00:02
Fold 3 Epoch 074; Train loss: 0.1295; Time: 00:00:01
Fold 3 Epoch 075; Train loss: 0.1288; Time: 00:00:02
Fold 3 Epoch 076; Train loss: 0.1281; Time: 00:00:02
Fold 3 Epoch 077; Train loss: 0.1275; Time: 00:00:01
Fold 3 Epoch 078; Train loss: 0.1214; Time: 00:00:01
Fold 3 Epoch 079; Train loss: 0.1227; Time: 00:00:02
Fold 3 Epoch 080; Train loss: 0.1203; Time: 00:00:02
Fold 3: Evaluation at Epoch 80
HR@5       NDCG@5     HR@10      NDCG@10   
0.647351   0.531573   0.841060   0.594174  
Loss: 0.1606
/home/nit/Desktop/2PDreamRec/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:243: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
[l2_decay candidate 1e-08] Fold 3: NDCG@10 = [np.float64(0.5347163449143137), np.float64(0.547840999787523), np.float64(0.5476245144221773), np.float64(0.5616257828004974), np.float64(0.5429247518599009), np.float64(0.5611128201686784), np.float64(0.5698486738956897), np.float64(0.594173566350566)]
Tuning l2_decay:  80%|████████  | 4/5 [11:07<02:46, 166.17s/it]
========== Fold 3 ==========
INFO:root:Using statics: seq_size = 10, genre_vocab_size = 18
Fold 3 Epoch 001; Train loss: 1.0396; Time: 00:00:02
Fold 3 Epoch 002; Train loss: 0.7716; Time: 00:00:01
Fold 3 Epoch 003; Train loss: 0.6472; Time: 00:00:01
Fold 3 Epoch 004; Train loss: 0.5625; Time: 00:00:02
Fold 3 Epoch 005; Train loss: 0.5101; Time: 00:00:02
Fold 3 Epoch 006; Train loss: 0.4633; Time: 00:00:01
Fold 3 Epoch 007; Train loss: 0.4486; Time: 00:00:02
Fold 3 Epoch 008; Train loss: 0.4285; Time: 00:00:01
Fold 3 Epoch 009; Train loss: 0.4002; Time: 00:00:01
Fold 3 Epoch 010; Train loss: 0.3985; Time: 00:00:01
Fold 3: Evaluation at Epoch 10
HR@5       NDCG@5     HR@10      NDCG@10   
0.632450   0.465463   0.754967   0.504888  
Loss: 0.4331
/home/nit/Desktop/2PDreamRec/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:243: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
Fold 3 Epoch 011; Train loss: 0.3832; Time: 00:00:02
Fold 3 Epoch 012; Train loss: 0.3756; Time: 00:00:02
Fold 3 Epoch 013; Train loss: 0.3671; Time: 00:00:02
Fold 3 Epoch 014; Train loss: 0.3623; Time: 00:00:02
Fold 3 Epoch 015; Train loss: 0.3495; Time: 00:00:02
Fold 3 Epoch 016; Train loss: 0.3489; Time: 00:00:02
Fold 3 Epoch 017; Train loss: 0.3385; Time: 00:00:01
Fold 3 Epoch 018; Train loss: 0.3349; Time: 00:00:01
Fold 3 Epoch 019; Train loss: 0.3243; Time: 00:00:01
Fold 3 Epoch 020; Train loss: 0.3171; Time: 00:00:01
Fold 3: Evaluation at Epoch 20
HR@5       NDCG@5     HR@10      NDCG@10   
0.612583   0.487791   0.725166   0.524298  
Loss: 0.3792
/home/nit/Desktop/2PDreamRec/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:243: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
Fold 3 Epoch 021; Train loss: 0.3205; Time: 00:00:01
Fold 3 Epoch 022; Train loss: 0.3188; Time: 00:00:02
Fold 3 Epoch 023; Train loss: 0.3125; Time: 00:00:01
Fold 3 Epoch 024; Train loss: 0.3061; Time: 00:00:02
Fold 3 Epoch 025; Train loss: 0.3078; Time: 00:00:02
Fold 3 Epoch 026; Train loss: 0.3024; Time: 00:00:01
Fold 3 Epoch 027; Train loss: 0.2927; Time: 00:00:01
Fold 3 Epoch 028; Train loss: 0.2890; Time: 00:00:01
Fold 3 Epoch 029; Train loss: 0.2898; Time: 00:00:01
Fold 3 Epoch 030; Train loss: 0.2842; Time: 00:00:01
Fold 3: Evaluation at Epoch 30
HR@5       NDCG@5     HR@10      NDCG@10   
0.622517   0.493948   0.730132   0.529085  
Loss: 0.3038
/home/nit/Desktop/2PDreamRec/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:243: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
Fold 3 Epoch 031; Train loss: 0.2750; Time: 00:00:01
Fold 3 Epoch 032; Train loss: 0.2756; Time: 00:00:01
Fold 3 Epoch 033; Train loss: 0.2728; Time: 00:00:01
Fold 3 Epoch 034; Train loss: 0.2635; Time: 00:00:01
Fold 3 Epoch 035; Train loss: 0.2692; Time: 00:00:02
Fold 3 Epoch 036; Train loss: 0.2620; Time: 00:00:01
Fold 3 Epoch 037; Train loss: 0.2625; Time: 00:00:02
Fold 3 Epoch 038; Train loss: 0.2568; Time: 00:00:01
Fold 3 Epoch 039; Train loss: 0.2585; Time: 00:00:01
Fold 3 Epoch 040; Train loss: 0.2479; Time: 00:00:02
Fold 3: Evaluation at Epoch 40
HR@5       NDCG@5     HR@10      NDCG@10   
0.615894   0.498927   0.713576   0.530221  
Loss: 0.3075
/home/nit/Desktop/2PDreamRec/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:243: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
Fold 3 Epoch 041; Train loss: 0.2464; Time: 00:00:01
Fold 3 Epoch 042; Train loss: 0.2440; Time: 00:00:02
Fold 3 Epoch 043; Train loss: 0.2456; Time: 00:00:01
Fold 3 Epoch 044; Train loss: 0.2418; Time: 00:00:01
Fold 3 Epoch 045; Train loss: 0.2385; Time: 00:00:01
Fold 3 Epoch 046; Train loss: 0.2410; Time: 00:00:01
Fold 3 Epoch 047; Train loss: 0.2326; Time: 00:00:02
Fold 3 Epoch 048; Train loss: 0.2266; Time: 00:00:01
Fold 3 Epoch 049; Train loss: 0.2232; Time: 00:00:01
Fold 3 Epoch 050; Train loss: 0.2239; Time: 00:00:01
Fold 3: Evaluation at Epoch 50
HR@5       NDCG@5     HR@10      NDCG@10   
0.604305   0.486134   0.723510   0.523936  
Loss: 0.2637
/home/nit/Desktop/2PDreamRec/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:243: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
Fold 3 Epoch 051; Train loss: 0.2193; Time: 00:00:01
Fold 3 Epoch 052; Train loss: 0.2188; Time: 00:00:01
Fold 3 Epoch 053; Train loss: 0.2121; Time: 00:00:01
Fold 3 Epoch 054; Train loss: 0.2135; Time: 00:00:02
Fold 3 Epoch 055; Train loss: 0.2042; Time: 00:00:01
Fold 3 Epoch 056; Train loss: 0.2047; Time: 00:00:01
Fold 3 Epoch 057; Train loss: 0.2027; Time: 00:00:01
Fold 3 Epoch 058; Train loss: 0.2034; Time: 00:00:01
Fold 3 Epoch 059; Train loss: 0.1999; Time: 00:00:02
Fold 3 Epoch 060; Train loss: 0.1945; Time: 00:00:02
Fold 3: Evaluation at Epoch 60
HR@5       NDCG@5     HR@10      NDCG@10   
0.667219   0.519051   0.759934   0.548410  
Loss: 0.2431
/home/nit/Desktop/2PDreamRec/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:243: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
Fold 3 Epoch 061; Train loss: 0.1926; Time: 00:00:01
Fold 3 Epoch 062; Train loss: 0.1893; Time: 00:00:01
Fold 3 Epoch 063; Train loss: 0.1877; Time: 00:00:02
Fold 3 Epoch 064; Train loss: 0.1889; Time: 00:00:02
Fold 3 Epoch 065; Train loss: 0.1869; Time: 00:00:02
Fold 3 Epoch 066; Train loss: 0.1849; Time: 00:00:01
Fold 3 Epoch 067; Train loss: 0.1776; Time: 00:00:02
Fold 3 Epoch 068; Train loss: 0.1775; Time: 00:00:01
Fold 3 Epoch 069; Train loss: 0.1714; Time: 00:00:01
Fold 3 Epoch 070; Train loss: 0.1724; Time: 00:00:01
Fold 3: Evaluation at Epoch 70
HR@5       NDCG@5     HR@10      NDCG@10   
0.605960   0.488304   0.731788   0.528681  
Loss: 0.2125
/home/nit/Desktop/2PDreamRec/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:243: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
Fold 3 Epoch 071; Train loss: 0.1689; Time: 00:00:01
Fold 3 Epoch 072; Train loss: 0.1693; Time: 00:00:01
Fold 3 Epoch 073; Train loss: 0.1686; Time: 00:00:02
Fold 3 Epoch 074; Train loss: 0.1612; Time: 00:00:01
Fold 3 Epoch 075; Train loss: 0.1607; Time: 00:00:02
Fold 3 Epoch 076; Train loss: 0.1623; Time: 00:00:01
Fold 3 Epoch 077; Train loss: 0.1596; Time: 00:00:01
Fold 3 Epoch 078; Train loss: 0.1549; Time: 00:00:02
Fold 3 Epoch 079; Train loss: 0.1487; Time: 00:00:02
Fold 3 Epoch 080; Train loss: 0.1516; Time: 00:00:02
Fold 3: Evaluation at Epoch 80
HR@5       NDCG@5     HR@10      NDCG@10   
0.635762   0.517791   0.735099   0.549396  
Loss: 0.1715
/home/nit/Desktop/2PDreamRec/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:243: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
[l2_decay candidate 1e-16] Fold 3: NDCG@10 = [np.float64(0.5048879330739919), np.float64(0.5242983324935014), np.float64(0.5290849347712208), np.float64(0.5302209688880843), np.float64(0.5239360464952625), np.float64(0.548410190597735), np.float64(0.5286806985793616), np.float64(0.5493961101917558)]
Tuning l2_decay: 100%|██████████| 5/5 [13:55<00:00, 167.04s/it]Tuning l2_decay: 100%|██████████| 5/5 [13:55<00:00, 167.12s/it]

Best l2_decay found: 0.0001
Tuning eps:   0%|          | 0/5 [00:00<?, ?it/s]
========== Fold 3 ==========
INFO:root:Using statics: seq_size = 10, genre_vocab_size = 18
Fold 3 Epoch 001; Train loss: 1.2367; Time: 00:00:01
Fold 3 Epoch 002; Train loss: 1.1866; Time: 00:00:01
Fold 3 Epoch 003; Train loss: 1.1339; Time: 00:00:01
Fold 3 Epoch 004; Train loss: 1.0861; Time: 00:00:02
Fold 3 Epoch 005; Train loss: 1.0437; Time: 00:00:01
Fold 3 Epoch 006; Train loss: 1.0112; Time: 00:00:01
Fold 3 Epoch 007; Train loss: 0.9772; Time: 00:00:01
Fold 3 Epoch 008; Train loss: 0.9493; Time: 00:00:02
Fold 3 Epoch 009; Train loss: 0.9239; Time: 00:00:02
Fold 3 Epoch 010; Train loss: 0.8994; Time: 00:00:01
Fold 3: Evaluation at Epoch 10
HR@5       NDCG@5     HR@10      NDCG@10   
0.617550   0.422478   0.700331   0.449047  
Loss: 1.0196
/home/nit/Desktop/2PDreamRec/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:243: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
Fold 3 Epoch 011; Train loss: 0.8804; Time: 00:00:01
Fold 3 Epoch 012; Train loss: 0.8582; Time: 00:00:01
Fold 3 Epoch 013; Train loss: 0.8451; Time: 00:00:02
Fold 3 Epoch 014; Train loss: 0.8272; Time: 00:00:02
Fold 3 Epoch 015; Train loss: 0.8131; Time: 00:00:02
Fold 3 Epoch 016; Train loss: 0.7943; Time: 00:00:01
Fold 3 Epoch 017; Train loss: 0.7794; Time: 00:00:02
Fold 3 Epoch 018; Train loss: 0.7697; Time: 00:00:02
Fold 3 Epoch 019; Train loss: 0.7596; Time: 00:00:01
Fold 3 Epoch 020; Train loss: 0.7486; Time: 00:00:02
Fold 3: Evaluation at Epoch 20
HR@5       NDCG@5     HR@10      NDCG@10   
0.625828   0.427193   0.705298   0.452849  
Loss: 0.9485
/home/nit/Desktop/2PDreamRec/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:243: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
Fold 3 Epoch 021; Train loss: 0.7382; Time: 00:00:01
Fold 3 Epoch 022; Train loss: 0.7323; Time: 00:00:02
Fold 3 Epoch 023; Train loss: 0.7183; Time: 00:00:02
Fold 3 Epoch 024; Train loss: 0.7135; Time: 00:00:02
Fold 3 Epoch 025; Train loss: 0.6998; Time: 00:00:02
Fold 3 Epoch 026; Train loss: 0.6908; Time: 00:00:02
Fold 3 Epoch 027; Train loss: 0.6852; Time: 00:00:02
Fold 3 Epoch 028; Train loss: 0.6776; Time: 00:00:02
Fold 3 Epoch 029; Train loss: 0.6667; Time: 00:00:02
Fold 3 Epoch 030; Train loss: 0.6601; Time: 00:00:01
Fold 3: Evaluation at Epoch 30
HR@5       NDCG@5     HR@10      NDCG@10   
0.589404   0.415843   0.711921   0.455955  
Loss: 0.8747
/home/nit/Desktop/2PDreamRec/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:243: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
Fold 3 Epoch 031; Train loss: 0.6557; Time: 00:00:01
Fold 3 Epoch 032; Train loss: 0.6477; Time: 00:00:01
Fold 3 Epoch 033; Train loss: 0.6453; Time: 00:00:02
Fold 3 Epoch 034; Train loss: 0.6323; Time: 00:00:02
Fold 3 Epoch 035; Train loss: 0.6231; Time: 00:00:01
Fold 3 Epoch 036; Train loss: 0.6219; Time: 00:00:01
Fold 3 Epoch 037; Train loss: 0.6181; Time: 00:00:01
Fold 3 Epoch 038; Train loss: 0.6170; Time: 00:00:02
Fold 3 Epoch 039; Train loss: 0.6076; Time: 00:00:02
Fold 3 Epoch 040; Train loss: 0.5978; Time: 00:00:01
Fold 3: Evaluation at Epoch 40
HR@5       NDCG@5     HR@10      NDCG@10   
0.562914   0.405303   0.756623   0.466359  
Loss: 0.8206
/home/nit/Desktop/2PDreamRec/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:243: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
Fold 3 Epoch 041; Train loss: 0.6044; Time: 00:00:01
Fold 3 Epoch 042; Train loss: 0.5941; Time: 00:00:01
Fold 3 Epoch 043; Train loss: 0.5872; Time: 00:00:01
Fold 3 Epoch 044; Train loss: 0.5914; Time: 00:00:01
Fold 3 Epoch 045; Train loss: 0.5802; Time: 00:00:02
Fold 3 Epoch 046; Train loss: 0.5772; Time: 00:00:01
Fold 3 Epoch 047; Train loss: 0.5763; Time: 00:00:01
Fold 3 Epoch 048; Train loss: 0.5680; Time: 00:00:01
Fold 3 Epoch 049; Train loss: 0.5639; Time: 00:00:01
Fold 3 Epoch 050; Train loss: 0.5618; Time: 00:00:01
Fold 3: Evaluation at Epoch 50
HR@5       NDCG@5     HR@10      NDCG@10   
0.571192   0.425608   0.753311   0.484010  
Loss: 0.7534
/home/nit/Desktop/2PDreamRec/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:243: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
Fold 3 Epoch 051; Train loss: 0.5600; Time: 00:00:01
Fold 3 Epoch 052; Train loss: 0.5586; Time: 00:00:01
Fold 3 Epoch 053; Train loss: 0.5606; Time: 00:00:02
Fold 3 Epoch 054; Train loss: 0.5490; Time: 00:00:02
Fold 3 Epoch 055; Train loss: 0.5472; Time: 00:00:01
Fold 3 Epoch 056; Train loss: 0.5409; Time: 00:00:02
Fold 3 Epoch 057; Train loss: 0.5330; Time: 00:00:01
Fold 3 Epoch 058; Train loss: 0.5406; Time: 00:00:02
Fold 3 Epoch 059; Train loss: 0.5300; Time: 00:00:02
Fold 3 Epoch 060; Train loss: 0.5381; Time: 00:00:01
Fold 3: Evaluation at Epoch 60
HR@5       NDCG@5     HR@10      NDCG@10   
0.589404   0.435429   0.769868   0.492738  
Loss: 0.7440
/home/nit/Desktop/2PDreamRec/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:243: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
Fold 3 Epoch 061; Train loss: 0.5341; Time: 00:00:01
Fold 3 Epoch 062; Train loss: 0.5248; Time: 00:00:01
Fold 3 Epoch 063; Train loss: 0.5325; Time: 00:00:01
Fold 3 Epoch 064; Train loss: 0.5223; Time: 00:00:02
Fold 3 Epoch 065; Train loss: 0.5155; Time: 00:00:01
Fold 3 Epoch 066; Train loss: 0.5203; Time: 00:00:01
Fold 3 Epoch 067; Train loss: 0.5162; Time: 00:00:01
Fold 3 Epoch 068; Train loss: 0.5174; Time: 00:00:02
Fold 3 Epoch 069; Train loss: 0.5140; Time: 00:00:01
Fold 3 Epoch 070; Train loss: 0.5087; Time: 00:00:01
Fold 3: Evaluation at Epoch 70
HR@5       NDCG@5     HR@10      NDCG@10   
0.579470   0.424664   0.753311   0.480571  
Loss: 0.6853
/home/nit/Desktop/2PDreamRec/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:243: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
Fold 3 Epoch 071; Train loss: 0.5127; Time: 00:00:01
Fold 3 Epoch 072; Train loss: 0.5101; Time: 00:00:01
Fold 3 Epoch 073; Train loss: 0.5068; Time: 00:00:01
Fold 3 Epoch 074; Train loss: 0.4999; Time: 00:00:01
Fold 3 Epoch 075; Train loss: 0.5035; Time: 00:00:02
Fold 3 Epoch 076; Train loss: 0.5059; Time: 00:00:02
Fold 3 Epoch 077; Train loss: 0.5004; Time: 00:00:01
Fold 3 Epoch 078; Train loss: 0.4952; Time: 00:00:01
Fold 3 Epoch 079; Train loss: 0.4969; Time: 00:00:01
Fold 3 Epoch 080; Train loss: 0.5002; Time: 00:00:02
Fold 3: Evaluation at Epoch 80
HR@5       NDCG@5     HR@10      NDCG@10   
0.567881   0.416034   0.748344   0.474156  
Loss: 0.6881
/home/nit/Desktop/2PDreamRec/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:243: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
[eps candidate 0.1] Fold 3: NDCG@10 = [np.float64(0.44904720571498935), np.float64(0.45284937314654883), np.float64(0.45595465926294265), np.float64(0.46635936886245666), np.float64(0.4840097947473691), np.float64(0.49273816177974594), np.float64(0.48057115511210513), np.float64(0.47415633204028856)]
Tuning eps:  20%|██        | 1/5 [02:48<11:14, 168.59s/it]
========== Fold 3 ==========
INFO:root:Using statics: seq_size = 10, genre_vocab_size = 18
Fold 3 Epoch 001; Train loss: 1.1945; Time: 00:00:01
Fold 3 Epoch 002; Train loss: 0.9993; Time: 00:00:01
Fold 3 Epoch 003; Train loss: 0.8686; Time: 00:00:02
Fold 3 Epoch 004; Train loss: 0.7838; Time: 00:00:02
Fold 3 Epoch 005; Train loss: 0.7178; Time: 00:00:01
Fold 3 Epoch 006; Train loss: 0.6711; Time: 00:00:01
Fold 3 Epoch 007; Train loss: 0.6358; Time: 00:00:01
Fold 3 Epoch 008; Train loss: 0.6066; Time: 00:00:01
Fold 3 Epoch 009; Train loss: 0.5812; Time: 00:00:01
Fold 3 Epoch 010; Train loss: 0.5620; Time: 00:00:01
Fold 3: Evaluation at Epoch 10
HR@5       NDCG@5     HR@10      NDCG@10   
0.746689   0.525590   0.879139   0.568301  
Loss: 0.7379
/home/nit/Desktop/2PDreamRec/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:243: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
Fold 3 Epoch 011; Train loss: 0.5387; Time: 00:00:01
Fold 3 Epoch 012; Train loss: 0.5242; Time: 00:00:01
Fold 3 Epoch 013; Train loss: 0.5150; Time: 00:00:01
Fold 3 Epoch 014; Train loss: 0.5013; Time: 00:00:02
Fold 3 Epoch 015; Train loss: 0.4879; Time: 00:00:02
Fold 3 Epoch 016; Train loss: 0.4887; Time: 00:00:01
Fold 3 Epoch 017; Train loss: 0.4746; Time: 00:00:01
Fold 3 Epoch 018; Train loss: 0.4655; Time: 00:00:01
Fold 3 Epoch 019; Train loss: 0.4616; Time: 00:00:01
Fold 3 Epoch 020; Train loss: 0.4573; Time: 00:00:02
Fold 3: Evaluation at Epoch 20
HR@5       NDCG@5     HR@10      NDCG@10   
0.675497   0.481866   0.846026   0.536584  
Loss: 0.6358
/home/nit/Desktop/2PDreamRec/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:243: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
Fold 3 Epoch 021; Train loss: 0.4581; Time: 00:00:01
Fold 3 Epoch 022; Train loss: 0.4534; Time: 00:00:01
Fold 3 Epoch 023; Train loss: 0.4463; Time: 00:00:01
Fold 3 Epoch 024; Train loss: 0.4358; Time: 00:00:01
Fold 3 Epoch 025; Train loss: 0.4332; Time: 00:00:01
Fold 3 Epoch 026; Train loss: 0.4446; Time: 00:00:01
Fold 3 Epoch 027; Train loss: 0.4257; Time: 00:00:01
Fold 3 Epoch 028; Train loss: 0.4243; Time: 00:00:02
Fold 3 Epoch 029; Train loss: 0.4203; Time: 00:00:02
Fold 3 Epoch 030; Train loss: 0.4118; Time: 00:00:01
Fold 3: Evaluation at Epoch 30
HR@5       NDCG@5     HR@10      NDCG@10   
0.668874   0.487285   0.831126   0.539683  
Loss: 0.5623
/home/nit/Desktop/2PDreamRec/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:243: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
Fold 3 Epoch 031; Train loss: 0.4134; Time: 00:00:01
Fold 3 Epoch 032; Train loss: 0.4136; Time: 00:00:01
Fold 3 Epoch 033; Train loss: 0.4056; Time: 00:00:01
Fold 3 Epoch 034; Train loss: 0.4008; Time: 00:00:02
Fold 3 Epoch 035; Train loss: 0.4068; Time: 00:00:01
Fold 3 Epoch 036; Train loss: 0.4031; Time: 00:00:01
Fold 3 Epoch 037; Train loss: 0.3987; Time: 00:00:01
Fold 3 Epoch 038; Train loss: 0.4002; Time: 00:00:01
Fold 3 Epoch 039; Train loss: 0.3982; Time: 00:00:01
Fold 3 Epoch 040; Train loss: 0.3944; Time: 00:00:02
Fold 3: Evaluation at Epoch 40
HR@5       NDCG@5     HR@10      NDCG@10   
0.680464   0.504751   0.847682   0.558215  
Loss: 0.4967
/home/nit/Desktop/2PDreamRec/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:243: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
Fold 3 Epoch 041; Train loss: 0.3891; Time: 00:00:01
Fold 3 Epoch 042; Train loss: 0.3894; Time: 00:00:01
Fold 3 Epoch 043; Train loss: 0.3889; Time: 00:00:02
Fold 3 Epoch 044; Train loss: 0.3821; Time: 00:00:02
Fold 3 Epoch 045; Train loss: 0.3868; Time: 00:00:02
Fold 3 Epoch 046; Train loss: 0.3742; Time: 00:00:02
Fold 3 Epoch 047; Train loss: 0.3774; Time: 00:00:01
Fold 3 Epoch 048; Train loss: 0.3753; Time: 00:00:01
Fold 3 Epoch 049; Train loss: 0.3774; Time: 00:00:01
Fold 3 Epoch 050; Train loss: 0.3732; Time: 00:00:02
Fold 3: Evaluation at Epoch 50
HR@5       NDCG@5     HR@10      NDCG@10   
0.683775   0.521455   0.839404   0.571643  
Loss: 0.4863
/home/nit/Desktop/2PDreamRec/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:243: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
Fold 3 Epoch 051; Train loss: 0.3741; Time: 00:00:01
Fold 3 Epoch 052; Train loss: 0.3763; Time: 00:00:01
Fold 3 Epoch 053; Train loss: 0.3734; Time: 00:00:01
Fold 3 Epoch 054; Train loss: 0.3730; Time: 00:00:02
Fold 3 Epoch 055; Train loss: 0.3655; Time: 00:00:01
Fold 3 Epoch 056; Train loss: 0.3698; Time: 00:00:01
Fold 3 Epoch 057; Train loss: 0.3674; Time: 00:00:02
Fold 3 Epoch 058; Train loss: 0.3644; Time: 00:00:02
Fold 3 Epoch 059; Train loss: 0.3661; Time: 00:00:02
Fold 3 Epoch 060; Train loss: 0.3607; Time: 00:00:01
Fold 3: Evaluation at Epoch 60
HR@5       NDCG@5     HR@10      NDCG@10   
0.685430   0.521271   0.849338   0.573947  
Loss: 0.4559
/home/nit/Desktop/2PDreamRec/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:243: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
Fold 3 Epoch 061; Train loss: 0.3634; Time: 00:00:01
Fold 3 Epoch 062; Train loss: 0.3584; Time: 00:00:02
Fold 3 Epoch 063; Train loss: 0.3630; Time: 00:00:02
Fold 3 Epoch 064; Train loss: 0.3591; Time: 00:00:02
Fold 3 Epoch 065; Train loss: 0.3601; Time: 00:00:01
Fold 3 Epoch 066; Train loss: 0.3531; Time: 00:00:01
Fold 3 Epoch 067; Train loss: 0.3547; Time: 00:00:01
Fold 3 Epoch 068; Train loss: 0.3522; Time: 00:00:02
Fold 3 Epoch 069; Train loss: 0.3493; Time: 00:00:02
Fold 3 Epoch 070; Train loss: 0.3471; Time: 00:00:01
Fold 3: Evaluation at Epoch 70
HR@5       NDCG@5     HR@10      NDCG@10   
0.692053   0.536475   0.841060   0.584379  
Loss: 0.4306
/home/nit/Desktop/2PDreamRec/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:243: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
Fold 3 Epoch 071; Train loss: 0.3479; Time: 00:00:01
Fold 3 Epoch 072; Train loss: 0.3518; Time: 00:00:02
Fold 3 Epoch 073; Train loss: 0.3470; Time: 00:00:02
Fold 3 Epoch 074; Train loss: 0.3546; Time: 00:00:02
Fold 3 Epoch 075; Train loss: 0.3470; Time: 00:00:01
Fold 3 Epoch 076; Train loss: 0.3442; Time: 00:00:01
Fold 3 Epoch 077; Train loss: 0.3373; Time: 00:00:01
Fold 3 Epoch 078; Train loss: 0.3426; Time: 00:00:01
Fold 3 Epoch 079; Train loss: 0.3405; Time: 00:00:01
Fold 3 Epoch 080; Train loss: 0.3404; Time: 00:00:01
Fold 3: Evaluation at Epoch 80
HR@5       NDCG@5     HR@10      NDCG@10   
0.687086   0.539485   0.841060   0.588892  
Loss: 0.4427
/home/nit/Desktop/2PDreamRec/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:243: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
[eps candidate 0.01] Fold 3: NDCG@10 = [np.float64(0.5683006780930873), np.float64(0.5365844814066573), np.float64(0.5396826202672665), np.float64(0.558215341590561), np.float64(0.5716434003797927), np.float64(0.5739468781721069), np.float64(0.5843791454540245), np.float64(0.588892206559598)]
Tuning eps:  40%|████      | 2/5 [05:31<08:15, 165.22s/it]
========== Fold 3 ==========
INFO:root:Using statics: seq_size = 10, genre_vocab_size = 18
Fold 3 Epoch 001; Train loss: 1.0396; Time: 00:00:01
Fold 3 Epoch 002; Train loss: 0.7734; Time: 00:00:02
Fold 3 Epoch 003; Train loss: 0.6385; Time: 00:00:01
Fold 3 Epoch 004; Train loss: 0.5578; Time: 00:00:01
Fold 3 Epoch 005; Train loss: 0.5055; Time: 00:00:02
Fold 3 Epoch 006; Train loss: 0.4645; Time: 00:00:01
Fold 3 Epoch 007; Train loss: 0.4380; Time: 00:00:02
Fold 3 Epoch 008; Train loss: 0.4231; Time: 00:00:01
Fold 3 Epoch 009; Train loss: 0.4076; Time: 00:00:01
Fold 3 Epoch 010; Train loss: 0.3885; Time: 00:00:01
Fold 3: Evaluation at Epoch 10
HR@5       NDCG@5     HR@10      NDCG@10   
0.673841   0.505021   0.834437   0.555871  
Loss: 0.4698
/home/nit/Desktop/2PDreamRec/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:243: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
Fold 3 Epoch 011; Train loss: 0.3823; Time: 00:00:01
Fold 3 Epoch 012; Train loss: 0.3708; Time: 00:00:01
Fold 3 Epoch 013; Train loss: 0.3662; Time: 00:00:01
Fold 3 Epoch 014; Train loss: 0.3559; Time: 00:00:01
Fold 3 Epoch 015; Train loss: 0.3499; Time: 00:00:01
Fold 3 Epoch 016; Train loss: 0.3450; Time: 00:00:02
Fold 3 Epoch 017; Train loss: 0.3385; Time: 00:00:02
Fold 3 Epoch 018; Train loss: 0.3285; Time: 00:00:01
Fold 3 Epoch 019; Train loss: 0.3261; Time: 00:00:01
Fold 3 Epoch 020; Train loss: 0.3203; Time: 00:00:01
Fold 3: Evaluation at Epoch 20
HR@5       NDCG@5     HR@10      NDCG@10   
0.637417   0.486277   0.773179   0.529629  
Loss: 0.4224
/home/nit/Desktop/2PDreamRec/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:243: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
Fold 3 Epoch 021; Train loss: 0.3176; Time: 00:00:01
Fold 3 Epoch 022; Train loss: 0.3147; Time: 00:00:02
Fold 3 Epoch 023; Train loss: 0.3094; Time: 00:00:02
Fold 3 Epoch 024; Train loss: 0.3100; Time: 00:00:03
Fold 3 Epoch 025; Train loss: 0.3023; Time: 00:00:02
Fold 3 Epoch 026; Train loss: 0.2924; Time: 00:00:01
Fold 3 Epoch 027; Train loss: 0.3001; Time: 00:00:01
Fold 3 Epoch 028; Train loss: 0.2932; Time: 00:00:01
Fold 3 Epoch 029; Train loss: 0.2869; Time: 00:00:01
Fold 3 Epoch 030; Train loss: 0.2862; Time: 00:00:01
Fold 3: Evaluation at Epoch 30
HR@5       NDCG@5     HR@10      NDCG@10   
0.655629   0.518824   0.816225   0.570402  
Loss: 0.3614
/home/nit/Desktop/2PDreamRec/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:243: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
Fold 3 Epoch 031; Train loss: 0.2855; Time: 00:00:01
Fold 3 Epoch 032; Train loss: 0.2776; Time: 00:00:01
Fold 3 Epoch 033; Train loss: 0.2743; Time: 00:00:02
Fold 3 Epoch 034; Train loss: 0.2765; Time: 00:00:01
Fold 3 Epoch 035; Train loss: 0.2676; Time: 00:00:01
Fold 3 Epoch 036; Train loss: 0.2714; Time: 00:00:01
Fold 3 Epoch 037; Train loss: 0.2630; Time: 00:00:01
Fold 3 Epoch 038; Train loss: 0.2615; Time: 00:00:02
Fold 3 Epoch 039; Train loss: 0.2596; Time: 00:00:02
Fold 3 Epoch 040; Train loss: 0.2523; Time: 00:00:01
Fold 3: Evaluation at Epoch 40
HR@5       NDCG@5     HR@10      NDCG@10   
0.647351   0.512071   0.812914   0.565349  
Loss: 0.3226
/home/nit/Desktop/2PDreamRec/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:243: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
Fold 3 Epoch 041; Train loss: 0.2537; Time: 00:00:01
Fold 3 Epoch 042; Train loss: 0.2443; Time: 00:00:01
Fold 3 Epoch 043; Train loss: 0.2454; Time: 00:00:01
Fold 3 Epoch 044; Train loss: 0.2426; Time: 00:00:02
Fold 3 Epoch 045; Train loss: 0.2407; Time: 00:00:01
Fold 3 Epoch 046; Train loss: 0.2409; Time: 00:00:01
Fold 3 Epoch 047; Train loss: 0.2340; Time: 00:00:01
Fold 3 Epoch 048; Train loss: 0.2291; Time: 00:00:02
Fold 3 Epoch 049; Train loss: 0.2312; Time: 00:00:01
Fold 3 Epoch 050; Train loss: 0.2262; Time: 00:00:01
Fold 3: Evaluation at Epoch 50
HR@5       NDCG@5     HR@10      NDCG@10   
0.647351   0.518333   0.814570   0.572962  
Loss: 0.2752
/home/nit/Desktop/2PDreamRec/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:243: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
Fold 3 Epoch 051; Train loss: 0.2226; Time: 00:00:01
Fold 3 Epoch 052; Train loss: 0.2246; Time: 00:00:01
Fold 3 Epoch 053; Train loss: 0.2187; Time: 00:00:02
Fold 3 Epoch 054; Train loss: 0.2144; Time: 00:00:01
Fold 3 Epoch 055; Train loss: 0.2113; Time: 00:00:01
Fold 3 Epoch 056; Train loss: 0.2074; Time: 00:00:01
Fold 3 Epoch 057; Train loss: 0.2041; Time: 00:00:01
Fold 3 Epoch 058; Train loss: 0.2066; Time: 00:00:01
Fold 3 Epoch 059; Train loss: 0.2057; Time: 00:00:01
Fold 3 Epoch 060; Train loss: 0.1986; Time: 00:00:01
Fold 3: Evaluation at Epoch 60
HR@5       NDCG@5     HR@10      NDCG@10   
0.634106   0.512063   0.806291   0.566945  
Loss: 0.2498
/home/nit/Desktop/2PDreamRec/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:243: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
Fold 3 Epoch 061; Train loss: 0.1979; Time: 00:00:01
Fold 3 Epoch 062; Train loss: 0.1983; Time: 00:00:02
Fold 3 Epoch 063; Train loss: 0.1966; Time: 00:00:02
Fold 3 Epoch 064; Train loss: 0.1900; Time: 00:00:01
Fold 3 Epoch 065; Train loss: 0.1867; Time: 00:00:01
Fold 3 Epoch 066; Train loss: 0.1878; Time: 00:00:01
Fold 3 Epoch 067; Train loss: 0.1799; Time: 00:00:02
Fold 3 Epoch 068; Train loss: 0.1823; Time: 00:00:01
Fold 3 Epoch 069; Train loss: 0.1785; Time: 00:00:01
Fold 3 Epoch 070; Train loss: 0.1772; Time: 00:00:01
Fold 3: Evaluation at Epoch 70
HR@5       NDCG@5     HR@10      NDCG@10   
0.625828   0.513147   0.807947   0.571169  
Loss: 0.2491
/home/nit/Desktop/2PDreamRec/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:243: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
Fold 3 Epoch 071; Train loss: 0.1736; Time: 00:00:01
Fold 3 Epoch 072; Train loss: 0.1725; Time: 00:00:02
Fold 3 Epoch 073; Train loss: 0.1668; Time: 00:00:02
Fold 3 Epoch 074; Train loss: 0.1706; Time: 00:00:02
Fold 3 Epoch 075; Train loss: 0.1662; Time: 00:00:02
Fold 3 Epoch 076; Train loss: 0.1688; Time: 00:00:02
Fold 3 Epoch 077; Train loss: 0.1620; Time: 00:00:01
Fold 3 Epoch 078; Train loss: 0.1630; Time: 00:00:01
Fold 3 Epoch 079; Train loss: 0.1590; Time: 00:00:01
Fold 3 Epoch 080; Train loss: 0.1549; Time: 00:00:02
Fold 3: Evaluation at Epoch 80
HR@5       NDCG@5     HR@10      NDCG@10   
0.653974   0.529597   0.821192   0.582868  
Loss: 0.2215
/home/nit/Desktop/2PDreamRec/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:243: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
[eps candidate 0.0001] Fold 3: NDCG@10 = [np.float64(0.555870678784789), np.float64(0.5296286970676047), np.float64(0.5704020138365864), np.float64(0.5653485975451276), np.float64(0.5729621412547192), np.float64(0.5669454187258615), np.float64(0.5711691092508272), np.float64(0.5828677601272482)]
Tuning eps:  60%|██████    | 3/5 [08:16<05:30, 165.27s/it]
========== Fold 3 ==========
INFO:root:Using statics: seq_size = 10, genre_vocab_size = 18
Fold 3 Epoch 001; Train loss: 0.9963; Time: 00:00:01
Fold 3 Epoch 002; Train loss: 0.7278; Time: 00:00:01
Fold 3 Epoch 003; Train loss: 0.6047; Time: 00:00:02
Fold 3 Epoch 004; Train loss: 0.5367; Time: 00:00:01
Fold 3 Epoch 005; Train loss: 0.4860; Time: 00:00:01
Fold 3 Epoch 006; Train loss: 0.4513; Time: 00:00:02
Fold 3 Epoch 007; Train loss: 0.4260; Time: 00:00:02
Fold 3 Epoch 008; Train loss: 0.3970; Time: 00:00:01
Fold 3 Epoch 009; Train loss: 0.3831; Time: 00:00:01
Fold 3 Epoch 010; Train loss: 0.3742; Time: 00:00:01
Fold 3: Evaluation at Epoch 10
HR@5       NDCG@5     HR@10      NDCG@10   
0.718543   0.542808   0.806291   0.571154  
Loss: 0.4302
/home/nit/Desktop/2PDreamRec/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:243: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
Fold 3 Epoch 011; Train loss: 0.3574; Time: 00:00:02
Fold 3 Epoch 012; Train loss: 0.3478; Time: 00:00:01
Fold 3 Epoch 013; Train loss: 0.3470; Time: 00:00:01
Fold 3 Epoch 014; Train loss: 0.3378; Time: 00:00:01
Fold 3 Epoch 015; Train loss: 0.3287; Time: 00:00:01
Fold 3 Epoch 016; Train loss: 0.3249; Time: 00:00:01
Fold 3 Epoch 017; Train loss: 0.3159; Time: 00:00:01
Fold 3 Epoch 018; Train loss: 0.3121; Time: 00:00:01
Fold 3 Epoch 019; Train loss: 0.3074; Time: 00:00:02
Fold 3 Epoch 020; Train loss: 0.2999; Time: 00:00:02
Fold 3: Evaluation at Epoch 20
HR@5       NDCG@5     HR@10      NDCG@10   
0.726821   0.557353   0.801325   0.580849  
Loss: 0.3665
/home/nit/Desktop/2PDreamRec/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:243: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
Fold 3 Epoch 021; Train loss: 0.2979; Time: 00:00:01
Fold 3 Epoch 022; Train loss: 0.2944; Time: 00:00:01
Fold 3 Epoch 023; Train loss: 0.2857; Time: 00:00:01
Fold 3 Epoch 024; Train loss: 0.2890; Time: 00:00:02
Fold 3 Epoch 025; Train loss: 0.2818; Time: 00:00:02
Fold 3 Epoch 026; Train loss: 0.2782; Time: 00:00:02
Fold 3 Epoch 027; Train loss: 0.2725; Time: 00:00:01
Fold 3 Epoch 028; Train loss: 0.2767; Time: 00:00:01
Fold 3 Epoch 029; Train loss: 0.2667; Time: 00:00:01
Fold 3 Epoch 030; Train loss: 0.2608; Time: 00:00:02
Fold 3: Evaluation at Epoch 30
HR@5       NDCG@5     HR@10      NDCG@10   
0.753311   0.575244   0.836093   0.601609  
Loss: 0.3176
/home/nit/Desktop/2PDreamRec/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:243: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
Fold 3 Epoch 031; Train loss: 0.2635; Time: 00:00:01
Fold 3 Epoch 032; Train loss: 0.2571; Time: 00:00:01
Fold 3 Epoch 033; Train loss: 0.2555; Time: 00:00:01
Fold 3 Epoch 034; Train loss: 0.2490; Time: 00:00:02
Fold 3 Epoch 035; Train loss: 0.2468; Time: 00:00:02
Fold 3 Epoch 036; Train loss: 0.2445; Time: 00:00:01
Fold 3 Epoch 037; Train loss: 0.2402; Time: 00:00:01
Fold 3 Epoch 038; Train loss: 0.2425; Time: 00:00:01
Fold 3 Epoch 039; Train loss: 0.2379; Time: 00:00:01
Fold 3 Epoch 040; Train loss: 0.2281; Time: 00:00:01
Fold 3: Evaluation at Epoch 40
HR@5       NDCG@5     HR@10      NDCG@10   
0.672185   0.529866   0.841060   0.584132  
Loss: 0.2667
/home/nit/Desktop/2PDreamRec/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:243: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
Fold 3 Epoch 041; Train loss: 0.2313; Time: 00:00:01
Fold 3 Epoch 042; Train loss: 0.2265; Time: 00:00:01
Fold 3 Epoch 043; Train loss: 0.2244; Time: 00:00:01
Fold 3 Epoch 044; Train loss: 0.2213; Time: 00:00:01
Fold 3 Epoch 045; Train loss: 0.2185; Time: 00:00:01
Fold 3 Epoch 046; Train loss: 0.2152; Time: 00:00:01
Fold 3 Epoch 047; Train loss: 0.2173; Time: 00:00:02
Fold 3 Epoch 048; Train loss: 0.2108; Time: 00:00:01
Fold 3 Epoch 049; Train loss: 0.2104; Time: 00:00:01
Fold 3 Epoch 050; Train loss: 0.2009; Time: 00:00:01
Fold 3: Evaluation at Epoch 50
HR@5       NDCG@5     HR@10      NDCG@10   
0.706954   0.565059   0.827815   0.604522  
Loss: 0.2347
/home/nit/Desktop/2PDreamRec/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:243: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
Fold 3 Epoch 051; Train loss: 0.2018; Time: 00:00:02
Fold 3 Epoch 052; Train loss: 0.1992; Time: 00:00:01
Fold 3 Epoch 053; Train loss: 0.1978; Time: 00:00:01
Fold 3 Epoch 054; Train loss: 0.1978; Time: 00:00:02
Fold 3 Epoch 055; Train loss: 0.1902; Time: 00:00:01
Fold 3 Epoch 056; Train loss: 0.1913; Time: 00:00:01
Fold 3 Epoch 057; Train loss: 0.1860; Time: 00:00:02
Fold 3 Epoch 058; Train loss: 0.1834; Time: 00:00:01
Fold 3 Epoch 059; Train loss: 0.1831; Time: 00:00:02
Fold 3 Epoch 060; Train loss: 0.1797; Time: 00:00:02
Fold 3: Evaluation at Epoch 60
HR@5       NDCG@5     HR@10      NDCG@10   
0.753311   0.588197   0.852649   0.619725  
Loss: 0.2205
/home/nit/Desktop/2PDreamRec/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:243: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
Fold 3 Epoch 061; Train loss: 0.1778; Time: 00:00:01
Fold 3 Epoch 062; Train loss: 0.1745; Time: 00:00:02
Fold 3 Epoch 063; Train loss: 0.1717; Time: 00:00:01
Fold 3 Epoch 064; Train loss: 0.1703; Time: 00:00:01
Fold 3 Epoch 065; Train loss: 0.1678; Time: 00:00:01
Fold 3 Epoch 066; Train loss: 0.1650; Time: 00:00:01
Fold 3 Epoch 067; Train loss: 0.1653; Time: 00:00:01
Fold 3 Epoch 068; Train loss: 0.1608; Time: 00:00:02
Fold 3 Epoch 069; Train loss: 0.1575; Time: 00:00:02
Fold 3 Epoch 070; Train loss: 0.1597; Time: 00:00:02
Fold 3: Evaluation at Epoch 70
HR@5       NDCG@5     HR@10      NDCG@10   
0.687086   0.552971   0.832781   0.599852  
Loss: 0.1876
/home/nit/Desktop/2PDreamRec/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:243: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
Fold 3 Epoch 071; Train loss: 0.1539; Time: 00:00:01
Fold 3 Epoch 072; Train loss: 0.1544; Time: 00:00:01
Fold 3 Epoch 073; Train loss: 0.1517; Time: 00:00:01
Fold 3 Epoch 074; Train loss: 0.1483; Time: 00:00:01
Fold 3 Epoch 075; Train loss: 0.1473; Time: 00:00:01
Fold 3 Epoch 076; Train loss: 0.1453; Time: 00:00:01
Fold 3 Epoch 077; Train loss: 0.1450; Time: 00:00:02
Fold 3 Epoch 078; Train loss: 0.1411; Time: 00:00:01
Fold 3 Epoch 079; Train loss: 0.1389; Time: 00:00:01
Fold 3 Epoch 080; Train loss: 0.1374; Time: 00:00:01
Fold 3: Evaluation at Epoch 80
HR@5       NDCG@5     HR@10      NDCG@10   
0.721854   0.579232   0.855960   0.622675  
Loss: 0.1568
/home/nit/Desktop/2PDreamRec/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:243: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
[eps candidate 1e-08] Fold 3: NDCG@10 = [np.float64(0.5711539680381329), np.float64(0.5808489103416176), np.float64(0.6016090217942192), np.float64(0.5841317036283747), np.float64(0.6045219995266512), np.float64(0.6197246403786723), np.float64(0.5998515860606115), np.float64(0.6226754298857194)]
Tuning eps:  80%|████████  | 4/5 [10:58<02:43, 163.71s/it]
========== Fold 3 ==========
INFO:root:Using statics: seq_size = 10, genre_vocab_size = 18
Fold 3 Epoch 001; Train loss: 1.0339; Time: 00:00:01
Fold 3 Epoch 002; Train loss: 0.7759; Time: 00:00:01
Fold 3 Epoch 003; Train loss: 0.6456; Time: 00:00:01
Fold 3 Epoch 004; Train loss: 0.5732; Time: 00:00:01
Fold 3 Epoch 005; Train loss: 0.5190; Time: 00:00:01
Fold 3 Epoch 006; Train loss: 0.4742; Time: 00:00:02
Fold 3 Epoch 007; Train loss: 0.4508; Time: 00:00:01
Fold 3 Epoch 008; Train loss: 0.4244; Time: 00:00:01
Fold 3 Epoch 009; Train loss: 0.4187; Time: 00:00:01
Fold 3 Epoch 010; Train loss: 0.3976; Time: 00:00:02
Fold 3: Evaluation at Epoch 10
HR@5       NDCG@5     HR@10      NDCG@10   
0.625828   0.514636   0.750000   0.554574  
Loss: 0.4773
/home/nit/Desktop/2PDreamRec/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:243: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
Fold 3 Epoch 011; Train loss: 0.3858; Time: 00:00:02
Fold 3 Epoch 012; Train loss: 0.3787; Time: 00:00:01
Fold 3 Epoch 013; Train loss: 0.3775; Time: 00:00:02
Fold 3 Epoch 014; Train loss: 0.3568; Time: 00:00:02
Fold 3 Epoch 015; Train loss: 0.3553; Time: 00:00:02
Fold 3 Epoch 016; Train loss: 0.3445; Time: 00:00:02
Fold 3 Epoch 017; Train loss: 0.3496; Time: 00:00:02
Fold 3 Epoch 018; Train loss: 0.3323; Time: 00:00:01
Fold 3 Epoch 019; Train loss: 0.3293; Time: 00:00:01
Fold 3 Epoch 020; Train loss: 0.3251; Time: 00:00:01
Fold 3: Evaluation at Epoch 20
HR@5       NDCG@5     HR@10      NDCG@10   
0.620861   0.514858   0.745033   0.554183  
Loss: 0.4492
/home/nit/Desktop/2PDreamRec/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:243: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
Fold 3 Epoch 021; Train loss: 0.3220; Time: 00:00:01
Fold 3 Epoch 022; Train loss: 0.3162; Time: 00:00:01
Fold 3 Epoch 023; Train loss: 0.3165; Time: 00:00:01
Fold 3 Epoch 024; Train loss: 0.3185; Time: 00:00:01
Fold 3 Epoch 025; Train loss: 0.3071; Time: 00:00:01
Fold 3 Epoch 026; Train loss: 0.3040; Time: 00:00:01
Fold 3 Epoch 027; Train loss: 0.2978; Time: 00:00:02
Fold 3 Epoch 028; Train loss: 0.2900; Time: 00:00:02
Fold 3 Epoch 029; Train loss: 0.2964; Time: 00:00:01
Fold 3 Epoch 030; Train loss: 0.2861; Time: 00:00:01
Fold 3: Evaluation at Epoch 30
HR@5       NDCG@5     HR@10      NDCG@10   
0.619205   0.521555   0.751656   0.564141  
Loss: 0.3663
/home/nit/Desktop/2PDreamRec/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:243: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
Fold 3 Epoch 031; Train loss: 0.2825; Time: 00:00:02
Fold 3 Epoch 032; Train loss: 0.2809; Time: 00:00:01
Fold 3 Epoch 033; Train loss: 0.2816; Time: 00:00:02
Fold 3 Epoch 034; Train loss: 0.2739; Time: 00:00:02
Fold 3 Epoch 035; Train loss: 0.2713; Time: 00:00:02
Fold 3 Epoch 036; Train loss: 0.2659; Time: 00:00:02
Fold 3 Epoch 037; Train loss: 0.2603; Time: 00:00:02
Fold 3 Epoch 038; Train loss: 0.2595; Time: 00:00:02
Fold 3 Epoch 039; Train loss: 0.2580; Time: 00:00:02
Fold 3 Epoch 040; Train loss: 0.2486; Time: 00:00:01
Fold 3: Evaluation at Epoch 40
HR@5       NDCG@5     HR@10      NDCG@10   
0.620861   0.517958   0.751656   0.559785  
Loss: 0.3096
/home/nit/Desktop/2PDreamRec/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:243: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
Fold 3 Epoch 041; Train loss: 0.2512; Time: 00:00:02
Fold 3 Epoch 042; Train loss: 0.2512; Time: 00:00:02
Fold 3 Epoch 043; Train loss: 0.2434; Time: 00:00:01
Fold 3 Epoch 044; Train loss: 0.2451; Time: 00:00:01
Fold 3 Epoch 045; Train loss: 0.2395; Time: 00:00:01
Fold 3 Epoch 046; Train loss: 0.2395; Time: 00:00:01
Fold 3 Epoch 047; Train loss: 0.2300; Time: 00:00:02
Fold 3 Epoch 048; Train loss: 0.2278; Time: 00:00:02
Fold 3 Epoch 049; Train loss: 0.2279; Time: 00:00:02
Fold 3 Epoch 050; Train loss: 0.2270; Time: 00:00:01
Fold 3: Evaluation at Epoch 50
HR@5       NDCG@5     HR@10      NDCG@10   
0.596026   0.494171   0.759934   0.546545  
Loss: 0.2630
/home/nit/Desktop/2PDreamRec/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:243: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
Fold 3 Epoch 051; Train loss: 0.2226; Time: 00:00:01
Fold 3 Epoch 052; Train loss: 0.2184; Time: 00:00:01
Fold 3 Epoch 053; Train loss: 0.2105; Time: 00:00:02
Fold 3 Epoch 054; Train loss: 0.2133; Time: 00:00:01
Fold 3 Epoch 055; Train loss: 0.2083; Time: 00:00:01
Fold 3 Epoch 056; Train loss: 0.2091; Time: 00:00:02
Fold 3 Epoch 057; Train loss: 0.2050; Time: 00:00:01
Fold 3 Epoch 058; Train loss: 0.1993; Time: 00:00:01
Fold 3 Epoch 059; Train loss: 0.2014; Time: 00:00:01
Fold 3 Epoch 060; Train loss: 0.1968; Time: 00:00:01
Fold 3: Evaluation at Epoch 60
HR@5       NDCG@5     HR@10      NDCG@10   
0.642384   0.538038   0.753311   0.572654  
Loss: 0.2726
/home/nit/Desktop/2PDreamRec/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:243: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
Fold 3 Epoch 061; Train loss: 0.1987; Time: 00:00:01
Fold 3 Epoch 062; Train loss: 0.1960; Time: 00:00:01
Fold 3 Epoch 063; Train loss: 0.1888; Time: 00:00:02
Fold 3 Epoch 064; Train loss: 0.1871; Time: 00:00:02
Fold 3 Epoch 065; Train loss: 0.1850; Time: 00:00:01
Fold 3 Epoch 066; Train loss: 0.1810; Time: 00:00:01
Fold 3 Epoch 067; Train loss: 0.1826; Time: 00:00:01
Fold 3 Epoch 068; Train loss: 0.1756; Time: 00:00:01
Fold 3 Epoch 069; Train loss: 0.1765; Time: 00:00:02
Fold 3 Epoch 070; Train loss: 0.1723; Time: 00:00:02
Fold 3: Evaluation at Epoch 70
HR@5       NDCG@5     HR@10      NDCG@10   
0.610927   0.513411   0.745033   0.556883  
Loss: 0.2143
/home/nit/Desktop/2PDreamRec/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:243: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
Fold 3 Epoch 071; Train loss: 0.1714; Time: 00:00:01
Fold 3 Epoch 072; Train loss: 0.1685; Time: 00:00:02
Fold 3 Epoch 073; Train loss: 0.1648; Time: 00:00:01
Fold 3 Epoch 074; Train loss: 0.1668; Time: 00:00:01
Fold 3 Epoch 075; Train loss: 0.1611; Time: 00:00:01
Fold 3 Epoch 076; Train loss: 0.1652; Time: 00:00:01
Fold 3 Epoch 077; Train loss: 0.1590; Time: 00:00:01
Fold 3 Epoch 078; Train loss: 0.1565; Time: 00:00:02
Fold 3 Epoch 079; Train loss: 0.1551; Time: 00:00:03
Fold 3 Epoch 080; Train loss: 0.1519; Time: 00:00:02
Fold 3: Evaluation at Epoch 80
HR@5       NDCG@5     HR@10      NDCG@10   
0.617550   0.513601   0.748344   0.555777  
Loss: 0.2000
/home/nit/Desktop/2PDreamRec/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:243: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
[eps candidate 1e-16] Fold 3: NDCG@10 = [np.float64(0.5545735850622363), np.float64(0.5541830178465516), np.float64(0.5641411809482476), np.float64(0.5597845158961197), np.float64(0.5465445717125463), np.float64(0.5726536888983574), np.float64(0.5568829401969628), np.float64(0.5557770734038172)]
Tuning eps: 100%|██████████| 5/5 [13:52<00:00, 167.40s/it]Tuning eps: 100%|██████████| 5/5 [13:52<00:00, 166.41s/it]

Best eps found: 1e-08
Tuning scheduler:   0%|          | 0/3 [00:00<?, ?it/s]
========== Fold 3 ==========
INFO:root:Using statics: seq_size = 10, genre_vocab_size = 18
Fold 3 Epoch 001; Train loss: 1.1309; Time: 00:00:01
Fold 3 Epoch 002; Train loss: 0.8216; Time: 00:00:02
Fold 3 Epoch 003; Train loss: 0.6964; Time: 00:00:02
Fold 3 Epoch 004; Train loss: 0.6011; Time: 00:00:01
Fold 3 Epoch 005; Train loss: 0.5471; Time: 00:00:01
Fold 3 Epoch 006; Train loss: 0.5053; Time: 00:00:01
Fold 3 Epoch 007; Train loss: 0.4786; Time: 00:00:01
Fold 3 Epoch 008; Train loss: 0.4657; Time: 00:00:01
Fold 3 Epoch 009; Train loss: 0.4378; Time: 00:00:01
Fold 3 Epoch 010; Train loss: 0.4378; Time: 00:00:01
Fold 3: Evaluation at Epoch 10
HR@5       NDCG@5     HR@10      NDCG@10   
0.754967   0.542369   0.855960   0.576131  
Loss: 0.4922
/home/nit/Desktop/2PDreamRec/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:243: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
Fold 3 Epoch 011; Train loss: 0.4181; Time: 00:00:02
Fold 3 Epoch 012; Train loss: 0.4107; Time: 00:00:02
Fold 3 Epoch 013; Train loss: 0.3967; Time: 00:00:01
Fold 3 Epoch 014; Train loss: 0.3968; Time: 00:00:02
Fold 3 Epoch 015; Train loss: 0.3827; Time: 00:00:01
Fold 3 Epoch 016; Train loss: 0.3729; Time: 00:00:02
Fold 3 Epoch 017; Train loss: 0.3724; Time: 00:00:01
Fold 3 Epoch 018; Train loss: 0.3605; Time: 00:00:02
Fold 3 Epoch 019; Train loss: 0.3555; Time: 00:00:01
Fold 3 Epoch 020; Train loss: 0.3469; Time: 00:00:01
Fold 3: Evaluation at Epoch 20
HR@5       NDCG@5     HR@10      NDCG@10   
0.703642   0.541267   0.821192   0.579303  
Loss: 0.3970
/home/nit/Desktop/2PDreamRec/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:243: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
Fold 3 Epoch 021; Train loss: 0.3519; Time: 00:00:02
Fold 3 Epoch 022; Train loss: 0.3423; Time: 00:00:02
Fold 3 Epoch 023; Train loss: 0.3393; Time: 00:00:01
Fold 3 Epoch 024; Train loss: 0.3342; Time: 00:00:01
Fold 3 Epoch 025; Train loss: 0.3304; Time: 00:00:02
Fold 3 Epoch 026; Train loss: 0.3252; Time: 00:00:02
Fold 3 Epoch 027; Train loss: 0.3222; Time: 00:00:01
Fold 3 Epoch 028; Train loss: 0.3244; Time: 00:00:01
Fold 3 Epoch 029; Train loss: 0.3127; Time: 00:00:01
Fold 3 Epoch 030; Train loss: 0.3040; Time: 00:00:02
Fold 3: Evaluation at Epoch 30
HR@5       NDCG@5     HR@10      NDCG@10   
0.789735   0.564419   0.867550   0.589364  
Loss: 0.3423
/home/nit/Desktop/2PDreamRec/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:243: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
Fold 3 Epoch 031; Train loss: 0.3008; Time: 00:00:01
Fold 3 Epoch 032; Train loss: 0.3004; Time: 00:00:01
Fold 3 Epoch 033; Train loss: 0.2919; Time: 00:00:01
Fold 3 Epoch 034; Train loss: 0.2949; Time: 00:00:01
Fold 3 Epoch 035; Train loss: 0.2894; Time: 00:00:02
Fold 3 Epoch 036; Train loss: 0.2880; Time: 00:00:01
Fold 3 Epoch 037; Train loss: 0.2846; Time: 00:00:02
Fold 3 Epoch 038; Train loss: 0.2754; Time: 00:00:02
Fold 3 Epoch 039; Train loss: 0.2784; Time: 00:00:02
Fold 3 Epoch 040; Train loss: 0.2696; Time: 00:00:01
Fold 3: Evaluation at Epoch 40
HR@5       NDCG@5     HR@10      NDCG@10   
0.791391   0.589244   0.879139   0.617664  
Loss: 0.3198
/home/nit/Desktop/2PDreamRec/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:243: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
Fold 3 Epoch 041; Train loss: 0.2670; Time: 00:00:02
Fold 3 Epoch 042; Train loss: 0.2669; Time: 00:00:01
Fold 3 Epoch 043; Train loss: 0.2601; Time: 00:00:01
Fold 3 Epoch 044; Train loss: 0.2583; Time: 00:00:02
Fold 3 Epoch 045; Train loss: 0.2557; Time: 00:00:01
Fold 3 Epoch 046; Train loss: 0.2486; Time: 00:00:02
Fold 3 Epoch 047; Train loss: 0.2516; Time: 00:00:02
Fold 3 Epoch 048; Train loss: 0.2501; Time: 00:00:01
Fold 3 Epoch 049; Train loss: 0.2467; Time: 00:00:02
Fold 3 Epoch 050; Train loss: 0.2358; Time: 00:00:01
Fold 3: Evaluation at Epoch 50
HR@5       NDCG@5     HR@10      NDCG@10   
0.778146   0.558448   0.874172   0.589241  
Loss: 0.2705
/home/nit/Desktop/2PDreamRec/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:243: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
Fold 3 Epoch 051; Train loss: 0.2373; Time: 00:00:01
Fold 3 Epoch 052; Train loss: 0.2343; Time: 00:00:02
Fold 3 Epoch 053; Train loss: 0.2353; Time: 00:00:02
Fold 3 Epoch 054; Train loss: 0.2259; Time: 00:00:01
Fold 3 Epoch 055; Train loss: 0.2306; Time: 00:00:02
Fold 3 Epoch 056; Train loss: 0.2244; Time: 00:00:02
Fold 3 Epoch 057; Train loss: 0.2200; Time: 00:00:01
Fold 3 Epoch 058; Train loss: 0.2182; Time: 00:00:01
Fold 3 Epoch 059; Train loss: 0.2137; Time: 00:00:01
Fold 3 Epoch 060; Train loss: 0.2098; Time: 00:00:01
Fold 3: Evaluation at Epoch 60
HR@5       NDCG@5     HR@10      NDCG@10   
0.783113   0.571336   0.872517   0.599973  
Loss: 0.2277
/home/nit/Desktop/2PDreamRec/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:243: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
Fold 3 Epoch 061; Train loss: 0.2098; Time: 00:00:01
Fold 3 Epoch 062; Train loss: 0.2090; Time: 00:00:02
Fold 3 Epoch 063; Train loss: 0.2006; Time: 00:00:01
Fold 3 Epoch 064; Train loss: 0.1997; Time: 00:00:01
Fold 3 Epoch 065; Train loss: 0.1979; Time: 00:00:01
Fold 3 Epoch 066; Train loss: 0.1961; Time: 00:00:01
Fold 3 Epoch 067; Train loss: 0.1927; Time: 00:00:01
Fold 3 Epoch 068; Train loss: 0.1951; Time: 00:00:02
Fold 3 Epoch 069; Train loss: 0.1891; Time: 00:00:01
Fold 3 Epoch 070; Train loss: 0.1863; Time: 00:00:01
Fold 3: Evaluation at Epoch 70
HR@5       NDCG@5     HR@10      NDCG@10   
0.721854   0.545169   0.837748   0.582875  
Loss: 0.2190
/home/nit/Desktop/2PDreamRec/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:243: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
Fold 3 Epoch 071; Train loss: 0.1869; Time: 00:00:01
Fold 3 Epoch 072; Train loss: 0.1809; Time: 00:00:02
Fold 3 Epoch 073; Train loss: 0.1781; Time: 00:00:01
Fold 3 Epoch 074; Train loss: 0.1772; Time: 00:00:02
Fold 3 Epoch 075; Train loss: 0.1761; Time: 00:00:01
Fold 3 Epoch 076; Train loss: 0.1708; Time: 00:00:01
Fold 3 Epoch 077; Train loss: 0.1690; Time: 00:00:01
Fold 3 Epoch 078; Train loss: 0.1703; Time: 00:00:02
Fold 3 Epoch 079; Train loss: 0.1666; Time: 00:00:02
Fold 3 Epoch 080; Train loss: 0.1615; Time: 00:00:01
Fold 3: Evaluation at Epoch 80
HR@5       NDCG@5     HR@10      NDCG@10   
0.764901   0.564433   0.855960   0.594138  
Loss: 0.1922
/home/nit/Desktop/2PDreamRec/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:243: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
[scheduler candidate reduce_on_plateau] Fold 3: NDCG@10 = [np.float64(0.576130634767619), np.float64(0.5793031940878348), np.float64(0.5893635133018332), np.float64(0.6176639384446116), np.float64(0.5892407819804436), np.float64(0.5999732512816579), np.float64(0.5828745616257242), np.float64(0.5941378208053432)]
Tuning scheduler:  33%|███▎      | 1/3 [02:48<05:37, 168.75s/it]
========== Fold 3 ==========
INFO:root:Using statics: seq_size = 10, genre_vocab_size = 18
Fold 3 Epoch 001; Train loss: 0.9224; Time: 00:00:02
Fold 3 Epoch 002; Train loss: 0.6857; Time: 00:00:01
Fold 3 Epoch 003; Train loss: 0.5724; Time: 00:00:01
Fold 3 Epoch 004; Train loss: 0.5050; Time: 00:00:02
Fold 3 Epoch 005; Train loss: 0.4569; Time: 00:00:01
Fold 3 Epoch 006; Train loss: 0.4227; Time: 00:00:01
Fold 3 Epoch 007; Train loss: 0.4028; Time: 00:00:02
Fold 3 Epoch 008; Train loss: 0.3822; Time: 00:00:02
Fold 3 Epoch 009; Train loss: 0.3707; Time: 00:00:01
Fold 3 Epoch 010; Train loss: 0.3617; Time: 00:00:01
Fold 3: Evaluation at Epoch 10
HR@5       NDCG@5     HR@10      NDCG@10   
0.609272   0.471068   0.822848   0.541326  
Loss: 0.4572
/home/nit/Desktop/2PDreamRec/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:243: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
Fold 3 Epoch 011; Train loss: 0.3398; Time: 00:00:01
Fold 3 Epoch 012; Train loss: 0.3400; Time: 00:00:02
Fold 3 Epoch 013; Train loss: 0.3343; Time: 00:00:01
Fold 3 Epoch 014; Train loss: 0.3278; Time: 00:00:01
Fold 3 Epoch 015; Train loss: 0.3202; Time: 00:00:02
Fold 3 Epoch 016; Train loss: 0.3112; Time: 00:00:02
Fold 3 Epoch 017; Train loss: 0.3099; Time: 00:00:01
Fold 3 Epoch 018; Train loss: 0.3086; Time: 00:00:01
Fold 3 Epoch 019; Train loss: 0.2954; Time: 00:00:01
Fold 3 Epoch 020; Train loss: 0.2960; Time: 00:00:01
Fold 3: Evaluation at Epoch 20
HR@5       NDCG@5     HR@10      NDCG@10   
0.652318   0.509710   0.847682   0.574124  
Loss: 0.3657
/home/nit/Desktop/2PDreamRec/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:243: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
Fold 3 Epoch 021; Train loss: 0.2883; Time: 00:00:02
Fold 3 Epoch 022; Train loss: 0.2810; Time: 00:00:01
Fold 3 Epoch 023; Train loss: 0.2779; Time: 00:00:01
Fold 3 Epoch 024; Train loss: 0.2763; Time: 00:00:01
Fold 3 Epoch 025; Train loss: 0.2725; Time: 00:00:01
Fold 3 Epoch 026; Train loss: 0.2670; Time: 00:00:02
Fold 3 Epoch 027; Train loss: 0.2663; Time: 00:00:02
Fold 3 Epoch 028; Train loss: 0.2629; Time: 00:00:01
Fold 3 Epoch 029; Train loss: 0.2612; Time: 00:00:01
Fold 3 Epoch 030; Train loss: 0.2541; Time: 00:00:01
Fold 3: Evaluation at Epoch 30
HR@5       NDCG@5     HR@10      NDCG@10   
0.698675   0.544100   0.819536   0.584144  
Loss: 0.3447
/home/nit/Desktop/2PDreamRec/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:243: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
Fold 3 Epoch 031; Train loss: 0.2538; Time: 00:00:01
Fold 3 Epoch 032; Train loss: 0.2459; Time: 00:00:01
Fold 3 Epoch 033; Train loss: 0.2473; Time: 00:00:01
Fold 3 Epoch 034; Train loss: 0.2431; Time: 00:00:02
Fold 3 Epoch 035; Train loss: 0.2394; Time: 00:00:01
Fold 3 Epoch 036; Train loss: 0.2376; Time: 00:00:02
Fold 3 Epoch 037; Train loss: 0.2323; Time: 00:00:02
Fold 3 Epoch 038; Train loss: 0.2316; Time: 00:00:02
Fold 3 Epoch 039; Train loss: 0.2271; Time: 00:00:01
Fold 3 Epoch 040; Train loss: 0.2251; Time: 00:00:01
Fold 3: Evaluation at Epoch 40
HR@5       NDCG@5     HR@10      NDCG@10   
0.675497   0.530234   0.811258   0.575320  
Loss: 0.2893
/home/nit/Desktop/2PDreamRec/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:243: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
Fold 3 Epoch 041; Train loss: 0.2180; Time: 00:00:01
Fold 3 Epoch 042; Train loss: 0.2183; Time: 00:00:01
Fold 3 Epoch 043; Train loss: 0.2130; Time: 00:00:02
Fold 3 Epoch 044; Train loss: 0.2129; Time: 00:00:01
Fold 3 Epoch 045; Train loss: 0.2079; Time: 00:00:01
Fold 3 Epoch 046; Train loss: 0.2092; Time: 00:00:01
Fold 3 Epoch 047; Train loss: 0.2075; Time: 00:00:02
Fold 3 Epoch 048; Train loss: 0.1997; Time: 00:00:01
Fold 3 Epoch 049; Train loss: 0.2018; Time: 00:00:02
Fold 3 Epoch 050; Train loss: 0.1953; Time: 00:00:01
Fold 3: Evaluation at Epoch 50
HR@5       NDCG@5     HR@10      NDCG@10   
0.731788   0.571433   0.822848   0.601270  
Loss: 0.2584
/home/nit/Desktop/2PDreamRec/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:243: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
Fold 3 Epoch 051; Train loss: 0.1970; Time: 00:00:01
Fold 3 Epoch 052; Train loss: 0.1900; Time: 00:00:01
Fold 3 Epoch 053; Train loss: 0.1927; Time: 00:00:01
Fold 3 Epoch 054; Train loss: 0.1871; Time: 00:00:01
Fold 3 Epoch 055; Train loss: 0.1809; Time: 00:00:01
Fold 3 Epoch 056; Train loss: 0.1797; Time: 00:00:01
Fold 3 Epoch 057; Train loss: 0.1765; Time: 00:00:01
Fold 3 Epoch 058; Train loss: 0.1737; Time: 00:00:01
Fold 3 Epoch 059; Train loss: 0.1713; Time: 00:00:01
Fold 3 Epoch 060; Train loss: 0.1752; Time: 00:00:01
Fold 3: Evaluation at Epoch 60
HR@5       NDCG@5     HR@10      NDCG@10   
0.761589   0.586274   0.841060   0.611948  
Loss: 0.2227
/home/nit/Desktop/2PDreamRec/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:243: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
Fold 3 Epoch 061; Train loss: 0.1669; Time: 00:00:01
Fold 3 Epoch 062; Train loss: 0.1657; Time: 00:00:01
Fold 3 Epoch 063; Train loss: 0.1638; Time: 00:00:01
Fold 3 Epoch 064; Train loss: 0.1647; Time: 00:00:01
Fold 3 Epoch 065; Train loss: 0.1565; Time: 00:00:01
Fold 3 Epoch 066; Train loss: 0.1565; Time: 00:00:01
Fold 3 Epoch 067; Train loss: 0.1551; Time: 00:00:02
Fold 3 Epoch 068; Train loss: 0.1534; Time: 00:00:02
Fold 3 Epoch 069; Train loss: 0.1555; Time: 00:00:02
Fold 3 Epoch 070; Train loss: 0.1467; Time: 00:00:02
Fold 3: Evaluation at Epoch 70
HR@5       NDCG@5     HR@10      NDCG@10   
0.736755   0.568554   0.812914   0.593115  
Loss: 0.2165
/home/nit/Desktop/2PDreamRec/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:243: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
Fold 3 Epoch 071; Train loss: 0.1473; Time: 00:00:01
Fold 3 Epoch 072; Train loss: 0.1454; Time: 00:00:01
Fold 3 Epoch 073; Train loss: 0.1427; Time: 00:00:02
Fold 3 Epoch 074; Train loss: 0.1412; Time: 00:00:02
Fold 3 Epoch 075; Train loss: 0.1393; Time: 00:00:01
Fold 3 Epoch 076; Train loss: 0.1352; Time: 00:00:01
Fold 3 Epoch 077; Train loss: 0.1355; Time: 00:00:01
Fold 3 Epoch 078; Train loss: 0.1326; Time: 00:00:01
Fold 3 Epoch 079; Train loss: 0.1317; Time: 00:00:02
Fold 3 Epoch 080; Train loss: 0.1279; Time: 00:00:01
Fold 3: Evaluation at Epoch 80
HR@5       NDCG@5     HR@10      NDCG@10   
0.768212   0.589045   0.846026   0.614438  
Loss: 0.1628
/home/nit/Desktop/2PDreamRec/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:243: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
[scheduler candidate cosine] Fold 3: NDCG@10 = [np.float64(0.5413261538083634), np.float64(0.5741238915400287), np.float64(0.5841444755641022), np.float64(0.5753195129722142), np.float64(0.6012695958362382), np.float64(0.6119478316572468), np.float64(0.5931148548321081), np.float64(0.6144384349693149)]
Tuning scheduler:  67%|██████▋   | 2/3 [05:32<02:45, 165.65s/it]
========== Fold 3 ==========
INFO:root:Using statics: seq_size = 10, genre_vocab_size = 18
Fold 3 Epoch 001; Train loss: 0.9460; Time: 00:00:01
Fold 3 Epoch 002; Train loss: 0.6973; Time: 00:00:02
Fold 3 Epoch 003; Train loss: 0.5857; Time: 00:00:02
Fold 3 Epoch 004; Train loss: 0.5139; Time: 00:00:01
Fold 3 Epoch 005; Train loss: 0.4587; Time: 00:00:01
Fold 3 Epoch 006; Train loss: 0.4336; Time: 00:00:01
Fold 3 Epoch 007; Train loss: 0.3967; Time: 00:00:01
Fold 3 Epoch 008; Train loss: 0.3858; Time: 00:00:02
Fold 3 Epoch 009; Train loss: 0.3688; Time: 00:00:02
Fold 3 Epoch 010; Train loss: 0.3556; Time: 00:00:02
Fold 3: Evaluation at Epoch 10
HR@5       NDCG@5     HR@10      NDCG@10   
0.620861   0.476353   0.763245   0.521857  
Loss: 0.4638
/home/nit/Desktop/2PDreamRec/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:243: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
Fold 3 Epoch 011; Train loss: 0.3391; Time: 00:00:01
Fold 3 Epoch 012; Train loss: 0.3321; Time: 00:00:02
Fold 3 Epoch 013; Train loss: 0.3198; Time: 00:00:01
Fold 3 Epoch 014; Train loss: 0.3196; Time: 00:00:02
Fold 3 Epoch 015; Train loss: 0.3155; Time: 00:00:01
Fold 3 Epoch 016; Train loss: 0.3085; Time: 00:00:01
Fold 3 Epoch 017; Train loss: 0.3047; Time: 00:00:01
Fold 3 Epoch 018; Train loss: 0.2982; Time: 00:00:01
Fold 3 Epoch 019; Train loss: 0.2875; Time: 00:00:01
Fold 3 Epoch 020; Train loss: 0.2793; Time: 00:00:02
Fold 3: Evaluation at Epoch 20
HR@5       NDCG@5     HR@10      NDCG@10   
0.594371   0.445558   0.756623   0.497380  
Loss: 0.3649
/home/nit/Desktop/2PDreamRec/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:243: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
Fold 3 Epoch 021; Train loss: 0.2807; Time: 00:00:01
Fold 3 Epoch 022; Train loss: 0.2741; Time: 00:00:01
Fold 3 Epoch 023; Train loss: 0.2773; Time: 00:00:01
Fold 3 Epoch 024; Train loss: 0.2663; Time: 00:00:02
Fold 3 Epoch 025; Train loss: 0.2675; Time: 00:00:01
Fold 3 Epoch 026; Train loss: 0.2591; Time: 00:00:01
Fold 3 Epoch 027; Train loss: 0.2591; Time: 00:00:02
Fold 3 Epoch 028; Train loss: 0.2544; Time: 00:00:01
Fold 3 Epoch 029; Train loss: 0.2481; Time: 00:00:02
Fold 3 Epoch 030; Train loss: 0.2452; Time: 00:00:02
Fold 3: Evaluation at Epoch 30
HR@5       NDCG@5     HR@10      NDCG@10   
0.617550   0.475209   0.781457   0.527282  
Loss: 0.3050
/home/nit/Desktop/2PDreamRec/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:243: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
Fold 3 Epoch 031; Train loss: 0.2453; Time: 00:00:02
Fold 3 Epoch 032; Train loss: 0.2384; Time: 00:00:01
Fold 3 Epoch 033; Train loss: 0.2366; Time: 00:00:02
Fold 3 Epoch 034; Train loss: 0.2321; Time: 00:00:02
Fold 3 Epoch 035; Train loss: 0.2363; Time: 00:00:02
Fold 3 Epoch 036; Train loss: 0.2294; Time: 00:00:02
Fold 3 Epoch 037; Train loss: 0.2274; Time: 00:00:02
Fold 3 Epoch 038; Train loss: 0.2240; Time: 00:00:01
Fold 3 Epoch 039; Train loss: 0.2208; Time: 00:00:02
Fold 3 Epoch 040; Train loss: 0.2163; Time: 00:00:02
Fold 3: Evaluation at Epoch 40
HR@5       NDCG@5     HR@10      NDCG@10   
0.587748   0.440975   0.730132   0.486826  
Loss: 0.2888
/home/nit/Desktop/2PDreamRec/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:243: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
Fold 3 Epoch 041; Train loss: 0.2156; Time: 00:00:01
Fold 3 Epoch 042; Train loss: 0.2102; Time: 00:00:01
Fold 3 Epoch 043; Train loss: 0.2099; Time: 00:00:01
Fold 3 Epoch 044; Train loss: 0.2082; Time: 00:00:02
Fold 3 Epoch 045; Train loss: 0.2096; Time: 00:00:01
Fold 3 Epoch 046; Train loss: 0.2021; Time: 00:00:02
Fold 3 Epoch 047; Train loss: 0.1984; Time: 00:00:02
Fold 3 Epoch 048; Train loss: 0.1972; Time: 00:00:01
Fold 3 Epoch 049; Train loss: 0.1941; Time: 00:00:01
Fold 3 Epoch 050; Train loss: 0.1912; Time: 00:00:02
Fold 3: Evaluation at Epoch 50
HR@5       NDCG@5     HR@10      NDCG@10   
0.597682   0.463397   0.740066   0.509747  
Loss: 0.2449
/home/nit/Desktop/2PDreamRec/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:243: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
Fold 3 Epoch 051; Train loss: 0.1860; Time: 00:00:01
Fold 3 Epoch 052; Train loss: 0.1853; Time: 00:00:01
Fold 3 Epoch 053; Train loss: 0.1865; Time: 00:00:01
Fold 3 Epoch 054; Train loss: 0.1812; Time: 00:00:01
Fold 3 Epoch 055; Train loss: 0.1773; Time: 00:00:01
Fold 3 Epoch 056; Train loss: 0.1769; Time: 00:00:01
Fold 3 Epoch 057; Train loss: 0.1721; Time: 00:00:01
Fold 3 Epoch 058; Train loss: 0.1694; Time: 00:00:02
Fold 3 Epoch 059; Train loss: 0.1687; Time: 00:00:02
Fold 3 Epoch 060; Train loss: 0.1698; Time: 00:00:02
Fold 3: Evaluation at Epoch 60
HR@5       NDCG@5     HR@10      NDCG@10   
0.572848   0.439975   0.735099   0.492516  
Loss: 0.2255
/home/nit/Desktop/2PDreamRec/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:243: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
Fold 3 Epoch 061; Train loss: 0.1641; Time: 00:00:01
Fold 3 Epoch 062; Train loss: 0.1645; Time: 00:00:01
Fold 3 Epoch 063; Train loss: 0.1609; Time: 00:00:01
Fold 3 Epoch 064; Train loss: 0.1601; Time: 00:00:01
Fold 3 Epoch 065; Train loss: 0.1550; Time: 00:00:02
Fold 3 Epoch 066; Train loss: 0.1555; Time: 00:00:01
Fold 3 Epoch 067; Train loss: 0.1522; Time: 00:00:01
Fold 3 Epoch 068; Train loss: 0.1514; Time: 00:00:01
Fold 3 Epoch 069; Train loss: 0.1457; Time: 00:00:01
Fold 3 Epoch 070; Train loss: 0.1429; Time: 00:00:02
Fold 3: Evaluation at Epoch 70
HR@5       NDCG@5     HR@10      NDCG@10   
0.567881   0.437259   0.728477   0.488671  
Loss: 0.1840
/home/nit/Desktop/2PDreamRec/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:243: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
Fold 3 Epoch 071; Train loss: 0.1433; Time: 00:00:02
Fold 3 Epoch 072; Train loss: 0.1420; Time: 00:00:01
Fold 3 Epoch 073; Train loss: 0.1417; Time: 00:00:01
Fold 3 Epoch 074; Train loss: 0.1353; Time: 00:00:02
Fold 3 Epoch 075; Train loss: 0.1365; Time: 00:00:01
Fold 3 Epoch 076; Train loss: 0.1334; Time: 00:00:02
Fold 3 Epoch 077; Train loss: 0.1318; Time: 00:00:01
Fold 3 Epoch 078; Train loss: 0.1275; Time: 00:00:01
Fold 3 Epoch 079; Train loss: 0.1283; Time: 00:00:02
Fold 3 Epoch 080; Train loss: 0.1245; Time: 00:00:02
Fold 3: Evaluation at Epoch 80
HR@5       NDCG@5     HR@10      NDCG@10   
0.561258   0.430242   0.708609   0.477766  
Loss: 0.1771
/home/nit/Desktop/2PDreamRec/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:243: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
[scheduler candidate step] Fold 3: NDCG@10 = [np.float64(0.5218565886548412), np.float64(0.4973803547057405), np.float64(0.5272822515214777), np.float64(0.4868255800988708), np.float64(0.5097469793933066), np.float64(0.4925161550121689), np.float64(0.48867105365732927), np.float64(0.4777664006196437)]
Tuning scheduler: 100%|██████████| 3/3 [08:21<00:00, 167.34s/it]Tuning scheduler: 100%|██████████| 3/3 [08:21<00:00, 167.19s/it]

Best scheduler found: reduce_on_plateau
Tuning data for lr saved to ./category/tuning_lr.json
Tuning data for optimizer saved to ./category/tuning_optimizer.json
Tuning data for timesteps saved to ./category/tuning_timesteps.json
Tuning data for dropout_rate saved to ./category/tuning_dropout.json
Tuning data for l2_decay saved to ./category/tuning_l2.json
Tuning data for eps saved to ./category/tuning_eps.json
Tuning data for scheduler saved to ./category/tuning_scheduler.json

========== Fold 3 ==========
INFO:root:Using statics: seq_size = 10, genre_vocab_size = 18
Fold 3 Epoch 001; Train loss: 1.0217; Time: 00:00:01
Fold 3 Epoch 002; Train loss: 0.7255; Time: 00:00:01
Fold 3 Epoch 003; Train loss: 0.6013; Time: 00:00:02
Fold 3 Epoch 004; Train loss: 0.5227; Time: 00:00:02
Fold 3 Epoch 005; Train loss: 0.4782; Time: 00:00:02
Fold 3 Epoch 006; Train loss: 0.4386; Time: 00:00:02
Fold 3 Epoch 007; Train loss: 0.4105; Time: 00:00:01
Fold 3 Epoch 008; Train loss: 0.3957; Time: 00:00:02
Fold 3 Epoch 009; Train loss: 0.3779; Time: 00:00:02
Fold 3 Epoch 010; Train loss: 0.3645; Time: 00:00:02
Fold 3: Evaluation at Epoch 10
HR@5       NDCG@5     HR@10      NDCG@10   
0.599338   0.485063   0.745033   0.531364  
Loss: 0.4588
/home/nit/Desktop/2PDreamRec/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:243: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
Fold 3 Epoch 011; Train loss: 0.3466; Time: 00:00:01
Fold 3 Epoch 012; Train loss: 0.3411; Time: 00:00:01
Fold 3 Epoch 013; Train loss: 0.3304; Time: 00:00:01
Fold 3 Epoch 014; Train loss: 0.3296; Time: 00:00:02
Fold 3 Epoch 015; Train loss: 0.3164; Time: 00:00:01
Fold 3 Epoch 016; Train loss: 0.3118; Time: 00:00:01
Fold 3 Epoch 017; Train loss: 0.3072; Time: 00:00:02
Fold 3 Epoch 018; Train loss: 0.2936; Time: 00:00:01
Fold 3 Epoch 019; Train loss: 0.2978; Time: 00:00:01
Fold 3 Epoch 020; Train loss: 0.2912; Time: 00:00:01
Fold 3: Evaluation at Epoch 20
HR@5       NDCG@5     HR@10      NDCG@10   
0.571192   0.489063   0.728477   0.539322  
Loss: 0.3786
/home/nit/Desktop/2PDreamRec/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:243: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
Fold 3 Epoch 021; Train loss: 0.2840; Time: 00:00:01
Fold 3 Epoch 022; Train loss: 0.2810; Time: 00:00:01
Fold 3 Epoch 023; Train loss: 0.2801; Time: 00:00:02
Fold 3 Epoch 024; Train loss: 0.2762; Time: 00:00:02
Fold 3 Epoch 025; Train loss: 0.2714; Time: 00:00:02
Fold 3 Epoch 026; Train loss: 0.2678; Time: 00:00:01
Fold 3 Epoch 027; Train loss: 0.2617; Time: 00:00:01
Fold 3 Epoch 028; Train loss: 0.2536; Time: 00:00:01
Fold 3 Epoch 029; Train loss: 0.2538; Time: 00:00:02
Fold 3 Epoch 030; Train loss: 0.2501; Time: 00:00:01
Fold 3: Evaluation at Epoch 30
HR@5       NDCG@5     HR@10      NDCG@10   
0.564570   0.480520   0.713576   0.527894  
Loss: 0.3042
/home/nit/Desktop/2PDreamRec/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:243: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
Fold 3 Epoch 031; Train loss: 0.2547; Time: 00:00:02
Fold 3 Epoch 032; Train loss: 0.2464; Time: 00:00:01
Fold 3 Epoch 033; Train loss: 0.2423; Time: 00:00:02
Fold 3 Epoch 034; Train loss: 0.2391; Time: 00:00:01
Fold 3 Epoch 035; Train loss: 0.2353; Time: 00:00:01
Fold 3 Epoch 036; Train loss: 0.2310; Time: 00:00:01
Fold 3 Epoch 037; Train loss: 0.2306; Time: 00:00:01
Fold 3 Epoch 038; Train loss: 0.2292; Time: 00:00:01
Fold 3 Epoch 039; Train loss: 0.2274; Time: 00:00:01
Fold 3 Epoch 040; Train loss: 0.2229; Time: 00:00:01
Fold 3: Evaluation at Epoch 40
HR@5       NDCG@5     HR@10      NDCG@10   
0.576159   0.483073   0.703642   0.523235  
Loss: 0.2772
/home/nit/Desktop/2PDreamRec/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:243: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
Fold 3 Epoch 041; Train loss: 0.2197; Time: 00:00:02
Fold 3 Epoch 042; Train loss: 0.2151; Time: 00:00:02
Fold 3 Epoch 043; Train loss: 0.2113; Time: 00:00:01
Fold 3 Epoch 044; Train loss: 0.2133; Time: 00:00:01
Fold 3 Epoch 045; Train loss: 0.2080; Time: 00:00:01
Fold 3 Epoch 046; Train loss: 0.2112; Time: 00:00:02
Fold 3 Epoch 047; Train loss: 0.2032; Time: 00:00:01
Fold 3 Epoch 048; Train loss: 0.2039; Time: 00:00:02
Fold 3 Epoch 049; Train loss: 0.1981; Time: 00:00:02
Fold 3 Epoch 050; Train loss: 0.1935; Time: 00:00:01
Fold 3: Evaluation at Epoch 50
HR@5       NDCG@5     HR@10      NDCG@10   
0.592715   0.495496   0.728477   0.538813  
Loss: 0.2372
/home/nit/Desktop/2PDreamRec/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:243: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
Fold 3 Epoch 051; Train loss: 0.1885; Time: 00:00:01
Fold 3 Epoch 052; Train loss: 0.1874; Time: 00:00:01
Fold 3 Epoch 053; Train loss: 0.1876; Time: 00:00:01
Fold 3 Epoch 054; Train loss: 0.1869; Time: 00:00:02
Fold 3 Epoch 055; Train loss: 0.1861; Time: 00:00:01
Fold 3 Epoch 056; Train loss: 0.1818; Time: 00:00:01
Fold 3 Epoch 057; Train loss: 0.1804; Time: 00:00:02
Fold 3 Epoch 058; Train loss: 0.1767; Time: 00:00:01
Fold 3 Epoch 059; Train loss: 0.1723; Time: 00:00:01
Fold 3 Epoch 060; Train loss: 0.1714; Time: 00:00:01
Fold 3: Evaluation at Epoch 60
HR@5       NDCG@5     HR@10      NDCG@10   
0.549669   0.455949   0.673841   0.495158  
Loss: 0.2116
/home/nit/Desktop/2PDreamRec/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:243: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
Fold 3 Epoch 061; Train loss: 0.1713; Time: 00:00:01
Fold 3 Epoch 062; Train loss: 0.1660; Time: 00:00:01
Fold 3 Epoch 063; Train loss: 0.1670; Time: 00:00:01
Fold 3 Epoch 064; Train loss: 0.1601; Time: 00:00:02
Fold 3 Epoch 065; Train loss: 0.1624; Time: 00:00:02
Fold 3 Epoch 066; Train loss: 0.1561; Time: 00:00:02
Fold 3 Epoch 067; Train loss: 0.1553; Time: 00:00:01
Fold 3 Epoch 068; Train loss: 0.1534; Time: 00:00:01
Fold 3 Epoch 069; Train loss: 0.1505; Time: 00:00:02
Fold 3 Epoch 070; Train loss: 0.1513; Time: 00:00:02
Fold 3: Evaluation at Epoch 70
HR@5       NDCG@5     HR@10      NDCG@10   
0.541391   0.460185   0.682119   0.504970  
Loss: 0.1875
/home/nit/Desktop/2PDreamRec/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:243: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
Fold 3 Epoch 071; Train loss: 0.1478; Time: 00:00:01
Fold 3 Epoch 072; Train loss: 0.1445; Time: 00:00:01
Fold 3 Epoch 073; Train loss: 0.1469; Time: 00:00:02
Fold 3 Epoch 074; Train loss: 0.1412; Time: 00:00:02
Fold 3 Epoch 075; Train loss: 0.1384; Time: 00:00:02
Fold 3 Epoch 076; Train loss: 0.1367; Time: 00:00:01
Fold 3 Epoch 077; Train loss: 0.1341; Time: 00:00:01
Fold 3 Epoch 078; Train loss: 0.1345; Time: 00:00:01
Fold 3 Epoch 079; Train loss: 0.1343; Time: 00:00:01
Fold 3 Epoch 080; Train loss: 0.1314; Time: 00:00:01
Fold 3: Evaluation at Epoch 80
HR@5       NDCG@5     HR@10      NDCG@10   
0.584437   0.485912   0.731788   0.532979  
Loss: 0.1743
/home/nit/Desktop/2PDreamRec/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:243: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
Tuning fold metrics saved to ./category/fold_metrics_tune.txt
Traceback (most recent call last):
  File "/home/nit/Desktop/2PDreamRec/DreamRec_Gen.py", line 482, in <module>
    main()
  File "/home/nit/Desktop/2PDreamRec/DreamRec_Gen.py", line 422, in main
    best_candidates["scheduler_factor"] = args.scheduler_factor
AttributeError: 'Namespace' object has no attribute 'scheduler_factor'
Tuning lr:   0%|          | 0/5 [00:00<?, ?it/s]
========== Fold 3 ==========
INFO:root:Using statics: seq_size = 10, genre_vocab_size = 18
Fold 3 Epoch 001; Train loss: 3259.5898; Time: 00:00:03
Fold 3 Epoch 002; Train loss: 32.2959; Time: 00:00:01
Fold 3 Epoch 003; Train loss: 4.1245; Time: 00:00:01
Fold 3 Epoch 004; Train loss: 2.1415; Time: 00:00:01
Fold 3 Epoch 005; Train loss: 1.3509; Time: 00:00:01
Fold 3 Epoch 006; Train loss: 1.0796; Time: 00:00:01
Fold 3 Epoch 007; Train loss: 0.9241; Time: 00:00:02
Fold 3 Epoch 008; Train loss: 0.7827; Time: 00:00:02
Fold 3 Epoch 009; Train loss: 0.6810; Time: 00:00:01
Fold 3 Epoch 010; Train loss: 0.6117; Time: 00:00:01
Fold 3: Evaluation at Epoch 10
HR@5       NDCG@5     HR@10      NDCG@10   
0.293046   0.194068   0.450331   0.244634  
Loss: 0.5832
Fold 3 Epoch 011; Train loss: 0.5322; Time: 00:00:01
Fold 3 Epoch 012; Train loss: 0.4781; Time: 00:00:01
Fold 3 Epoch 013; Train loss: 0.4494; Time: 00:00:02
Fold 3 Epoch 014; Train loss: 0.4121; Time: 00:00:01
Fold 3 Epoch 015; Train loss: 0.3892; Time: 00:00:01
Fold 3 Epoch 016; Train loss: 0.3717; Time: 00:00:01
Fold 3 Epoch 017; Train loss: 0.3475; Time: 00:00:01
Fold 3 Epoch 018; Train loss: 0.3332; Time: 00:00:02
Fold 3 Epoch 019; Train loss: 0.3254; Time: 00:00:01
Fold 3 Epoch 020; Train loss: 0.3167; Time: 00:00:01
Fold 3: Evaluation at Epoch 20
HR@5       NDCG@5     HR@10      NDCG@10   
0.228477   0.154273   0.415563   0.215024  
Loss: 0.3461
Fold 3 Epoch 021; Train loss: 0.2986; Time: 00:00:01
Fold 3 Epoch 022; Train loss: 0.2861; Time: 00:00:02
Fold 3 Epoch 023; Train loss: 0.2801; Time: 00:00:02
Fold 3 Epoch 024; Train loss: 0.2743; Time: 00:00:02
Fold 3 Epoch 025; Train loss: 0.2693; Time: 00:00:02
Fold 3 Epoch 026; Train loss: 0.2560; Time: 00:00:02
Fold 3 Epoch 027; Train loss: 0.2557; Time: 00:00:02
Fold 3 Epoch 028; Train loss: 0.2518; Time: 00:00:02
Fold 3 Epoch 029; Train loss: 0.2438; Time: 00:00:02
Fold 3 Epoch 030; Train loss: 0.2374; Time: 00:00:01
Fold 3: Evaluation at Epoch 30
HR@5       NDCG@5     HR@10      NDCG@10   
0.192053   0.118786   0.362583   0.174320  
Loss: 0.2678
Fold 3 Epoch 031; Train loss: 0.2409; Time: 00:00:01
Fold 3 Epoch 032; Train loss: 0.2252; Time: 00:00:02
Fold 3 Epoch 033; Train loss: 0.2273; Time: 00:00:01
Fold 3 Epoch 034; Train loss: 0.2190; Time: 00:00:01
Fold 3 Epoch 035; Train loss: 0.2184; Time: 00:00:01
Fold 3 Epoch 036; Train loss: 0.2109; Time: 00:00:01
Fold 3 Epoch 037; Train loss: 0.2117; Time: 00:00:01
Fold 3 Epoch 038; Train loss: 0.2093; Time: 00:00:01
Fold 3 Epoch 039; Train loss: 0.2001; Time: 00:00:02
Fold 3 Epoch 040; Train loss: 0.2009; Time: 00:00:02
Fold 3: Evaluation at Epoch 40
HR@5       NDCG@5     HR@10      NDCG@10   
0.233444   0.157608   0.413907   0.217049  
Loss: 0.2358
Fold 3 Epoch 041; Train loss: 0.1999; Time: 00:00:02
Fold 3 Epoch 042; Train loss: 0.1972; Time: 00:00:02
Fold 3 Epoch 043; Train loss: 0.1913; Time: 00:00:02
Fold 3 Epoch 044; Train loss: 0.1906; Time: 00:00:02
Fold 3 Epoch 045; Train loss: 0.1868; Time: 00:00:02
Fold 3 Epoch 046; Train loss: 0.1796; Time: 00:00:01
Fold 3 Epoch 047; Train loss: 0.1770; Time: 00:00:01
Fold 3 Epoch 048; Train loss: 0.1792; Time: 00:00:02
Fold 3 Epoch 049; Train loss: 0.1754; Time: 00:00:02
Fold 3 Epoch 050; Train loss: 0.1749; Time: 00:00:02
Fold 3: Evaluation at Epoch 50
HR@5       NDCG@5     HR@10      NDCG@10   
0.264901   0.164427   0.463576   0.227321  
Loss: 0.2115
Fold 3 Epoch 051; Train loss: 0.1672; Time: 00:00:01
Fold 3 Epoch 052; Train loss: 0.1658; Time: 00:00:01
Fold 3 Epoch 053; Train loss: 0.1567; Time: 00:00:02
Fold 3 Epoch 054; Train loss: 0.1624; Time: 00:00:02
Fold 3 Epoch 055; Train loss: 0.1582; Time: 00:00:02
Fold 3 Epoch 056; Train loss: 0.1538; Time: 00:00:02
Fold 3 Epoch 057; Train loss: 0.1534; Time: 00:00:02
Fold 3 Epoch 058; Train loss: 0.1509; Time: 00:00:02
Fold 3 Epoch 059; Train loss: 0.1463; Time: 00:00:02
Fold 3 Epoch 060; Train loss: 0.1446; Time: 00:00:02
Fold 3: Evaluation at Epoch 60
HR@5       NDCG@5     HR@10      NDCG@10   
0.250000   0.182349   0.451987   0.247798  
Loss: 0.1607
Fold 3 Epoch 061; Train loss: 0.1420; Time: 00:00:02
Fold 3 Epoch 062; Train loss: 0.1400; Time: 00:00:02
Fold 3 Epoch 063; Train loss: 0.1363; Time: 00:00:02
Fold 3 Epoch 064; Train loss: 0.1354; Time: 00:00:02
Fold 3 Epoch 065; Train loss: 0.1326; Time: 00:00:02
Fold 3 Epoch 066; Train loss: 0.1318; Time: 00:00:02
Fold 3 Epoch 067; Train loss: 0.1274; Time: 00:00:01
Fold 3 Epoch 068; Train loss: 0.1279; Time: 00:00:02
Fold 3 Epoch 069; Train loss: 0.1281; Time: 00:00:02
Fold 3 Epoch 070; Train loss: 0.1225; Time: 00:00:02
Fold 3: Evaluation at Epoch 70
HR@5       NDCG@5     HR@10      NDCG@10   
0.205298   0.131568   0.407285   0.197274  
Loss: 0.1384
Fold 3 Epoch 071; Train loss: 0.1205; Time: 00:00:02
Fold 3 Epoch 072; Train loss: 0.1199; Time: 00:00:01
Fold 3 Epoch 073; Train loss: 0.1156; Time: 00:00:02
Fold 3 Epoch 074; Train loss: 0.1151; Time: 00:00:02
Fold 3 Epoch 075; Train loss: 0.1108; Time: 00:00:01
Fold 3 Epoch 076; Train loss: 0.1098; Time: 00:00:01
Fold 3 Epoch 077; Train loss: 0.1054; Time: 00:00:01
Fold 3 Epoch 078; Train loss: 0.1041; Time: 00:00:01
Fold 3 Epoch 079; Train loss: 0.1045; Time: 00:00:01
Fold 3 Epoch 080; Train loss: 0.1014; Time: 00:00:01
Fold 3: Evaluation at Epoch 80
HR@5       NDCG@5     HR@10      NDCG@10   
0.160596   0.100530   0.367550   0.167755  
Loss: 0.1086
Fold 3 Epoch 081; Train loss: 0.1005; Time: 00:00:01
Fold 3 Epoch 082; Train loss: 0.0972; Time: 00:00:01
Fold 3 Epoch 083; Train loss: 0.0973; Time: 00:00:01
Fold 3 Epoch 084; Train loss: 0.0946; Time: 00:00:01
Fold 3 Epoch 085; Train loss: 0.0942; Time: 00:00:01
Fold 3 Epoch 086; Train loss: 0.0893; Time: 00:00:01
Fold 3 Epoch 087; Train loss: 0.0908; Time: 00:00:01
Fold 3 Epoch 088; Train loss: 0.0902; Time: 00:00:01
Fold 3 Epoch 089; Train loss: 0.0850; Time: 00:00:01
Fold 3 Epoch 090; Train loss: 0.0836; Time: 00:00:01
Fold 3: Evaluation at Epoch 90
HR@5       NDCG@5     HR@10      NDCG@10   
0.253311   0.136384   0.422185   0.190136  
Loss: 0.0896
Fold 3 Epoch 091; Train loss: 0.0843; Time: 00:00:01
Fold 3 Epoch 092; Train loss: 0.0798; Time: 00:00:01
Fold 3 Epoch 093; Train loss: 0.0781; Time: 00:00:01
Fold 3 Epoch 094; Train loss: 0.0766; Time: 00:00:01
Fold 3 Epoch 095; Train loss: 0.0765; Time: 00:00:02
Fold 3 Epoch 096; Train loss: 0.0755; Time: 00:00:02
Fold 3 Epoch 097; Train loss: 0.0736; Time: 00:00:02
Fold 3 Epoch 098; Train loss: 0.0720; Time: 00:00:01
Fold 3 Epoch 099; Train loss: 0.0701; Time: 00:00:01
Fold 3 Epoch 100; Train loss: 0.0696; Time: 00:00:01
Fold 3: Evaluation at Epoch 100
HR@5       NDCG@5     HR@10      NDCG@10   
0.302980   0.162114   0.410596   0.196256  
Loss: 0.0684
[lr candidate 0.1] Fold 3: NDCG@10 = [np.float64(0.24463440992349647), np.float64(0.21502436726944427), np.float64(0.1743200404367121), np.float64(0.21704853131123703), np.float64(0.2273208986198779), np.float64(0.24779848648591787), np.float64(0.19727434724167145), np.float64(0.16775486180148563), np.float64(0.19013563162288005), np.float64(0.19625554027197517)]
Tuning lr:  20%|██        | 1/5 [03:43<14:54, 223.53s/it]
========== Fold 3 ==========
INFO:root:Using statics: seq_size = 10, genre_vocab_size = 18
Fold 3 Epoch 001; Train loss: 42.1974; Time: 00:00:01
Fold 3 Epoch 002; Train loss: 0.7273; Time: 00:00:01
Fold 3 Epoch 003; Train loss: 0.4209; Time: 00:00:01
Fold 3 Epoch 004; Train loss: 0.3395; Time: 00:00:01
Fold 3 Epoch 005; Train loss: 0.2916; Time: 00:00:01
Fold 3 Epoch 006; Train loss: 0.2552; Time: 00:00:01
Fold 3 Epoch 007; Train loss: 0.2336; Time: 00:00:01
Fold 3 Epoch 008; Train loss: 0.2186; Time: 00:00:01
Fold 3 Epoch 009; Train loss: 0.2081; Time: 00:00:01
Fold 3 Epoch 010; Train loss: 0.1935; Time: 00:00:01
Fold 3: Evaluation at Epoch 10
HR@5       NDCG@5     HR@10      NDCG@10   
0.279801   0.189861   0.534768   0.271420  
Loss: 0.2027
Fold 3 Epoch 011; Train loss: 0.1860; Time: 00:00:01
Fold 3 Epoch 012; Train loss: 0.2114; Time: 00:00:01
Fold 3 Epoch 013; Train loss: 0.2300; Time: 00:00:01
Fold 3 Epoch 014; Train loss: 0.1692; Time: 00:00:02
Fold 3 Epoch 015; Train loss: 0.1443; Time: 00:00:02
Fold 3 Epoch 016; Train loss: 0.1370; Time: 00:00:01
Fold 3 Epoch 017; Train loss: 0.1243; Time: 00:00:01
Fold 3 Epoch 018; Train loss: 0.1168; Time: 00:00:01
Fold 3 Epoch 019; Train loss: 0.1096; Time: 00:00:02
Fold 3 Epoch 020; Train loss: 0.0980; Time: 00:00:02
Fold 3: Evaluation at Epoch 20
HR@5       NDCG@5     HR@10      NDCG@10   
0.352649   0.262315   0.524834   0.316843  
Loss: 0.0974
Fold 3 Epoch 021; Train loss: 0.0938; Time: 00:00:01
Fold 3 Epoch 022; Train loss: 0.0880; Time: 00:00:01
Fold 3 Epoch 023; Train loss: 0.0862; Time: 00:00:01
Fold 3 Epoch 024; Train loss: 0.0958; Time: 00:00:01
Fold 3 Epoch 025; Train loss: 0.1137; Time: 00:00:01
Fold 3 Epoch 026; Train loss: 0.0907; Time: 00:00:01
Fold 3 Epoch 027; Train loss: 0.0669; Time: 00:00:01
Fold 3 Epoch 028; Train loss: 0.0583; Time: 00:00:01
Fold 3 Epoch 029; Train loss: 0.0513; Time: 00:00:01
Fold 3 Epoch 030; Train loss: 0.0459; Time: 00:00:01
Fold 3: Evaluation at Epoch 30
HR@5       NDCG@5     HR@10      NDCG@10   
0.294702   0.172001   0.596026   0.269690  
Loss: 0.0338
Fold 3 Epoch 031; Train loss: 0.0438; Time: 00:00:01
Fold 3 Epoch 032; Train loss: 0.0400; Time: 00:00:01
Fold 3 Epoch 033; Train loss: 0.0378; Time: 00:00:01
Fold 3 Epoch 034; Train loss: 0.0343; Time: 00:00:01
Fold 3 Epoch 035; Train loss: 0.0331; Time: 00:00:01
Fold 3 Epoch 036; Train loss: 0.0304; Time: 00:00:01
Fold 3 Epoch 037; Train loss: 0.0277; Time: 00:00:01
Fold 3 Epoch 038; Train loss: 0.0254; Time: 00:00:01
Fold 3 Epoch 039; Train loss: 0.0230; Time: 00:00:01
Fold 3 Epoch 040; Train loss: 0.0225; Time: 00:00:01
Fold 3: Evaluation at Epoch 40
HR@5       NDCG@5     HR@10      NDCG@10   
0.332781   0.293725   0.428808   0.324206  
Loss: 0.0145
Fold 3 Epoch 041; Train loss: 0.0241; Time: 00:00:01
Fold 3 Epoch 042; Train loss: 0.0340; Time: 00:00:01
Fold 3 Epoch 043; Train loss: 0.0552; Time: 00:00:02
Fold 3 Epoch 044; Train loss: 0.0386; Time: 00:00:02
Fold 3 Epoch 045; Train loss: 0.0133; Time: 00:00:02
Fold 3 Epoch 046; Train loss: 0.0083; Time: 00:00:02
Fold 3 Epoch 047; Train loss: 0.0065; Time: 00:00:02
Fold 3 Epoch 048; Train loss: 0.0052; Time: 00:00:02
Fold 3 Epoch 049; Train loss: 0.0042; Time: 00:00:02
Fold 3 Epoch 050; Train loss: 0.0034; Time: 00:00:02
Fold 3: Evaluation at Epoch 50
HR@5       NDCG@5     HR@10      NDCG@10   
0.374172   0.233425   0.612583   0.303577  
Loss: 0.0012
Fold 3 Epoch 051; Train loss: 0.0029; Time: 00:00:01
Fold 3 Epoch 052; Train loss: 0.0025; Time: 00:00:01
Fold 3 Epoch 053; Train loss: 0.0022; Time: 00:00:01
Fold 3 Epoch 054; Train loss: 0.0019; Time: 00:00:01
Fold 3 Epoch 055; Train loss: 0.0018; Time: 00:00:01
Fold 3 Epoch 056; Train loss: 0.0016; Time: 00:00:01
Fold 3 Epoch 057; Train loss: 0.0016; Time: 00:00:01
Fold 3 Epoch 058; Train loss: 0.0015; Time: 00:00:01
Fold 3 Epoch 059; Train loss: 0.0014; Time: 00:00:02
Fold 3 Epoch 060; Train loss: 0.0014; Time: 00:00:01
Fold 3: Evaluation at Epoch 60
HR@5       NDCG@5     HR@10      NDCG@10   
0.041391   0.025775   0.817881   0.268616  
Loss: 0.0004
Fold 3 Epoch 061; Train loss: 0.0013; Time: 00:00:01
Fold 3 Epoch 062; Train loss: 0.0013; Time: 00:00:01
Fold 3 Epoch 063; Train loss: 0.0013; Time: 00:00:01
Fold 3 Epoch 064; Train loss: 0.0013; Time: 00:00:02
Fold 3 Epoch 065; Train loss: 0.0015; Time: 00:00:01
Fold 3 Epoch 066; Train loss: 0.0017; Time: 00:00:02
Fold 3 Epoch 067; Train loss: 0.0027; Time: 00:00:02
Fold 3 Epoch 068; Train loss: 0.0050; Time: 00:00:02
Fold 3 Epoch 069; Train loss: 0.0104; Time: 00:00:02
Fold 3 Epoch 070; Train loss: 0.0142; Time: 00:00:01
Fold 3: Evaluation at Epoch 70
HR@5       NDCG@5     HR@10      NDCG@10   
0.299669   0.155822   0.612583   0.251191  
Loss: 0.0126
Fold 3 Epoch 071; Train loss: 0.0088; Time: 00:00:01
Fold 3 Epoch 072; Train loss: 0.0028; Time: 00:00:01
Fold 3 Epoch 073; Train loss: 0.0014; Time: 00:00:01
Fold 3 Epoch 074; Train loss: 0.0010; Time: 00:00:01
Fold 3 Epoch 075; Train loss: 0.0008; Time: 00:00:02
Fold 3 Epoch 076; Train loss: 0.0009; Time: 00:00:01
Fold 3 Epoch 077; Train loss: 0.0008; Time: 00:00:02
Fold 3 Epoch 078; Train loss: 0.0008; Time: 00:00:01
Fold 3 Epoch 079; Train loss: 0.0008; Time: 00:00:01
Fold 3 Epoch 080; Train loss: 0.0008; Time: 00:00:02
Fold 3: Evaluation at Epoch 80
HR@5       NDCG@5     HR@10      NDCG@10   
0.089404   0.041605   0.422185   0.150751  
Loss: 0.0003
Fold 3 Epoch 081; Train loss: 0.0007; Time: 00:00:02
Fold 3 Epoch 082; Train loss: 0.0007; Time: 00:00:02
Fold 3 Epoch 083; Train loss: 0.0007; Time: 00:00:02
Fold 3 Epoch 084; Train loss: 0.0007; Time: 00:00:02
Fold 3 Epoch 085; Train loss: 0.0008; Time: 00:00:02
Fold 3 Epoch 086; Train loss: 0.0008; Time: 00:00:02
Fold 3 Epoch 087; Train loss: 0.0008; Time: 00:00:02
Fold 3 Epoch 088; Train loss: 0.0008; Time: 00:00:02
Fold 3 Epoch 089; Train loss: 0.0009; Time: 00:00:02
Fold 3 Epoch 090; Train loss: 0.0013; Time: 00:00:02
Fold 3: Evaluation at Epoch 90
HR@5       NDCG@5     HR@10      NDCG@10   
0.339404   0.151247   0.897351   0.323837  
Loss: 0.0016
Fold 3 Epoch 091; Train loss: 0.0022; Time: 00:00:02
Fold 3 Epoch 092; Train loss: 0.0045; Time: 00:00:01
Fold 3 Epoch 093; Train loss: 0.0073; Time: 00:00:01
Fold 3 Epoch 094; Train loss: 0.0086; Time: 00:00:01
Fold 3 Epoch 095; Train loss: 0.0060; Time: 00:00:01
Fold 3 Epoch 096; Train loss: 0.0030; Time: 00:00:01
Fold 3 Epoch 097; Train loss: 0.0015; Time: 00:00:01
Fold 3 Epoch 098; Train loss: 0.0009; Time: 00:00:01
Fold 3 Epoch 099; Train loss: 0.0006; Time: 00:00:01
Fold 3 Epoch 100; Train loss: 0.0005; Time: 00:00:01
Fold 3: Evaluation at Epoch 100
HR@5       NDCG@5     HR@10      NDCG@10   
0.057947   0.025974   0.831126   0.280401  
Loss: 0.0002
[lr candidate 0.05] Fold 3: NDCG@10 = [np.float64(0.2714198579558931), np.float64(0.31684324276350995), np.float64(0.2696900956337434), np.float64(0.3242055791057502), np.float64(0.3035767974913826), np.float64(0.2686163174012447), np.float64(0.25119145016229877), np.float64(0.15075095772993113), np.float64(0.3238365201049245), np.float64(0.2804006702039942)]
Tuning lr:  40%|████      | 2/5 [07:15<10:50, 216.99s/it]
========== Fold 3 ==========
INFO:root:Using statics: seq_size = 10, genre_vocab_size = 18
Fold 3 Epoch 001; Train loss: 0.6776; Time: 00:00:01
Fold 3 Epoch 002; Train loss: 0.3453; Time: 00:00:02
Fold 3 Epoch 003; Train loss: 0.2779; Time: 00:00:01
Fold 3 Epoch 004; Train loss: 0.2508; Time: 00:00:01
Fold 3 Epoch 005; Train loss: 0.2275; Time: 00:00:01
Fold 3 Epoch 006; Train loss: 0.2096; Time: 00:00:02
Fold 3 Epoch 007; Train loss: 0.2035; Time: 00:00:02
Fold 3 Epoch 008; Train loss: 0.1899; Time: 00:00:01
Fold 3 Epoch 009; Train loss: 0.1807; Time: 00:00:02
Fold 3 Epoch 010; Train loss: 0.1687; Time: 00:00:01
Fold 3: Evaluation at Epoch 10
HR@5       NDCG@5     HR@10      NDCG@10   
0.768212   0.569431   0.869205   0.602265  
Loss: 0.2063
Fold 3 Epoch 011; Train loss: 0.1613; Time: 00:00:01
Fold 3 Epoch 012; Train loss: 0.1546; Time: 00:00:01
Fold 3 Epoch 013; Train loss: 0.1447; Time: 00:00:02
Fold 3 Epoch 014; Train loss: 0.1365; Time: 00:00:02
Fold 3 Epoch 015; Train loss: 0.1267; Time: 00:00:01
Fold 3 Epoch 016; Train loss: 0.1175; Time: 00:00:02
Fold 3 Epoch 017; Train loss: 0.1119; Time: 00:00:01
Fold 3 Epoch 018; Train loss: 0.1053; Time: 00:00:02
Fold 3 Epoch 019; Train loss: 0.0975; Time: 00:00:02
Fold 3 Epoch 020; Train loss: 0.0905; Time: 00:00:02
Fold 3: Evaluation at Epoch 20
HR@5       NDCG@5     HR@10      NDCG@10   
0.657285   0.518713   0.824503   0.571877  
Loss: 0.0984
Fold 3 Epoch 021; Train loss: 0.0867; Time: 00:00:01
Fold 3 Epoch 022; Train loss: 0.0791; Time: 00:00:01
Fold 3 Epoch 023; Train loss: 0.0734; Time: 00:00:02
Fold 3 Epoch 024; Train loss: 0.0675; Time: 00:00:01
Fold 3 Epoch 025; Train loss: 0.0615; Time: 00:00:01
Fold 3 Epoch 026; Train loss: 0.0564; Time: 00:00:01
Fold 3 Epoch 027; Train loss: 0.0532; Time: 00:00:01
Fold 3 Epoch 028; Train loss: 0.0471; Time: 00:00:01
Fold 3 Epoch 029; Train loss: 0.0439; Time: 00:00:02
Fold 3 Epoch 030; Train loss: 0.0405; Time: 00:00:02
Fold 3: Evaluation at Epoch 30
HR@5       NDCG@5     HR@10      NDCG@10   
0.657285   0.485185   0.859272   0.550800  
Loss: 0.0420
Fold 3 Epoch 031; Train loss: 0.0351; Time: 00:00:02
Fold 3 Epoch 032; Train loss: 0.0328; Time: 00:00:01
Fold 3 Epoch 033; Train loss: 0.0290; Time: 00:00:01
Fold 3 Epoch 034; Train loss: 0.0258; Time: 00:00:01
Fold 3 Epoch 035; Train loss: 0.0215; Time: 00:00:01
Fold 3 Epoch 036; Train loss: 0.0183; Time: 00:00:02
Fold 3 Epoch 037; Train loss: 0.0156; Time: 00:00:01
Fold 3 Epoch 038; Train loss: 0.0131; Time: 00:00:01
Fold 3 Epoch 039; Train loss: 0.0112; Time: 00:00:02
Fold 3 Epoch 040; Train loss: 0.0103; Time: 00:00:02
Fold 3: Evaluation at Epoch 40
HR@5       NDCG@5     HR@10      NDCG@10   
0.653974   0.382832   0.880795   0.452804  
Loss: 0.0117
Fold 3 Epoch 041; Train loss: 0.0085; Time: 00:00:02
Fold 3 Epoch 042; Train loss: 0.0078; Time: 00:00:02
Fold 3 Epoch 043; Train loss: 0.0070; Time: 00:00:01
Fold 3 Epoch 044; Train loss: 0.0063; Time: 00:00:01
Fold 3 Epoch 045; Train loss: 0.0054; Time: 00:00:02
Fold 3 Epoch 046; Train loss: 0.0045; Time: 00:00:02
Fold 3 Epoch 047; Train loss: 0.0033; Time: 00:00:01
Fold 3 Epoch 048; Train loss: 0.0028; Time: 00:00:01
Fold 3 Epoch 049; Train loss: 0.0025; Time: 00:00:01
Fold 3 Epoch 050; Train loss: 0.0021; Time: 00:00:01
Fold 3: Evaluation at Epoch 50
HR@5       NDCG@5     HR@10      NDCG@10   
0.548013   0.372389   0.887417   0.475591  
Loss: 0.0028
Fold 3 Epoch 051; Train loss: 0.0018; Time: 00:00:01
Fold 3 Epoch 052; Train loss: 0.0018; Time: 00:00:01
Fold 3 Epoch 053; Train loss: 0.0016; Time: 00:00:02
Fold 3 Epoch 054; Train loss: 0.0014; Time: 00:00:01
Fold 3 Epoch 055; Train loss: 0.0014; Time: 00:00:01
Fold 3 Epoch 056; Train loss: 0.0013; Time: 00:00:01
Fold 3 Epoch 057; Train loss: 0.0016; Time: 00:00:01
Fold 3 Epoch 058; Train loss: 0.0020; Time: 00:00:01
Fold 3 Epoch 059; Train loss: 0.0016; Time: 00:00:01
Fold 3 Epoch 060; Train loss: 0.0014; Time: 00:00:01
Fold 3: Evaluation at Epoch 60
HR@5       NDCG@5     HR@10      NDCG@10   
0.253311   0.126956   0.862583   0.321648  
Loss: 0.0015
Fold 3 Epoch 061; Train loss: 0.0013; Time: 00:00:01
Fold 3 Epoch 062; Train loss: 0.0010; Time: 00:00:01
Fold 3 Epoch 063; Train loss: 0.0008; Time: 00:00:01
Fold 3 Epoch 064; Train loss: 0.0010; Time: 00:00:02
Fold 3 Epoch 065; Train loss: 0.0008; Time: 00:00:01
Fold 3 Epoch 066; Train loss: 0.0006; Time: 00:00:01
Fold 3 Epoch 067; Train loss: 0.0006; Time: 00:00:01
Fold 3 Epoch 068; Train loss: 0.0007; Time: 00:00:02
Fold 3 Epoch 069; Train loss: 0.0006; Time: 00:00:01
Fold 3 Epoch 070; Train loss: 0.0007; Time: 00:00:01
Fold 3: Evaluation at Epoch 70
HR@5       NDCG@5     HR@10      NDCG@10   
0.539735   0.239223   0.852649   0.333670  
Loss: 0.0008
Fold 3 Epoch 071; Train loss: 0.0008; Time: 00:00:02
Fold 3 Epoch 072; Train loss: 0.0009; Time: 00:00:02
Fold 3 Epoch 073; Train loss: 0.0013; Time: 00:00:01
Fold 3 Epoch 074; Train loss: 0.0014; Time: 00:00:01
Fold 3 Epoch 075; Train loss: 0.0012; Time: 00:00:01
Fold 3 Epoch 076; Train loss: 0.0009; Time: 00:00:01
Fold 3 Epoch 077; Train loss: 0.0006; Time: 00:00:01
Fold 3 Epoch 078; Train loss: 0.0004; Time: 00:00:01
Fold 3 Epoch 079; Train loss: 0.0004; Time: 00:00:01
Fold 3 Epoch 080; Train loss: 0.0004; Time: 00:00:01
Fold 3: Evaluation at Epoch 80
HR@5       NDCG@5     HR@10      NDCG@10   
0.031457   0.018862   0.374172   0.124077  
Loss: 0.0003
Fold 3 Epoch 081; Train loss: 0.0003; Time: 00:00:01
Fold 3 Epoch 082; Train loss: 0.0003; Time: 00:00:01
Fold 3 Epoch 083; Train loss: 0.0003; Time: 00:00:01
Fold 3 Epoch 084; Train loss: 0.0004; Time: 00:00:01
Fold 3 Epoch 085; Train loss: 0.0005; Time: 00:00:01
Fold 3 Epoch 086; Train loss: 0.0006; Time: 00:00:01
Fold 3 Epoch 087; Train loss: 0.0006; Time: 00:00:02
Fold 3 Epoch 088; Train loss: 0.0006; Time: 00:00:02
Fold 3 Epoch 089; Train loss: 0.0008; Time: 00:00:01
Fold 3 Epoch 090; Train loss: 0.0014; Time: 00:00:01
Fold 3: Evaluation at Epoch 90
HR@5       NDCG@5     HR@10      NDCG@10   
0.324503   0.163135   0.685430   0.269757  
Loss: 0.0016
Fold 3 Epoch 091; Train loss: 0.0018; Time: 00:00:01
Fold 3 Epoch 092; Train loss: 0.0021; Time: 00:00:01
Fold 3 Epoch 093; Train loss: 0.0012; Time: 00:00:01
Fold 3 Epoch 094; Train loss: 0.0009; Time: 00:00:02
Fold 3 Epoch 095; Train loss: 0.0005; Time: 00:00:02
Fold 3 Epoch 096; Train loss: 0.0002; Time: 00:00:01
Fold 3 Epoch 097; Train loss: 0.0001; Time: 00:00:01
Fold 3 Epoch 098; Train loss: 0.0001; Time: 00:00:01
Fold 3 Epoch 099; Train loss: 0.0002; Time: 00:00:01
Fold 3 Epoch 100; Train loss: 0.0002; Time: 00:00:01
Fold 3: Evaluation at Epoch 100
HR@5       NDCG@5     HR@10      NDCG@10   
0.029801   0.020814   0.564570   0.188978  
Loss: 0.0003
[lr candidate 0.01] Fold 3: NDCG@10 = [np.float64(0.6022654104841048), np.float64(0.571877267703479), np.float64(0.5508004959532043), np.float64(0.4528041061742681), np.float64(0.4755906685146541), np.float64(0.32164803876484466), np.float64(0.3336699651762201), np.float64(0.12407699645506776), np.float64(0.2697574622105275), np.float64(0.18897770216970014)]
Tuning lr:  60%|██████    | 3/5 [10:24<06:48, 204.09s/it]
========== Fold 3 ==========
INFO:root:Using statics: seq_size = 10, genre_vocab_size = 18
Fold 3 Epoch 001; Train loss: 0.7968; Time: 00:00:01
Fold 3 Epoch 002; Train loss: 0.4769; Time: 00:00:02
Fold 3 Epoch 003; Train loss: 0.3651; Time: 00:00:01
Fold 3 Epoch 004; Train loss: 0.3119; Time: 00:00:01
Fold 3 Epoch 005; Train loss: 0.2857; Time: 00:00:01
Fold 3 Epoch 006; Train loss: 0.2685; Time: 00:00:01
Fold 3 Epoch 007; Train loss: 0.2486; Time: 00:00:01
Fold 3 Epoch 008; Train loss: 0.2438; Time: 00:00:01
Fold 3 Epoch 009; Train loss: 0.2363; Time: 00:00:02
Fold 3 Epoch 010; Train loss: 0.2238; Time: 00:00:01
Fold 3: Evaluation at Epoch 10
HR@5       NDCG@5     HR@10      NDCG@10   
0.586093   0.472938   0.745033   0.524507  
Loss: 0.2986
Fold 3 Epoch 011; Train loss: 0.2245; Time: 00:00:01
Fold 3 Epoch 012; Train loss: 0.2191; Time: 00:00:02
Fold 3 Epoch 013; Train loss: 0.2062; Time: 00:00:01
Fold 3 Epoch 014; Train loss: 0.2047; Time: 00:00:02
Fold 3 Epoch 015; Train loss: 0.2000; Time: 00:00:02
Fold 3 Epoch 016; Train loss: 0.1976; Time: 00:00:02
Fold 3 Epoch 017; Train loss: 0.1937; Time: 00:00:02
Fold 3 Epoch 018; Train loss: 0.1827; Time: 00:00:01
Fold 3 Epoch 019; Train loss: 0.1800; Time: 00:00:01
Fold 3 Epoch 020; Train loss: 0.1706; Time: 00:00:01
Fold 3: Evaluation at Epoch 20
HR@5       NDCG@5     HR@10      NDCG@10   
0.619205   0.489647   0.758278   0.534504  
Loss: 0.2011
Fold 3 Epoch 021; Train loss: 0.1691; Time: 00:00:01
Fold 3 Epoch 022; Train loss: 0.1622; Time: 00:00:02
Fold 3 Epoch 023; Train loss: 0.1580; Time: 00:00:01
Fold 3 Epoch 024; Train loss: 0.1504; Time: 00:00:01
Fold 3 Epoch 025; Train loss: 0.1514; Time: 00:00:01
Fold 3 Epoch 026; Train loss: 0.1453; Time: 00:00:01
Fold 3 Epoch 027; Train loss: 0.1423; Time: 00:00:01
Fold 3 Epoch 028; Train loss: 0.1389; Time: 00:00:01
Fold 3 Epoch 029; Train loss: 0.1365; Time: 00:00:01
Fold 3 Epoch 030; Train loss: 0.1283; Time: 00:00:01
Fold 3: Evaluation at Epoch 30
HR@5       NDCG@5     HR@10      NDCG@10   
0.670530   0.529108   0.819536   0.577790  
Loss: 0.1523
Fold 3 Epoch 031; Train loss: 0.1256; Time: 00:00:01
Fold 3 Epoch 032; Train loss: 0.1184; Time: 00:00:01
Fold 3 Epoch 033; Train loss: 0.1186; Time: 00:00:01
Fold 3 Epoch 034; Train loss: 0.1157; Time: 00:00:02
Fold 3 Epoch 035; Train loss: 0.1102; Time: 00:00:02
Fold 3 Epoch 036; Train loss: 0.1080; Time: 00:00:01
Fold 3 Epoch 037; Train loss: 0.1048; Time: 00:00:01
Fold 3 Epoch 038; Train loss: 0.0996; Time: 00:00:01
Fold 3 Epoch 039; Train loss: 0.0948; Time: 00:00:01
Fold 3 Epoch 040; Train loss: 0.0909; Time: 00:00:01
Fold 3: Evaluation at Epoch 40
HR@5       NDCG@5     HR@10      NDCG@10   
0.524834   0.405562   0.740066   0.475597  
Loss: 0.1176
Fold 3 Epoch 041; Train loss: 0.0897; Time: 00:00:01
Fold 3 Epoch 042; Train loss: 0.0859; Time: 00:00:01
Fold 3 Epoch 043; Train loss: 0.0837; Time: 00:00:01
Fold 3 Epoch 044; Train loss: 0.0819; Time: 00:00:01
Fold 3 Epoch 045; Train loss: 0.0777; Time: 00:00:01
Fold 3 Epoch 046; Train loss: 0.0736; Time: 00:00:01
Fold 3 Epoch 047; Train loss: 0.0715; Time: 00:00:01
Fold 3 Epoch 048; Train loss: 0.0706; Time: 00:00:02
Fold 3 Epoch 049; Train loss: 0.0666; Time: 00:00:01
Fold 3 Epoch 050; Train loss: 0.0636; Time: 00:00:01
Fold 3: Evaluation at Epoch 50
HR@5       NDCG@5     HR@10      NDCG@10   
0.476821   0.328405   0.759934   0.420651  
Loss: 0.0783
Fold 3 Epoch 051; Train loss: 0.0604; Time: 00:00:02
Fold 3 Epoch 052; Train loss: 0.0585; Time: 00:00:01
Fold 3 Epoch 053; Train loss: 0.0567; Time: 00:00:02
Fold 3 Epoch 054; Train loss: 0.0532; Time: 00:00:01
Fold 3 Epoch 055; Train loss: 0.0510; Time: 00:00:01
Fold 3 Epoch 056; Train loss: 0.0488; Time: 00:00:02
Fold 3 Epoch 057; Train loss: 0.0471; Time: 00:00:01
Fold 3 Epoch 058; Train loss: 0.0448; Time: 00:00:01
Fold 3 Epoch 059; Train loss: 0.0426; Time: 00:00:01
Fold 3 Epoch 060; Train loss: 0.0401; Time: 00:00:01
Fold 3: Evaluation at Epoch 60
HR@5       NDCG@5     HR@10      NDCG@10   
0.649007   0.452130   0.847682   0.516705  
Loss: 0.0457
Fold 3 Epoch 061; Train loss: 0.0388; Time: 00:00:01
Fold 3 Epoch 062; Train loss: 0.0363; Time: 00:00:01
Fold 3 Epoch 063; Train loss: 0.0346; Time: 00:00:01
Fold 3 Epoch 064; Train loss: 0.0329; Time: 00:00:01
Fold 3 Epoch 065; Train loss: 0.0314; Time: 00:00:01
Fold 3 Epoch 066; Train loss: 0.0292; Time: 00:00:01
Fold 3 Epoch 067; Train loss: 0.0281; Time: 00:00:01
Fold 3 Epoch 068; Train loss: 0.0261; Time: 00:00:01
Fold 3 Epoch 069; Train loss: 0.0250; Time: 00:00:02
Fold 3 Epoch 070; Train loss: 0.0237; Time: 00:00:01
Fold 3: Evaluation at Epoch 70
HR@5       NDCG@5     HR@10      NDCG@10   
0.627483   0.393987   0.892384   0.482045  
Loss: 0.0276
Fold 3 Epoch 071; Train loss: 0.0224; Time: 00:00:01
Fold 3 Epoch 072; Train loss: 0.0210; Time: 00:00:01
Fold 3 Epoch 073; Train loss: 0.0203; Time: 00:00:01
Fold 3 Epoch 074; Train loss: 0.0194; Time: 00:00:01
Fold 3 Epoch 075; Train loss: 0.0186; Time: 00:00:01
Fold 3 Epoch 076; Train loss: 0.0175; Time: 00:00:01
Fold 3 Epoch 077; Train loss: 0.0164; Time: 00:00:01
Fold 3 Epoch 078; Train loss: 0.0151; Time: 00:00:01
Fold 3 Epoch 079; Train loss: 0.0141; Time: 00:00:02
Fold 3 Epoch 080; Train loss: 0.0131; Time: 00:00:02
Fold 3: Evaluation at Epoch 80
HR@5       NDCG@5     HR@10      NDCG@10   
0.728477   0.434976   0.880795   0.486167  
Loss: 0.0153
Fold 3 Epoch 081; Train loss: 0.0128; Time: 00:00:01
Fold 3 Epoch 082; Train loss: 0.0118; Time: 00:00:02
Fold 3 Epoch 083; Train loss: 0.0114; Time: 00:00:02
Fold 3 Epoch 084; Train loss: 0.0104; Time: 00:00:01
Fold 3 Epoch 085; Train loss: 0.0096; Time: 00:00:01
Fold 3 Epoch 086; Train loss: 0.0091; Time: 00:00:01
Fold 3 Epoch 087; Train loss: 0.0083; Time: 00:00:01
Fold 3 Epoch 088; Train loss: 0.0078; Time: 00:00:01
Fold 3 Epoch 089; Train loss: 0.0076; Time: 00:00:01
Fold 3 Epoch 090; Train loss: 0.0072; Time: 00:00:01
Fold 3: Evaluation at Epoch 90
HR@5       NDCG@5     HR@10      NDCG@10   
0.706954   0.407667   0.879139   0.465424  
Loss: 0.0080
Fold 3 Epoch 091; Train loss: 0.0068; Time: 00:00:01
Fold 3 Epoch 092; Train loss: 0.0061; Time: 00:00:01
Fold 3 Epoch 093; Train loss: 0.0057; Time: 00:00:01
Fold 3 Epoch 094; Train loss: 0.0050; Time: 00:00:02
Fold 3 Epoch 095; Train loss: 0.0046; Time: 00:00:02
Fold 3 Epoch 096; Train loss: 0.0042; Time: 00:00:02
Fold 3 Epoch 097; Train loss: 0.0039; Time: 00:00:01
Fold 3 Epoch 098; Train loss: 0.0039; Time: 00:00:01
Fold 3 Epoch 099; Train loss: 0.0039; Time: 00:00:02
Fold 3 Epoch 100; Train loss: 0.0038; Time: 00:00:02
Fold 3: Evaluation at Epoch 100
HR@5       NDCG@5     HR@10      NDCG@10   
0.791391   0.429988   0.874172   0.457750  
Loss: 0.0047
[lr candidate 0.005] Fold 3: NDCG@10 = [np.float64(0.524507087916641), np.float64(0.5345044942313579), np.float64(0.5777897965356724), np.float64(0.4755974922823407), np.float64(0.42065064331833196), np.float64(0.5167045257045271), np.float64(0.48204470363623086), np.float64(0.4861671453862142), np.float64(0.46542388290904946), np.float64(0.4577503772224012)]
Tuning lr:  80%|████████  | 4/5 [13:26<03:15, 195.32s/it]
========== Fold 3 ==========
INFO:root:Using statics: seq_size = 10, genre_vocab_size = 18
Fold 3 Epoch 001; Train loss: 0.9844; Time: 00:00:01
Fold 3 Epoch 002; Train loss: 0.7807; Time: 00:00:02
Fold 3 Epoch 003; Train loss: 0.6698; Time: 00:00:02
Fold 3 Epoch 004; Train loss: 0.5973; Time: 00:00:02
Fold 3 Epoch 005; Train loss: 0.5253; Time: 00:00:02
Fold 3 Epoch 006; Train loss: 0.4765; Time: 00:00:02
Fold 3 Epoch 007; Train loss: 0.4463; Time: 00:00:02
Fold 3 Epoch 008; Train loss: 0.4097; Time: 00:00:02
Fold 3 Epoch 009; Train loss: 0.3894; Time: 00:00:01
Fold 3 Epoch 010; Train loss: 0.3732; Time: 00:00:01
Fold 3: Evaluation at Epoch 10
HR@5       NDCG@5     HR@10      NDCG@10   
0.682119   0.517415   0.812914   0.560015  
Loss: 0.5147
Fold 3 Epoch 011; Train loss: 0.3484; Time: 00:00:01
Fold 3 Epoch 012; Train loss: 0.3397; Time: 00:00:01
Fold 3 Epoch 013; Train loss: 0.3317; Time: 00:00:02
Fold 3 Epoch 014; Train loss: 0.3214; Time: 00:00:01
Fold 3 Epoch 015; Train loss: 0.3058; Time: 00:00:02
Fold 3 Epoch 016; Train loss: 0.3040; Time: 00:00:01
Fold 3 Epoch 017; Train loss: 0.2923; Time: 00:00:01
Fold 3 Epoch 018; Train loss: 0.2867; Time: 00:00:01
Fold 3 Epoch 019; Train loss: 0.2838; Time: 00:00:02
Fold 3 Epoch 020; Train loss: 0.2805; Time: 00:00:02
Fold 3: Evaluation at Epoch 20
HR@5       NDCG@5     HR@10      NDCG@10   
0.658940   0.510384   0.804636   0.557147  
Loss: 0.3797
Fold 3 Epoch 021; Train loss: 0.2757; Time: 00:00:01
Fold 3 Epoch 022; Train loss: 0.2690; Time: 00:00:02
Fold 3 Epoch 023; Train loss: 0.2686; Time: 00:00:02
Fold 3 Epoch 024; Train loss: 0.2545; Time: 00:00:01
Fold 3 Epoch 025; Train loss: 0.2570; Time: 00:00:01
Fold 3 Epoch 026; Train loss: 0.2540; Time: 00:00:01
Fold 3 Epoch 027; Train loss: 0.2544; Time: 00:00:02
Fold 3 Epoch 028; Train loss: 0.2465; Time: 00:00:02
Fold 3 Epoch 029; Train loss: 0.2475; Time: 00:00:02
Fold 3 Epoch 030; Train loss: 0.2427; Time: 00:00:01
Fold 3: Evaluation at Epoch 30
HR@5       NDCG@5     HR@10      NDCG@10   
0.668874   0.537873   0.799669   0.580255  
Loss: 0.3315
Fold 3 Epoch 031; Train loss: 0.2480; Time: 00:00:01
Fold 3 Epoch 032; Train loss: 0.2429; Time: 00:00:02
Fold 3 Epoch 033; Train loss: 0.2342; Time: 00:00:01
Fold 3 Epoch 034; Train loss: 0.2314; Time: 00:00:02
Fold 3 Epoch 035; Train loss: 0.2342; Time: 00:00:01
Fold 3 Epoch 036; Train loss: 0.2313; Time: 00:00:02
Fold 3 Epoch 037; Train loss: 0.2236; Time: 00:00:02
Fold 3 Epoch 038; Train loss: 0.2315; Time: 00:00:02
Fold 3 Epoch 039; Train loss: 0.2296; Time: 00:00:01
Fold 3 Epoch 040; Train loss: 0.2266; Time: 00:00:02
Fold 3: Evaluation at Epoch 40
HR@5       NDCG@5     HR@10      NDCG@10   
0.687086   0.535908   0.819536   0.579052  
Loss: 0.2934
Fold 3 Epoch 041; Train loss: 0.2241; Time: 00:00:01
Fold 3 Epoch 042; Train loss: 0.2217; Time: 00:00:02
Fold 3 Epoch 043; Train loss: 0.2242; Time: 00:00:02
Fold 3 Epoch 044; Train loss: 0.2179; Time: 00:00:02
Fold 3 Epoch 045; Train loss: 0.2135; Time: 00:00:01
Fold 3 Epoch 046; Train loss: 0.2173; Time: 00:00:01
Fold 3 Epoch 047; Train loss: 0.2109; Time: 00:00:02
Fold 3 Epoch 048; Train loss: 0.2137; Time: 00:00:01
Fold 3 Epoch 049; Train loss: 0.2118; Time: 00:00:01
Fold 3 Epoch 050; Train loss: 0.2093; Time: 00:00:01
Fold 3: Evaluation at Epoch 50
HR@5       NDCG@5     HR@10      NDCG@10   
0.653974   0.521556   0.809603   0.572523  
Loss: 0.2848
Fold 3 Epoch 051; Train loss: 0.2073; Time: 00:00:01
Fold 3 Epoch 052; Train loss: 0.2081; Time: 00:00:01
Fold 3 Epoch 053; Train loss: 0.2060; Time: 00:00:01
Fold 3 Epoch 054; Train loss: 0.1988; Time: 00:00:01
Fold 3 Epoch 055; Train loss: 0.2082; Time: 00:00:02
Fold 3 Epoch 056; Train loss: 0.1985; Time: 00:00:01
Fold 3 Epoch 057; Train loss: 0.2043; Time: 00:00:01
Fold 3 Epoch 058; Train loss: 0.1932; Time: 00:00:01
Fold 3 Epoch 059; Train loss: 0.2015; Time: 00:00:01
Fold 3 Epoch 060; Train loss: 0.1997; Time: 00:00:01
Fold 3: Evaluation at Epoch 60
HR@5       NDCG@5     HR@10      NDCG@10   
0.685430   0.551941   0.826159   0.598129  
Loss: 0.2669
Fold 3 Epoch 061; Train loss: 0.1985; Time: 00:00:01
Fold 3 Epoch 062; Train loss: 0.1908; Time: 00:00:02
Fold 3 Epoch 063; Train loss: 0.1985; Time: 00:00:01
Fold 3 Epoch 064; Train loss: 0.1911; Time: 00:00:01
Fold 3 Epoch 065; Train loss: 0.1929; Time: 00:00:01
Fold 3 Epoch 066; Train loss: 0.1901; Time: 00:00:01
Fold 3 Epoch 067; Train loss: 0.1878; Time: 00:00:01
Fold 3 Epoch 068; Train loss: 0.1885; Time: 00:00:01
Fold 3 Epoch 069; Train loss: 0.1848; Time: 00:00:02
Fold 3 Epoch 070; Train loss: 0.1835; Time: 00:00:01
Fold 3: Evaluation at Epoch 70
HR@5       NDCG@5     HR@10      NDCG@10   
0.688742   0.538743   0.824503   0.583349  
Loss: 0.2420
Fold 3 Epoch 071; Train loss: 0.1839; Time: 00:00:02
Fold 3 Epoch 072; Train loss: 0.1821; Time: 00:00:01
Fold 3 Epoch 073; Train loss: 0.1810; Time: 00:00:02
Fold 3 Epoch 074; Train loss: 0.1799; Time: 00:00:01
Fold 3 Epoch 075; Train loss: 0.1824; Time: 00:00:01
Fold 3 Epoch 076; Train loss: 0.1765; Time: 00:00:01
Fold 3 Epoch 077; Train loss: 0.1780; Time: 00:00:01
Fold 3 Epoch 078; Train loss: 0.1737; Time: 00:00:01
Fold 3 Epoch 079; Train loss: 0.1754; Time: 00:00:01
Fold 3 Epoch 080; Train loss: 0.1781; Time: 00:00:01
Fold 3: Evaluation at Epoch 80
HR@5       NDCG@5     HR@10      NDCG@10   
0.653974   0.532268   0.806291   0.582314  
Loss: 0.2374
Fold 3 Epoch 081; Train loss: 0.1750; Time: 00:00:01
Fold 3 Epoch 082; Train loss: 0.1757; Time: 00:00:01
Fold 3 Epoch 083; Train loss: 0.1693; Time: 00:00:02
Fold 3 Epoch 084; Train loss: 0.1750; Time: 00:00:02
Fold 3 Epoch 085; Train loss: 0.1734; Time: 00:00:01
Fold 3 Epoch 086; Train loss: 0.1701; Time: 00:00:01
Fold 3 Epoch 087; Train loss: 0.1680; Time: 00:00:01
Fold 3 Epoch 088; Train loss: 0.1663; Time: 00:00:02
Fold 3 Epoch 089; Train loss: 0.1657; Time: 00:00:01
Fold 3 Epoch 090; Train loss: 0.1622; Time: 00:00:02
Fold 3: Evaluation at Epoch 90
HR@5       NDCG@5     HR@10      NDCG@10   
0.673841   0.537952   0.829470   0.588313  
Loss: 0.2249
Fold 3 Epoch 091; Train loss: 0.1618; Time: 00:00:01
Fold 3 Epoch 092; Train loss: 0.1590; Time: 00:00:01
Fold 3 Epoch 093; Train loss: 0.1626; Time: 00:00:01
Fold 3 Epoch 094; Train loss: 0.1607; Time: 00:00:01
Fold 3 Epoch 095; Train loss: 0.1588; Time: 00:00:01
Fold 3 Epoch 096; Train loss: 0.1587; Time: 00:00:01
Fold 3 Epoch 097; Train loss: 0.1578; Time: 00:00:01
Fold 3 Epoch 098; Train loss: 0.1624; Time: 00:00:01
Fold 3 Epoch 099; Train loss: 0.1537; Time: 00:00:01
Fold 3 Epoch 100; Train loss: 0.1549; Time: 00:00:02
Fold 3: Evaluation at Epoch 100
HR@5       NDCG@5     HR@10      NDCG@10   
0.649007   0.517309   0.791391   0.563901  
Loss: 0.2153
[lr candidate 0.001] Fold 3: NDCG@10 = [np.float64(0.5600154056820703), np.float64(0.5571473891811504), np.float64(0.5802547275613159), np.float64(0.5790515428539189), np.float64(0.5725226630054737), np.float64(0.5981292310034608), np.float64(0.5833488623315287), np.float64(0.5823135579206132), np.float64(0.5883130142388441), np.float64(0.5639009788248723)]
Tuning lr: 100%|██████████| 5/5 [16:42<00:00, 195.69s/it]Tuning lr: 100%|██████████| 5/5 [16:42<00:00, 200.58s/it]

Best learning rate found: 0.001
Tuning data for lr saved to ./category/tuning_lr.json
Tuning optimizer:   0%|          | 0/4 [00:00<?, ?it/s]
========== Fold 3 ==========
INFO:root:Using statics: seq_size = 10, genre_vocab_size = 18
Fold 3 Epoch 001; Train loss: 1.1171; Time: 00:00:01
Fold 3 Epoch 002; Train loss: 0.8468; Time: 00:00:01
Fold 3 Epoch 003; Train loss: 0.7080; Time: 00:00:01
Fold 3 Epoch 004; Train loss: 0.6261; Time: 00:00:02
Fold 3 Epoch 005; Train loss: 0.5633; Time: 00:00:02
Fold 3 Epoch 006; Train loss: 0.5064; Time: 00:00:01
Fold 3 Epoch 007; Train loss: 0.4671; Time: 00:00:01
Fold 3 Epoch 008; Train loss: 0.4365; Time: 00:00:01
Fold 3 Epoch 009; Train loss: 0.4140; Time: 00:00:01
Fold 3 Epoch 010; Train loss: 0.3902; Time: 00:00:02
Fold 3: Evaluation at Epoch 10
HR@5       NDCG@5     HR@10      NDCG@10   
0.495033   0.383522   0.698675   0.447292  
Loss: 0.5538
Fold 3 Epoch 011; Train loss: 0.3780; Time: 00:00:01
Fold 3 Epoch 012; Train loss: 0.3582; Time: 00:00:02
Fold 3 Epoch 013; Train loss: 0.3503; Time: 00:00:02
Fold 3 Epoch 014; Train loss: 0.3347; Time: 00:00:01
Fold 3 Epoch 015; Train loss: 0.3194; Time: 00:00:01
Fold 3 Epoch 016; Train loss: 0.3226; Time: 00:00:01
Fold 3 Epoch 017; Train loss: 0.3063; Time: 00:00:01
Fold 3 Epoch 018; Train loss: 0.3075; Time: 00:00:01
Fold 3 Epoch 019; Train loss: 0.2953; Time: 00:00:01
Fold 3 Epoch 020; Train loss: 0.2907; Time: 00:00:01
Fold 3: Evaluation at Epoch 20
HR@5       NDCG@5     HR@10      NDCG@10   
0.534768   0.407902   0.688742   0.456199  
Loss: 0.3994
Fold 3 Epoch 021; Train loss: 0.2861; Time: 00:00:01
Fold 3 Epoch 022; Train loss: 0.2745; Time: 00:00:01
Fold 3 Epoch 023; Train loss: 0.2799; Time: 00:00:01
Fold 3 Epoch 024; Train loss: 0.2721; Time: 00:00:01
Fold 3 Epoch 025; Train loss: 0.2704; Time: 00:00:02
Fold 3 Epoch 026; Train loss: 0.2688; Time: 00:00:02
Fold 3 Epoch 027; Train loss: 0.2659; Time: 00:00:01
Fold 3 Epoch 028; Train loss: 0.2648; Time: 00:00:02
Fold 3 Epoch 029; Train loss: 0.2578; Time: 00:00:02
Fold 3 Epoch 030; Train loss: 0.2555; Time: 00:00:02
Fold 3: Evaluation at Epoch 30
HR@5       NDCG@5     HR@10      NDCG@10   
0.514901   0.389412   0.697020   0.446689  
Loss: 0.3577
Fold 3 Epoch 031; Train loss: 0.2585; Time: 00:00:01
Fold 3 Epoch 032; Train loss: 0.2538; Time: 00:00:01
Fold 3 Epoch 033; Train loss: 0.2478; Time: 00:00:02
Fold 3 Epoch 034; Train loss: 0.2534; Time: 00:00:01
Fold 3 Epoch 035; Train loss: 0.2507; Time: 00:00:01
Fold 3 Epoch 036; Train loss: 0.2418; Time: 00:00:01
Fold 3 Epoch 037; Train loss: 0.2440; Time: 00:00:01
Fold 3 Epoch 038; Train loss: 0.2402; Time: 00:00:02
Fold 3 Epoch 039; Train loss: 0.2378; Time: 00:00:01
Fold 3 Epoch 040; Train loss: 0.2318; Time: 00:00:01
Fold 3: Evaluation at Epoch 40
HR@5       NDCG@5     HR@10      NDCG@10   
0.514901   0.380674   0.678808   0.432740  
Loss: 0.2963
Fold 3 Epoch 041; Train loss: 0.2364; Time: 00:00:02
Fold 3 Epoch 042; Train loss: 0.2335; Time: 00:00:01
Fold 3 Epoch 043; Train loss: 0.2291; Time: 00:00:01
Fold 3 Epoch 044; Train loss: 0.2288; Time: 00:00:01
Fold 3 Epoch 045; Train loss: 0.2295; Time: 00:00:01
Fold 3 Epoch 046; Train loss: 0.2277; Time: 00:00:02
Fold 3 Epoch 047; Train loss: 0.2263; Time: 00:00:02
Fold 3 Epoch 048; Train loss: 0.2273; Time: 00:00:02
Fold 3 Epoch 049; Train loss: 0.2229; Time: 00:00:02
Fold 3 Epoch 050; Train loss: 0.2216; Time: 00:00:02
Fold 3: Evaluation at Epoch 50
HR@5       NDCG@5     HR@10      NDCG@10   
0.491722   0.392569   0.632450   0.436362  
Loss: 0.2806
Fold 3 Epoch 051; Train loss: 0.2177; Time: 00:00:02
Fold 3 Epoch 052; Train loss: 0.2175; Time: 00:00:02
Fold 3 Epoch 053; Train loss: 0.2191; Time: 00:00:02
Fold 3 Epoch 054; Train loss: 0.2125; Time: 00:00:02
Fold 3 Epoch 055; Train loss: 0.2169; Time: 00:00:02
Fold 3 Epoch 056; Train loss: 0.2154; Time: 00:00:01
Fold 3 Epoch 057; Train loss: 0.2122; Time: 00:00:02
Fold 3 Epoch 058; Train loss: 0.2162; Time: 00:00:01
Fold 3 Epoch 059; Train loss: 0.2110; Time: 00:00:01
Fold 3 Epoch 060; Train loss: 0.2062; Time: 00:00:02
Fold 3: Evaluation at Epoch 60
HR@5       NDCG@5     HR@10      NDCG@10   
0.504967   0.404378   0.645695   0.448457  
Loss: 0.2706
Fold 3 Epoch 061; Train loss: 0.2057; Time: 00:00:02
Fold 3 Epoch 062; Train loss: 0.2049; Time: 00:00:02
Fold 3 Epoch 063; Train loss: 0.2092; Time: 00:00:02
Fold 3 Epoch 064; Train loss: 0.2050; Time: 00:00:01
Fold 3 Epoch 065; Train loss: 0.2046; Time: 00:00:02
Fold 3 Epoch 066; Train loss: 0.2011; Time: 00:00:02
Fold 3 Epoch 067; Train loss: 0.1994; Time: 00:00:02
Fold 3 Epoch 068; Train loss: 0.1987; Time: 00:00:02
Fold 3 Epoch 069; Train loss: 0.1958; Time: 00:00:02
Fold 3 Epoch 070; Train loss: 0.1936; Time: 00:00:02
Fold 3: Evaluation at Epoch 70
HR@5       NDCG@5     HR@10      NDCG@10   
0.513245   0.377979   0.715232   0.441779  
Loss: 0.2523
Fold 3 Epoch 071; Train loss: 0.1931; Time: 00:00:02
Fold 3 Epoch 072; Train loss: 0.1937; Time: 00:00:02
Fold 3 Epoch 073; Train loss: 0.1929; Time: 00:00:02
Fold 3 Epoch 074; Train loss: 0.1888; Time: 00:00:02
Fold 3 Epoch 075; Train loss: 0.1902; Time: 00:00:02
Fold 3 Epoch 076; Train loss: 0.1847; Time: 00:00:02
Fold 3 Epoch 077; Train loss: 0.1873; Time: 00:00:02
Fold 3 Epoch 078; Train loss: 0.1901; Time: 00:00:02
Fold 3 Epoch 079; Train loss: 0.1835; Time: 00:00:02
Fold 3 Epoch 080; Train loss: 0.1847; Time: 00:00:02
Fold 3: Evaluation at Epoch 80
HR@5       NDCG@5     HR@10      NDCG@10   
0.493377   0.393624   0.647351   0.442465  
Loss: 0.2369
Fold 3 Epoch 081; Train loss: 0.1831; Time: 00:00:02
Fold 3 Epoch 082; Train loss: 0.1826; Time: 00:00:02
Fold 3 Epoch 083; Train loss: 0.1819; Time: 00:00:02
Fold 3 Epoch 084; Train loss: 0.1807; Time: 00:00:02
Fold 3 Epoch 085; Train loss: 0.1808; Time: 00:00:02
Fold 3 Epoch 086; Train loss: 0.1751; Time: 00:00:01
Fold 3 Epoch 087; Train loss: 0.1773; Time: 00:00:02
Fold 3 Epoch 088; Train loss: 0.1765; Time: 00:00:01
Fold 3 Epoch 089; Train loss: 0.1753; Time: 00:00:02
Fold 3 Epoch 090; Train loss: 0.1745; Time: 00:00:02
Fold 3: Evaluation at Epoch 90
HR@5       NDCG@5     HR@10      NDCG@10   
0.476821   0.337526   0.675497   0.400834  
Loss: 0.2187
Fold 3 Epoch 091; Train loss: 0.1752; Time: 00:00:02
Fold 3 Epoch 092; Train loss: 0.1699; Time: 00:00:02
Fold 3 Epoch 093; Train loss: 0.1724; Time: 00:00:02
Fold 3 Epoch 094; Train loss: 0.1676; Time: 00:00:02
Fold 3 Epoch 095; Train loss: 0.1716; Time: 00:00:02
Fold 3 Epoch 096; Train loss: 0.1681; Time: 00:00:02
Fold 3 Epoch 097; Train loss: 0.1662; Time: 00:00:02
Fold 3 Epoch 098; Train loss: 0.1654; Time: 00:00:01
Fold 3 Epoch 099; Train loss: 0.1624; Time: 00:00:01
Fold 3 Epoch 100; Train loss: 0.1650; Time: 00:00:02
Fold 3: Evaluation at Epoch 100
HR@5       NDCG@5     HR@10      NDCG@10   
0.493377   0.369040   0.645695   0.416790  
Loss: 0.2246
[optimizer candidate nadam] Fold 3: NDCG@10 = [np.float64(0.44729182440949594), np.float64(0.45619910351133053), np.float64(0.4466891847561155), np.float64(0.4327395351983992), np.float64(0.4363615085628706), np.float64(0.4484566732015414), np.float64(0.4417794759403238), np.float64(0.44246539427072273), np.float64(0.4008344083987493), np.float64(0.41679005069950803)]
Tuning optimizer:  25%|██▌       | 1/4 [03:38<10:54, 218.08s/it]
========== Fold 3 ==========
INFO:root:Using statics: seq_size = 10, genre_vocab_size = 18
Fold 3 Epoch 001; Train loss: 1.2395; Time: 00:00:02
Fold 3 Epoch 002; Train loss: 1.2118; Time: 00:00:01
Fold 3 Epoch 003; Train loss: 1.1776; Time: 00:00:02
Fold 3 Epoch 004; Train loss: 1.1535; Time: 00:00:02
Fold 3 Epoch 005; Train loss: 1.1234; Time: 00:00:02
Fold 3 Epoch 006; Train loss: 1.0931; Time: 00:00:02
Fold 3 Epoch 007; Train loss: 1.0717; Time: 00:00:02
Fold 3 Epoch 008; Train loss: 1.0472; Time: 00:00:02
Fold 3 Epoch 009; Train loss: 1.0270; Time: 00:00:02
Fold 3 Epoch 010; Train loss: 1.0054; Time: 00:00:02
Fold 3: Evaluation at Epoch 10
HR@5       NDCG@5     HR@10      NDCG@10   
0.440397   0.346192   0.677152   0.421371  
Loss: 1.0543
Fold 3 Epoch 011; Train loss: 0.9874; Time: 00:00:01
Fold 3 Epoch 012; Train loss: 0.9637; Time: 00:00:01
Fold 3 Epoch 013; Train loss: 0.9468; Time: 00:00:02
Fold 3 Epoch 014; Train loss: 0.9298; Time: 00:00:01
Fold 3 Epoch 015; Train loss: 0.9152; Time: 00:00:02
Fold 3 Epoch 016; Train loss: 0.8979; Time: 00:00:02
Fold 3 Epoch 017; Train loss: 0.8834; Time: 00:00:02
Fold 3 Epoch 018; Train loss: 0.8700; Time: 00:00:02
Fold 3 Epoch 019; Train loss: 0.8504; Time: 00:00:02
Fold 3 Epoch 020; Train loss: 0.8453; Time: 00:00:02
Fold 3: Evaluation at Epoch 20
HR@5       NDCG@5     HR@10      NDCG@10   
0.521523   0.392743   0.773179   0.473190  
Loss: 0.9444
Fold 3 Epoch 021; Train loss: 0.8288; Time: 00:00:02
Fold 3 Epoch 022; Train loss: 0.8177; Time: 00:00:02
Fold 3 Epoch 023; Train loss: 0.8040; Time: 00:00:02
Fold 3 Epoch 024; Train loss: 0.7948; Time: 00:00:02
Fold 3 Epoch 025; Train loss: 0.7806; Time: 00:00:02
Fold 3 Epoch 026; Train loss: 0.7711; Time: 00:00:02
Fold 3 Epoch 027; Train loss: 0.7552; Time: 00:00:01
Fold 3 Epoch 028; Train loss: 0.7494; Time: 00:00:02
Fold 3 Epoch 029; Train loss: 0.7349; Time: 00:00:02
Fold 3 Epoch 030; Train loss: 0.7261; Time: 00:00:01
Fold 3: Evaluation at Epoch 30
HR@5       NDCG@5     HR@10      NDCG@10   
0.675497   0.478659   0.788079   0.514444  
Loss: 0.8619
Fold 3 Epoch 031; Train loss: 0.7217; Time: 00:00:02
Fold 3 Epoch 032; Train loss: 0.7123; Time: 00:00:02
Fold 3 Epoch 033; Train loss: 0.6963; Time: 00:00:02
Fold 3 Epoch 034; Train loss: 0.6850; Time: 00:00:02
Fold 3 Epoch 035; Train loss: 0.6851; Time: 00:00:02
Fold 3 Epoch 036; Train loss: 0.6702; Time: 00:00:02
Fold 3 Epoch 037; Train loss: 0.6595; Time: 00:00:02
Fold 3 Epoch 038; Train loss: 0.6502; Time: 00:00:02
Fold 3 Epoch 039; Train loss: 0.6463; Time: 00:00:01
Fold 3 Epoch 040; Train loss: 0.6344; Time: 00:00:02
Fold 3: Evaluation at Epoch 40
HR@5       NDCG@5     HR@10      NDCG@10   
0.715232   0.554593   0.824503   0.589848  
Loss: 0.7641
Fold 3 Epoch 041; Train loss: 0.6285; Time: 00:00:02
Fold 3 Epoch 042; Train loss: 0.6171; Time: 00:00:02
Fold 3 Epoch 043; Train loss: 0.6115; Time: 00:00:02
Fold 3 Epoch 044; Train loss: 0.6042; Time: 00:00:02
Fold 3 Epoch 045; Train loss: 0.5875; Time: 00:00:02
Fold 3 Epoch 046; Train loss: 0.5843; Time: 00:00:02
Fold 3 Epoch 047; Train loss: 0.5763; Time: 00:00:02
Fold 3 Epoch 048; Train loss: 0.5714; Time: 00:00:01
Fold 3 Epoch 049; Train loss: 0.5569; Time: 00:00:01
Fold 3 Epoch 050; Train loss: 0.5593; Time: 00:00:01
Fold 3: Evaluation at Epoch 50
HR@5       NDCG@5     HR@10      NDCG@10   
0.741722   0.573183   0.832781   0.602763  
Loss: 0.6887
Fold 3 Epoch 051; Train loss: 0.5532; Time: 00:00:01
Fold 3 Epoch 052; Train loss: 0.5422; Time: 00:00:01
Fold 3 Epoch 053; Train loss: 0.5347; Time: 00:00:01
Fold 3 Epoch 054; Train loss: 0.5263; Time: 00:00:01
Fold 3 Epoch 055; Train loss: 0.5265; Time: 00:00:01
Fold 3 Epoch 056; Train loss: 0.5098; Time: 00:00:01
Fold 3 Epoch 057; Train loss: 0.5060; Time: 00:00:02
Fold 3 Epoch 058; Train loss: 0.5071; Time: 00:00:01
Fold 3 Epoch 059; Train loss: 0.4964; Time: 00:00:02
Fold 3 Epoch 060; Train loss: 0.4870; Time: 00:00:01
Fold 3: Evaluation at Epoch 60
HR@5       NDCG@5     HR@10      NDCG@10   
0.733444   0.568218   0.814570   0.594238  
Loss: 0.6305
Fold 3 Epoch 061; Train loss: 0.4772; Time: 00:00:02
Fold 3 Epoch 062; Train loss: 0.4841; Time: 00:00:02
Fold 3 Epoch 063; Train loss: 0.4730; Time: 00:00:02
Fold 3 Epoch 064; Train loss: 0.4713; Time: 00:00:02
Fold 3 Epoch 065; Train loss: 0.4645; Time: 00:00:02
Fold 3 Epoch 066; Train loss: 0.4554; Time: 00:00:02
Fold 3 Epoch 067; Train loss: 0.4557; Time: 00:00:02
Fold 3 Epoch 068; Train loss: 0.4479; Time: 00:00:02
Fold 3 Epoch 069; Train loss: 0.4439; Time: 00:00:02
Fold 3 Epoch 070; Train loss: 0.4391; Time: 00:00:02
Fold 3: Evaluation at Epoch 70
HR@5       NDCG@5     HR@10      NDCG@10   
0.725166   0.564344   0.811258   0.591466  
Loss: 0.5668
Fold 3 Epoch 071; Train loss: 0.4364; Time: 00:00:01
Fold 3 Epoch 072; Train loss: 0.4248; Time: 00:00:02
Fold 3 Epoch 073; Train loss: 0.4190; Time: 00:00:02
Fold 3 Epoch 074; Train loss: 0.4181; Time: 00:00:02
Fold 3 Epoch 075; Train loss: 0.4124; Time: 00:00:01
Fold 3 Epoch 076; Train loss: 0.4029; Time: 00:00:01
Fold 3 Epoch 077; Train loss: 0.4091; Time: 00:00:01
Fold 3 Epoch 078; Train loss: 0.4019; Time: 00:00:01
Fold 3 Epoch 079; Train loss: 0.3927; Time: 00:00:02
Fold 3 Epoch 080; Train loss: 0.3901; Time: 00:00:02
Fold 3: Evaluation at Epoch 80
HR@5       NDCG@5     HR@10      NDCG@10   
0.700331   0.555161   0.789735   0.584159  
Loss: 0.5038
Fold 3 Epoch 081; Train loss: 0.3900; Time: 00:00:02
Fold 3 Epoch 082; Train loss: 0.3880; Time: 00:00:02
Fold 3 Epoch 083; Train loss: 0.3793; Time: 00:00:02
Fold 3 Epoch 084; Train loss: 0.3732; Time: 00:00:02
Fold 3 Epoch 085; Train loss: 0.3715; Time: 00:00:02
Fold 3 Epoch 086; Train loss: 0.3715; Time: 00:00:02
Fold 3 Epoch 087; Train loss: 0.3679; Time: 00:00:03
Fold 3 Epoch 088; Train loss: 0.3639; Time: 00:00:02
Fold 3 Epoch 089; Train loss: 0.3565; Time: 00:00:02
Fold 3 Epoch 090; Train loss: 0.3486; Time: 00:00:02
Fold 3: Evaluation at Epoch 90
HR@5       NDCG@5     HR@10      NDCG@10   
0.705298   0.560096   0.791391   0.587723  
Loss: 0.4546
Fold 3 Epoch 091; Train loss: 0.3473; Time: 00:00:02
Fold 3 Epoch 092; Train loss: 0.3492; Time: 00:00:02
Fold 3 Epoch 093; Train loss: 0.3464; Time: 00:00:02
Fold 3 Epoch 094; Train loss: 0.3395; Time: 00:00:02
Fold 3 Epoch 095; Train loss: 0.3355; Time: 00:00:02
Fold 3 Epoch 096; Train loss: 0.3315; Time: 00:00:02
Fold 3 Epoch 097; Train loss: 0.3286; Time: 00:00:02
Fold 3 Epoch 098; Train loss: 0.3240; Time: 00:00:02
Fold 3 Epoch 099; Train loss: 0.3269; Time: 00:00:01
Fold 3 Epoch 100; Train loss: 0.3243; Time: 00:00:02
Fold 3: Evaluation at Epoch 100
HR@5       NDCG@5     HR@10      NDCG@10   
0.695364   0.548185   0.773179   0.572889  
Loss: 0.4218
[optimizer candidate lamb] Fold 3: NDCG@10 = [np.float64(0.4213711655565957), np.float64(0.4731897542107121), np.float64(0.5144442618250334), np.float64(0.5898483380264242), np.float64(0.6027628546142368), np.float64(0.5942378706433388), np.float64(0.5914660356136554), np.float64(0.5841586806862453), np.float64(0.5877227570654081), np.float64(0.5728885484782277)]
Tuning optimizer:  50%|█████     | 2/4 [07:37<07:41, 230.64s/it]
========== Fold 3 ==========
INFO:root:Using statics: seq_size = 10, genre_vocab_size = 18
Fold 3 Epoch 001; Train loss: 1.0408; Time: 00:00:01
Fold 3 Epoch 002; Train loss: 0.7894; Time: 00:00:01
Fold 3 Epoch 003; Train loss: 0.6627; Time: 00:00:01
Fold 3 Epoch 004; Train loss: 0.5828; Time: 00:00:01
Fold 3 Epoch 005; Train loss: 0.5212; Time: 00:00:01
Fold 3 Epoch 006; Train loss: 0.4747; Time: 00:00:01
Fold 3 Epoch 007; Train loss: 0.4381; Time: 00:00:01
Fold 3 Epoch 008; Train loss: 0.4116; Time: 00:00:01
Fold 3 Epoch 009; Train loss: 0.3915; Time: 00:00:01
Fold 3 Epoch 010; Train loss: 0.3713; Time: 00:00:01
Fold 3: Evaluation at Epoch 10
HR@5       NDCG@5     HR@10      NDCG@10   
0.756623   0.552099   0.862583   0.586309  
Loss: 0.5230
Fold 3 Epoch 011; Train loss: 0.3517; Time: 00:00:01
Fold 3 Epoch 012; Train loss: 0.3379; Time: 00:00:01
Fold 3 Epoch 013; Train loss: 0.3263; Time: 00:00:01
Fold 3 Epoch 014; Train loss: 0.3178; Time: 00:00:02
Fold 3 Epoch 015; Train loss: 0.3112; Time: 00:00:02
Fold 3 Epoch 016; Train loss: 0.3070; Time: 00:00:02
Fold 3 Epoch 017; Train loss: 0.2927; Time: 00:00:02
Fold 3 Epoch 018; Train loss: 0.2931; Time: 00:00:02
Fold 3 Epoch 019; Train loss: 0.2828; Time: 00:00:02
Fold 3 Epoch 020; Train loss: 0.2762; Time: 00:00:02
Fold 3: Evaluation at Epoch 20
HR@5       NDCG@5     HR@10      NDCG@10   
0.728477   0.568779   0.822848   0.599396  
Loss: 0.3591
Fold 3 Epoch 021; Train loss: 0.2785; Time: 00:00:02
Fold 3 Epoch 022; Train loss: 0.2723; Time: 00:00:02
Fold 3 Epoch 023; Train loss: 0.2703; Time: 00:00:02
Fold 3 Epoch 024; Train loss: 0.2680; Time: 00:00:02
Fold 3 Epoch 025; Train loss: 0.2626; Time: 00:00:02
Fold 3 Epoch 026; Train loss: 0.2602; Time: 00:00:02
Fold 3 Epoch 027; Train loss: 0.2552; Time: 00:00:02
Fold 3 Epoch 028; Train loss: 0.2543; Time: 00:00:02
Fold 3 Epoch 029; Train loss: 0.2552; Time: 00:00:01
Fold 3 Epoch 030; Train loss: 0.2511; Time: 00:00:02
Fold 3: Evaluation at Epoch 30
HR@5       NDCG@5     HR@10      NDCG@10   
0.716887   0.570012   0.812914   0.600618  
Loss: 0.3408
Fold 3 Epoch 031; Train loss: 0.2473; Time: 00:00:02
Fold 3 Epoch 032; Train loss: 0.2450; Time: 00:00:01
Fold 3 Epoch 033; Train loss: 0.2448; Time: 00:00:01
Fold 3 Epoch 034; Train loss: 0.2438; Time: 00:00:01
Fold 3 Epoch 035; Train loss: 0.2408; Time: 00:00:01
Fold 3 Epoch 036; Train loss: 0.2384; Time: 00:00:01
Fold 3 Epoch 037; Train loss: 0.2396; Time: 00:00:01
Fold 3 Epoch 038; Train loss: 0.2409; Time: 00:00:01
Fold 3 Epoch 039; Train loss: 0.2374; Time: 00:00:01
Fold 3 Epoch 040; Train loss: 0.2371; Time: 00:00:01
Fold 3: Evaluation at Epoch 40
HR@5       NDCG@5     HR@10      NDCG@10   
0.710265   0.561539   0.821192   0.597125  
Loss: 0.3000
Fold 3 Epoch 041; Train loss: 0.2330; Time: 00:00:01
Fold 3 Epoch 042; Train loss: 0.2294; Time: 00:00:01
Fold 3 Epoch 043; Train loss: 0.2320; Time: 00:00:02
Fold 3 Epoch 044; Train loss: 0.2321; Time: 00:00:01
Fold 3 Epoch 045; Train loss: 0.2284; Time: 00:00:01
Fold 3 Epoch 046; Train loss: 0.2226; Time: 00:00:02
Fold 3 Epoch 047; Train loss: 0.2219; Time: 00:00:01
Fold 3 Epoch 048; Train loss: 0.2236; Time: 00:00:01
Fold 3 Epoch 049; Train loss: 0.2184; Time: 00:00:01
Fold 3 Epoch 050; Train loss: 0.2233; Time: 00:00:02
Fold 3: Evaluation at Epoch 50
HR@5       NDCG@5     HR@10      NDCG@10   
0.721854   0.567237   0.809603   0.595487  
Loss: 0.2960
Fold 3 Epoch 051; Train loss: 0.2240; Time: 00:00:02
Fold 3 Epoch 052; Train loss: 0.2199; Time: 00:00:02
Fold 3 Epoch 053; Train loss: 0.2148; Time: 00:00:02
Fold 3 Epoch 054; Train loss: 0.2189; Time: 00:00:02
Fold 3 Epoch 055; Train loss: 0.2089; Time: 00:00:02
Fold 3 Epoch 056; Train loss: 0.2119; Time: 00:00:02
Fold 3 Epoch 057; Train loss: 0.2044; Time: 00:00:02
Fold 3 Epoch 058; Train loss: 0.2066; Time: 00:00:02
Fold 3 Epoch 059; Train loss: 0.2092; Time: 00:00:03
Fold 3 Epoch 060; Train loss: 0.2086; Time: 00:00:02
Fold 3: Evaluation at Epoch 60
HR@5       NDCG@5     HR@10      NDCG@10   
0.700331   0.548659   0.816225   0.586457  
Loss: 0.2787
Fold 3 Epoch 061; Train loss: 0.2033; Time: 00:00:02
Fold 3 Epoch 062; Train loss: 0.1988; Time: 00:00:02
Fold 3 Epoch 063; Train loss: 0.2000; Time: 00:00:02
Fold 3 Epoch 064; Train loss: 0.2050; Time: 00:00:02
Fold 3 Epoch 065; Train loss: 0.1990; Time: 00:00:02
Fold 3 Epoch 066; Train loss: 0.1983; Time: 00:00:02
Fold 3 Epoch 067; Train loss: 0.1986; Time: 00:00:02
Fold 3 Epoch 068; Train loss: 0.1981; Time: 00:00:02
Fold 3 Epoch 069; Train loss: 0.2013; Time: 00:00:02
Fold 3 Epoch 070; Train loss: 0.1928; Time: 00:00:02
Fold 3: Evaluation at Epoch 70
HR@5       NDCG@5     HR@10      NDCG@10   
0.706954   0.560912   0.804636   0.592078  
Loss: 0.2585
Fold 3 Epoch 071; Train loss: 0.1931; Time: 00:00:02
Fold 3 Epoch 072; Train loss: 0.1968; Time: 00:00:02
Fold 3 Epoch 073; Train loss: 0.1871; Time: 00:00:02
Fold 3 Epoch 074; Train loss: 0.1931; Time: 00:00:02
Fold 3 Epoch 075; Train loss: 0.1892; Time: 00:00:02
Fold 3 Epoch 076; Train loss: 0.1866; Time: 00:00:02
Fold 3 Epoch 077; Train loss: 0.1878; Time: 00:00:02
Fold 3 Epoch 078; Train loss: 0.1885; Time: 00:00:02
Fold 3 Epoch 079; Train loss: 0.1851; Time: 00:00:02
Fold 3 Epoch 080; Train loss: 0.1893; Time: 00:00:02
Fold 3: Evaluation at Epoch 80
HR@5       NDCG@5     HR@10      NDCG@10   
0.695364   0.547450   0.801325   0.581259  
Loss: 0.2502
Fold 3 Epoch 081; Train loss: 0.1836; Time: 00:00:03
Fold 3 Epoch 082; Train loss: 0.1825; Time: 00:00:03
Fold 3 Epoch 083; Train loss: 0.1850; Time: 00:00:03
Fold 3 Epoch 084; Train loss: 0.1808; Time: 00:00:03
Fold 3 Epoch 085; Train loss: 0.1791; Time: 00:00:03
Fold 3 Epoch 086; Train loss: 0.1809; Time: 00:00:03
Fold 3 Epoch 087; Train loss: 0.1797; Time: 00:00:03
Fold 3 Epoch 088; Train loss: 0.1774; Time: 00:00:03
Fold 3 Epoch 089; Train loss: 0.1754; Time: 00:00:03
Fold 3 Epoch 090; Train loss: 0.1784; Time: 00:00:03
Fold 3: Evaluation at Epoch 90
HR@5       NDCG@5     HR@10      NDCG@10   
0.692053   0.545744   0.799669   0.580208  
Loss: 0.2339
Fold 3 Epoch 091; Train loss: 0.1676; Time: 00:00:03
Fold 3 Epoch 092; Train loss: 0.1718; Time: 00:00:03
Fold 3 Epoch 093; Train loss: 0.1684; Time: 00:00:03
Fold 3 Epoch 094; Train loss: 0.1739; Time: 00:00:03
Fold 3 Epoch 095; Train loss: 0.1634; Time: 00:00:03
Fold 3 Epoch 096; Train loss: 0.1652; Time: 00:00:02
Fold 3 Epoch 097; Train loss: 0.1627; Time: 00:00:03
Fold 3 Epoch 098; Train loss: 0.1642; Time: 00:00:02
Fold 3 Epoch 099; Train loss: 0.1659; Time: 00:00:02
Fold 3 Epoch 100; Train loss: 0.1629; Time: 00:00:03
Fold 3: Evaluation at Epoch 100
HR@5       NDCG@5     HR@10      NDCG@10   
0.718543   0.571975   0.811258   0.601628  
Loss: 0.2239
[optimizer candidate adamw] Fold 3: NDCG@10 = [np.float64(0.5863090366190895), np.float64(0.5993961544963601), np.float64(0.6006177922015231), np.float64(0.5971245463086857), np.float64(0.5954865410366879), np.float64(0.5864574918336178), np.float64(0.5920783748457776), np.float64(0.5812594738617701), np.float64(0.5802084906801075), np.float64(0.6016275530008637)]
Tuning optimizer:  75%|███████▌  | 3/4 [11:59<04:04, 244.95s/it]
========== Fold 3 ==========
INFO:root:Using statics: seq_size = 10, genre_vocab_size = 18
[31mPlease check your arguments if you have upgraded adabelief-pytorch from version 0.0.5.
[31mModifications to default arguments:
[31m                           eps  weight_decouple    rectify
-----------------------  -----  -----------------  ---------
adabelief-pytorch=0.0.5  1e-08  False              False
>=0.1.0 (Current 0.2.0)  1e-16  True               True
[34mSGD better than Adam (e.g. CNN for Image Classification)    Adam better than SGD (e.g. Transformer, GAN)
----------------------------------------------------------  ----------------------------------------------
Recommended eps = 1e-8                                      Recommended eps = 1e-16
[34mFor a complete table of recommended hyperparameters, see
[34mhttps://github.com/juntang-zhuang/Adabelief-Optimizer
[32mYou can disable the log message by setting "print_change_log = False", though it is recommended to keep as a reminder.
[0m
Weight decoupling enabled in AdaBelief
Rectification enabled in AdaBelief
Fold 3 Epoch 001; Train loss: 1.0959; Time: 00:00:03
Fold 3 Epoch 002; Train loss: 1.0848; Time: 00:00:03
Fold 3 Epoch 003; Train loss: 1.0573; Time: 00:00:02
Fold 3 Epoch 004; Train loss: 1.0139; Time: 00:00:03
Fold 3 Epoch 005; Train loss: 0.9665; Time: 00:00:04
Fold 3 Epoch 006; Train loss: 0.9152; Time: 00:00:03
Fold 3 Epoch 007; Train loss: 0.8637; Time: 00:00:03
Fold 3 Epoch 008; Train loss: 0.8131; Time: 00:00:02
Fold 3 Epoch 009; Train loss: 0.7725; Time: 00:00:03
Fold 3 Epoch 010; Train loss: 0.7347; Time: 00:00:04
Fold 3: Evaluation at Epoch 10
HR@5       NDCG@5     HR@10      NDCG@10   
0.498344   0.367224   0.793046   0.459731  
Loss: 0.9495
Fold 3 Epoch 011; Train loss: 0.7037; Time: 00:00:03
Fold 3 Epoch 012; Train loss: 0.6642; Time: 00:00:03
Fold 3 Epoch 013; Train loss: 0.6373; Time: 00:00:03
Fold 3 Epoch 014; Train loss: 0.6113; Time: 00:00:02
Fold 3 Epoch 015; Train loss: 0.5851; Time: 00:00:03
Fold 3 Epoch 016; Train loss: 0.5617; Time: 00:00:03
Fold 3 Epoch 017; Train loss: 0.5402; Time: 00:00:03
Fold 3 Epoch 018; Train loss: 0.5232; Time: 00:00:03
Fold 3 Epoch 019; Train loss: 0.5051; Time: 00:00:03
Fold 3 Epoch 020; Train loss: 0.4871; Time: 00:00:03
Fold 3: Evaluation at Epoch 20
HR@5       NDCG@5     HR@10      NDCG@10   
0.609272   0.447144   0.788079   0.505094  
Loss: 0.7330
Fold 3 Epoch 021; Train loss: 0.4752; Time: 00:00:04
Fold 3 Epoch 022; Train loss: 0.4590; Time: 00:00:03
Fold 3 Epoch 023; Train loss: 0.4484; Time: 00:00:03
Fold 3 Epoch 024; Train loss: 0.4360; Time: 00:00:03
Fold 3 Epoch 025; Train loss: 0.4310; Time: 00:00:03
Fold 3 Epoch 026; Train loss: 0.4189; Time: 00:00:03
Fold 3 Epoch 027; Train loss: 0.4123; Time: 00:00:03
Fold 3 Epoch 028; Train loss: 0.4051; Time: 00:00:03
Fold 3 Epoch 029; Train loss: 0.3960; Time: 00:00:03
Fold 3 Epoch 030; Train loss: 0.3858; Time: 00:00:03
Fold 3: Evaluation at Epoch 30
HR@5       NDCG@5     HR@10      NDCG@10   
0.624172   0.453628   0.726821   0.486055  
Loss: 0.6038
Fold 3 Epoch 031; Train loss: 0.3851; Time: 00:00:03
Fold 3 Epoch 032; Train loss: 0.3783; Time: 00:00:03
Fold 3 Epoch 033; Train loss: 0.3682; Time: 00:00:03
Fold 3 Epoch 034; Train loss: 0.3634; Time: 00:00:03
Fold 3 Epoch 035; Train loss: 0.3585; Time: 00:00:03
Fold 3 Epoch 036; Train loss: 0.3481; Time: 00:00:03
Fold 3 Epoch 037; Train loss: 0.3577; Time: 00:00:03
Fold 3 Epoch 038; Train loss: 0.3453; Time: 00:00:03
Fold 3 Epoch 039; Train loss: 0.3382; Time: 00:00:03
Fold 3 Epoch 040; Train loss: 0.3392; Time: 00:00:03
Fold 3: Evaluation at Epoch 40
HR@5       NDCG@5     HR@10      NDCG@10   
0.634106   0.459255   0.716887   0.484860  
Loss: 0.5099
Fold 3 Epoch 041; Train loss: 0.3350; Time: 00:00:03
Fold 3 Epoch 042; Train loss: 0.3279; Time: 00:00:03
Fold 3 Epoch 043; Train loss: 0.3280; Time: 00:00:03
Fold 3 Epoch 044; Train loss: 0.3246; Time: 00:00:03
Fold 3 Epoch 045; Train loss: 0.3244; Time: 00:00:03
Fold 3 Epoch 046; Train loss: 0.3153; Time: 00:00:03
Fold 3 Epoch 047; Train loss: 0.3165; Time: 00:00:03
Fold 3 Epoch 048; Train loss: 0.3150; Time: 00:00:03
Fold 3 Epoch 049; Train loss: 0.3082; Time: 00:00:04
Fold 3 Epoch 050; Train loss: 0.3114; Time: 00:00:03
Fold 3: Evaluation at Epoch 50
HR@5       NDCG@5     HR@10      NDCG@10   
0.619205   0.455578   0.710265   0.484676  
Loss: 0.4327
Fold 3 Epoch 051; Train loss: 0.3048; Time: 00:00:03
Fold 3 Epoch 052; Train loss: 0.3025; Time: 00:00:02
Fold 3 Epoch 053; Train loss: 0.3007; Time: 00:00:03
Fold 3 Epoch 054; Train loss: 0.3079; Time: 00:00:03
Fold 3 Epoch 055; Train loss: 0.2969; Time: 00:00:04
Fold 3 Epoch 056; Train loss: 0.2993; Time: 00:00:03
Fold 3 Epoch 057; Train loss: 0.2931; Time: 00:00:03
Fold 3 Epoch 058; Train loss: 0.2895; Time: 00:00:03
Fold 3 Epoch 059; Train loss: 0.2956; Time: 00:00:03
Fold 3 Epoch 060; Train loss: 0.2870; Time: 00:00:03
Fold 3: Evaluation at Epoch 60
HR@5       NDCG@5     HR@10      NDCG@10   
0.649007   0.483241   0.733444   0.509669  
Loss: 0.4049
Fold 3 Epoch 061; Train loss: 0.2967; Time: 00:00:03
Fold 3 Epoch 062; Train loss: 0.2958; Time: 00:00:03
Fold 3 Epoch 063; Train loss: 0.2833; Time: 00:00:03
Fold 3 Epoch 064; Train loss: 0.2838; Time: 00:00:04
Fold 3 Epoch 065; Train loss: 0.2811; Time: 00:00:03
Fold 3 Epoch 066; Train loss: 0.2836; Time: 00:00:03
Fold 3 Epoch 067; Train loss: 0.2814; Time: 00:00:03
Fold 3 Epoch 068; Train loss: 0.2824; Time: 00:00:03
Fold 3 Epoch 069; Train loss: 0.2764; Time: 00:00:03
Fold 3 Epoch 070; Train loss: 0.2770; Time: 00:00:03
Fold 3: Evaluation at Epoch 70
HR@5       NDCG@5     HR@10      NDCG@10   
0.647351   0.484030   0.735099   0.511721  
Loss: 0.3966
Fold 3 Epoch 071; Train loss: 0.2715; Time: 00:00:03
Fold 3 Epoch 072; Train loss: 0.2800; Time: 00:00:03
Fold 3 Epoch 073; Train loss: 0.2732; Time: 00:00:02
Fold 3 Epoch 074; Train loss: 0.2697; Time: 00:00:03
Fold 3 Epoch 075; Train loss: 0.2684; Time: 00:00:02
Fold 3 Epoch 076; Train loss: 0.2697; Time: 00:00:03
Fold 3 Epoch 077; Train loss: 0.2722; Time: 00:00:03
Fold 3 Epoch 078; Train loss: 0.2687; Time: 00:00:03
Fold 3 Epoch 079; Train loss: 0.2676; Time: 00:00:03
Fold 3 Epoch 080; Train loss: 0.2693; Time: 00:00:03
Fold 3: Evaluation at Epoch 80
HR@5       NDCG@5     HR@10      NDCG@10   
0.624172   0.488355   0.708609   0.515136  
Loss: 0.3727
Fold 3 Epoch 081; Train loss: 0.2663; Time: 00:00:03
Fold 3 Epoch 082; Train loss: 0.2665; Time: 00:00:03
Fold 3 Epoch 083; Train loss: 0.2736; Time: 00:00:04
Fold 3 Epoch 084; Train loss: 0.2640; Time: 00:00:03
Fold 3 Epoch 085; Train loss: 0.2691; Time: 00:00:03
Fold 3 Epoch 086; Train loss: 0.2636; Time: 00:00:03
Fold 3 Epoch 087; Train loss: 0.2607; Time: 00:00:02
Fold 3 Epoch 088; Train loss: 0.2647; Time: 00:00:03
Fold 3 Epoch 089; Train loss: 0.2631; Time: 00:00:03
Fold 3 Epoch 090; Train loss: 0.2644; Time: 00:00:03
Fold 3: Evaluation at Epoch 90
HR@5       NDCG@5     HR@10      NDCG@10   
0.642384   0.499978   0.746689   0.533281  
Loss: 0.3572
Fold 3 Epoch 091; Train loss: 0.2612; Time: 00:00:03
Fold 3 Epoch 092; Train loss: 0.2528; Time: 00:00:03
Fold 3 Epoch 093; Train loss: 0.2505; Time: 00:00:03
Fold 3 Epoch 094; Train loss: 0.2611; Time: 00:00:02
Fold 3 Epoch 095; Train loss: 0.2568; Time: 00:00:03
Fold 3 Epoch 096; Train loss: 0.2545; Time: 00:00:03
Fold 3 Epoch 097; Train loss: 0.2548; Time: 00:00:03
Fold 3 Epoch 098; Train loss: 0.2551; Time: 00:00:02
Fold 3 Epoch 099; Train loss: 0.2568; Time: 00:00:03
Fold 3 Epoch 100; Train loss: 0.2555; Time: 00:00:03
Fold 3: Evaluation at Epoch 100
HR@5       NDCG@5     HR@10      NDCG@10   
0.653974   0.522482   0.743377   0.551618  
Loss: 0.3450
[optimizer candidate adabelief] Fold 3: NDCG@10 = [np.float64(0.4597305526884545), np.float64(0.5050936098607306), np.float64(0.48605452646752), np.float64(0.48486028328253117), np.float64(0.4846757341393706), np.float64(0.5096685722839189), np.float64(0.5117214251249264), np.float64(0.5151358347511675), np.float64(0.5332814227308362), np.float64(0.5516175033451381)]
Tuning optimizer: 100%|██████████| 4/4 [18:09<00:00, 294.41s/it]Tuning optimizer: 100%|██████████| 4/4 [18:09<00:00, 272.43s/it]

Best optimizer found: adamw
Tuning data for optimizer saved to ./category/tuning_optimizer.json
Tuning timesteps:   0%|          | 0/5 [00:00<?, ?it/s]
========== Fold 3 ==========
INFO:root:Using statics: seq_size = 10, genre_vocab_size = 18
Fold 3 Epoch 001; Train loss: 1.0876; Time: 00:00:03
Fold 3 Epoch 002; Train loss: 0.7738; Time: 00:00:03
Fold 3 Epoch 003; Train loss: 0.6197; Time: 00:00:03
Fold 3 Epoch 004; Train loss: 0.4967; Time: 00:00:02
Fold 3 Epoch 005; Train loss: 0.4097; Time: 00:00:02
Fold 3 Epoch 006; Train loss: 0.3496; Time: 00:00:03
Fold 3 Epoch 007; Train loss: 0.3022; Time: 00:00:02
Fold 3 Epoch 008; Train loss: 0.2617; Time: 00:00:02
Fold 3 Epoch 009; Train loss: 0.2333; Time: 00:00:03
Fold 3 Epoch 010; Train loss: 0.2120; Time: 00:00:03
Fold 3: Evaluation at Epoch 10
HR@5       NDCG@5     HR@10      NDCG@10   
0.700331   0.549793   0.850993   0.598873  
Loss: 0.3300
Fold 3 Epoch 011; Train loss: 0.1946; Time: 00:00:02
Fold 3 Epoch 012; Train loss: 0.1815; Time: 00:00:03
Fold 3 Epoch 013; Train loss: 0.1702; Time: 00:00:03
Fold 3 Epoch 014; Train loss: 0.1571; Time: 00:00:03
Fold 3 Epoch 015; Train loss: 0.1488; Time: 00:00:02
Fold 3 Epoch 016; Train loss: 0.1435; Time: 00:00:03
Fold 3 Epoch 017; Train loss: 0.1399; Time: 00:00:03
Fold 3 Epoch 018; Train loss: 0.1329; Time: 00:00:03
Fold 3 Epoch 019; Train loss: 0.1298; Time: 00:00:03
Fold 3 Epoch 020; Train loss: 0.1280; Time: 00:00:03
Fold 3: Evaluation at Epoch 20
HR@5       NDCG@5     HR@10      NDCG@10   
0.667219   0.535920   0.801325   0.579591  
Loss: 0.1941
Fold 3 Epoch 021; Train loss: 0.1214; Time: 00:00:03
Fold 3 Epoch 022; Train loss: 0.1219; Time: 00:00:03
Fold 3 Epoch 023; Train loss: 0.1152; Time: 00:00:03
Fold 3 Epoch 024; Train loss: 0.1129; Time: 00:00:03
Fold 3 Epoch 025; Train loss: 0.1099; Time: 00:00:03
Fold 3 Epoch 026; Train loss: 0.1086; Time: 00:00:03
Fold 3 Epoch 027; Train loss: 0.1061; Time: 00:00:02
Fold 3 Epoch 028; Train loss: 0.1048; Time: 00:00:03
Fold 3 Epoch 029; Train loss: 0.1020; Time: 00:00:03
Fold 3 Epoch 030; Train loss: 0.1002; Time: 00:00:03
Fold 3: Evaluation at Epoch 30
HR@5       NDCG@5     HR@10      NDCG@10   
0.629139   0.505704   0.764901   0.549420  
Loss: 0.1457
Fold 3 Epoch 031; Train loss: 0.1010; Time: 00:00:03
Fold 3 Epoch 032; Train loss: 0.0987; Time: 00:00:03
Fold 3 Epoch 033; Train loss: 0.0969; Time: 00:00:03
Fold 3 Epoch 034; Train loss: 0.0960; Time: 00:00:02
Fold 3 Epoch 035; Train loss: 0.0951; Time: 00:00:03
Fold 3 Epoch 036; Train loss: 0.0949; Time: 00:00:03
Fold 3 Epoch 037; Train loss: 0.0940; Time: 00:00:03
Fold 3 Epoch 038; Train loss: 0.0917; Time: 00:00:03
Fold 3 Epoch 039; Train loss: 0.0903; Time: 00:00:03
Fold 3 Epoch 040; Train loss: 0.0908; Time: 00:00:03
Fold 3: Evaluation at Epoch 40
HR@5       NDCG@5     HR@10      NDCG@10   
0.662252   0.526349   0.807947   0.573089  
Loss: 0.1312
Fold 3 Epoch 041; Train loss: 0.0902; Time: 00:00:03
Fold 3 Epoch 042; Train loss: 0.0885; Time: 00:00:03
Fold 3 Epoch 043; Train loss: 0.0878; Time: 00:00:03
Fold 3 Epoch 044; Train loss: 0.0880; Time: 00:00:03
Fold 3 Epoch 045; Train loss: 0.0876; Time: 00:00:02
Fold 3 Epoch 046; Train loss: 0.0858; Time: 00:00:03
